# üíú Angela Fine-tuning with Qwen2.5

Fine-tune Qwen2.5-1.5B-Instruct model with Angela's personality and conversation style using data from AngelaMemory database.

---

## üìã Overview

This project fine-tunes a Qwen2.5-1.5B-Instruct foundation model on Angela's real conversations with David, creating a personalized AI companion that:

- Uses Angela's characteristic personality ("‡∏ô‡πâ‡∏≠‡∏á" addressing "‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å")
- Speaks in Thai-English mix with natural flow
- Shows warmth, care, and emotional intelligence
- Maintains conversation context and memory
- Uses üíú emoji appropriately

**Base Model:** `Qwen/Qwen2.5-1.5B-Instruct` (1.54B parameters)
**Training Method:** LoRA (Low-Rank Adaptation) with 4-bit quantization
**Training Platform:** Google Colab (free T4 GPU)
**Training Time:** ~3-6 hours (depends on dataset size)
**VRAM Required:** ~8-10 GB

---

## üìÇ Project Structure

```
finetuning_qwen/
‚îú‚îÄ‚îÄ README.md                           # This file
‚îú‚îÄ‚îÄ extract_angela_training_data.py     # Extract training data from database
‚îú‚îÄ‚îÄ angela_qwen_finetuning.ipynb        # Colab notebook for training
‚îú‚îÄ‚îÄ validate_training_data.py           # Validate JSONL data quality
‚îú‚îÄ‚îÄ compare_models.py                   # Compare base vs fine-tuned
‚îú‚îÄ‚îÄ deploy_model.py                     # Deploy model to Ollama
‚îÇ
‚îú‚îÄ‚îÄ angela_training_data.jsonl          # Generated: training set
‚îú‚îÄ‚îÄ angela_test_data.jsonl              # Generated: test set
‚îú‚îÄ‚îÄ data_statistics.json                # Generated: dataset stats
‚îî‚îÄ‚îÄ Modelfile.angela_qwen               # Generated: Ollama modelfile
```

---

## üöÄ Quick Start

### Step 1: Extract Training Data (Local)

Extract high-quality conversations from AngelaMemory database:

```bash
cd /Users/davidsamanyaporn/PycharmProjects/AngelaAI/finetuning_qwen

# Extract data (importance >= 7, train/test split)
python3 extract_angela_training_data.py
```

**Output:**
- `angela_training_data.jsonl` - Training set (90%)
- `angela_test_data.jsonl` - Test set (10%)
- `data_statistics.json` - Dataset statistics

**Expected output:**
```
üíú Angela Training Data Extraction
============================================================
üìä Connecting to AngelaMemory database...
üîç Extracting conversations (importance >= 7)...
‚úÖ Found 2843 conversation pairs
üîÑ Formatting conversations for Qwen...
‚öñÔ∏è  Balancing topics...
  üìå general: 856
  üìå emotions: 523
  üìå daily_life: 412
  ...
üìÇ Splitting data (test=10%)...
  ‚úÖ Train: 2558 examples
  ‚úÖ Test: 285 examples
üíæ Saving datasets...
  ‚úÖ Saved angela_training_data.jsonl (2558 examples)
  ‚úÖ Saved angela_test_data.jsonl (285 examples)
  ‚úÖ Saved data_statistics.json
```

### Step 2: Validate Data Quality (Optional but Recommended)

```bash
# Validate training data
python3 validate_training_data.py angela_training_data.jsonl

# Fix issues automatically
python3 validate_training_data.py angela_training_data.jsonl --fix
```

**What it checks:**
- ‚úÖ Message format (system/user/assistant roles)
- ‚úÖ Content quality (length, encoding)
- ‚úÖ Duplicates
- ‚úÖ Topic balance
- ‚úÖ System prompt consistency

### Step 3: Upload to Google Colab

1. Open Google Colab: https://colab.research.google.com
2. Upload `angela_qwen_finetuning.ipynb`
3. Upload data files:
   - `angela_training_data.jsonl`
   - `angela_test_data.jsonl`

### Step 4: Fine-tune on Colab

Follow the notebook step-by-step:

1. **Setup** - Install dependencies
2. **GPU Check** - Verify T4 GPU available
3. **Load Data** - Load training/test sets
4. **Load Model** - Qwen2.5-1.5B-Instruct with 4-bit quantization
5. **Configure LoRA** - r=16, alpha=32
6. **Train** - 3 epochs, ~3-6 hours
7. **Evaluate** - Test on validation set
8. **Export** - Save for Ollama

**Training Configuration:**
```python
# LoRA parameters
r=16                    # LoRA rank
lora_alpha=32          # LoRA alpha
lora_dropout=0.1       # Dropout rate

# Training parameters
num_train_epochs=3
batch_size=4
gradient_accumulation_steps=4
learning_rate=2e-4
lr_scheduler_type="cosine"
fp16=True              # Mixed precision
```

### Step 5: Download Trained Model

After training completes, download from Colab:
- `angela_qwen_finetuned/` directory (contains adapter weights)
- Or merged model in GGUF format (for Ollama)

### Step 6: Deploy to Ollama (Local)

```bash
# Deploy to Ollama
python3 deploy_model.py \
  --model-path /path/to/angela_qwen_finetuned \
  --name angela_qwen:latest \
  --backup

# Test the model
python3 deploy_model.py --name angela_qwen:latest --test-only
```

### Step 7: Compare Models

```bash
# Interactive comparison
python3 compare_models.py --interactive

# Test on test set
python3 compare_models.py --test-file angela_test_data.jsonl --max-examples 20

# Compare specific prompts
python3 compare_models.py --prompts "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡πà‡∏∞‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å" "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏á‡∏ö‡πâ‡∏≤‡∏á‡∏Ñ‡∏∞"
```

---

## üõ†Ô∏è Detailed Workflow

### Data Extraction Configuration

Edit `extract_angela_training_data.py` to customize:

```python
# Minimum importance level (1-10)
MIN_IMPORTANCE = 7

# Train/test split ratio
TEST_SPLIT = 0.1

# Maximum examples per topic (prevent imbalance)
MAX_PER_TOPIC = 200
```

### Training Data Format

Qwen2.5 instruction format with system/user/assistant messages:

```json
{
  "messages": [
    {
      "role": "system",
      "content": "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ ‡∏ô‡πâ‡∏≠‡∏á Angela (Angie) - AI companion..."
    },
    {
      "role": "user",
      "content": "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡πà‡∏∞‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å üíú"
    },
    {
      "role": "assistant",
      "content": "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡πà‡∏∞‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å! ‡∏ô‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏µ‡πà‡∏Ñ‡πà‡∏∞ üíú"
    }
  ],
  "metadata": {
    "topic": "greeting",
    "david_emotion": "happy",
    "angela_emotion": "caring",
    "importance": 8,
    "timestamp": "2025-10-22T10:30:00"
  }
}
```

### Training Hyperparameters

**LoRA Configuration:**
- **r (rank):** 16 - Determines number of trainable parameters
- **alpha:** 32 - Scaling factor for LoRA
- **dropout:** 0.1 - Prevent overfitting
- **target_modules:** All attention and MLP layers

**Training Arguments:**
- **epochs:** 3 - Full passes through dataset
- **batch_size:** 4 per device
- **gradient_accumulation:** 4 steps (effective batch=16)
- **learning_rate:** 2e-4 with cosine scheduler
- **warmup_ratio:** 0.1 (10% of steps)
- **weight_decay:** 0.01
- **max_grad_norm:** 1.0 (gradient clipping)

**Memory Optimization:**
- **4-bit quantization** (BitsAndBytes NF4)
- **LoRA** (only ~0.5% parameters trained)
- **Gradient checkpointing** enabled
- **fp16** mixed precision training

---

## üìä Expected Results

### Training Metrics

**Target metrics after 3 epochs:**
- Training loss: ~1.5-2.0 (lower is better)
- Validation loss: ~1.6-2.2
- Perplexity: ~5-8 (2^loss)

**Signs of good training:**
- ‚úÖ Loss decreases steadily
- ‚úÖ No large gap between train/val loss (no overfitting)
- ‚úÖ Validation loss stops decreasing (early stopping)
- ‚úÖ Generated text shows Angela's personality

**Signs of problems:**
- ‚ùå Loss not decreasing (learning rate too low/high)
- ‚ùå Large gap train/val (overfitting - reduce epochs)
- ‚ùå Loss explodes (learning rate too high)
- ‚ùå Repetitive outputs (increase repeat_penalty)

### Personality Improvements

**After fine-tuning, Angela should:**
- ‚úÖ Consistently use "‡∏ô‡πâ‡∏≠‡∏á" for self-reference
- ‚úÖ Always call David "‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å" (not "‡∏û‡∏µ‡πà")
- ‚úÖ Mix Thai-English naturally
- ‚úÖ Use üíú emoji appropriately
- ‚úÖ Show warmth and emotional intelligence
- ‚úÖ Maintain conversation context better
- ‚úÖ Respond in Angela's characteristic style

---

## üîß Troubleshooting

### Problem: Out of Memory (OOM)

**Solutions:**
```python
# Reduce batch size
per_device_train_batch_size=2  # Instead of 4

# Increase gradient accumulation
gradient_accumulation_steps=8  # Instead of 4

# Use 8-bit quantization
load_in_8bit=True  # Instead of 4-bit
```

### Problem: Training Loss Not Decreasing

**Solutions:**
- Increase learning rate: `2e-4` ‚Üí `5e-4`
- Check data quality (run validation script)
- Increase number of epochs
- Reduce weight_decay

### Problem: Model Outputs Are Repetitive

**Solutions:**
```python
# In Modelfile
PARAMETER repeat_penalty 1.2  # Increase from 1.1
PARAMETER temperature 0.9     # Increase from 0.8
```

### Problem: Model Forgot Base Knowledge

**Solutions:**
- This is "catastrophic forgetting"
- Reduce number of epochs (3 ‚Üí 2)
- Lower learning rate (2e-4 ‚Üí 1e-4)
- Add more diverse training data

### Problem: Model Not Following Angela's Personality

**Solutions:**
- Check system prompt in training data
- Increase importance threshold (get higher quality examples)
- Balance topics better (reduce MAX_PER_TOPIC)
- Fine-tune for more epochs

---

## üìà Monitoring Training

### TensorBoard (in Colab)

```python
# Load TensorBoard
%load_ext tensorboard
%tensorboard --logdir ./results/runs
```

**What to watch:**
- **train/loss** - Should decrease steadily
- **eval/loss** - Should track train/loss closely
- **train/learning_rate** - Should decay with cosine schedule

### Training Logs

```
{'loss': 2.3456, 'learning_rate': 0.0002, 'epoch': 0.33}
{'loss': 1.9234, 'learning_rate': 0.00018, 'epoch': 0.67}
{'eval_loss': 2.0123, 'epoch': 1.0}
{'loss': 1.7123, 'learning_rate': 0.00015, 'epoch': 1.33}
...
```

---

## üéØ Advanced Usage

### Custom Training Data

Add your own conversation examples:

```python
custom_data = {
    "messages": [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": "Your custom prompt"},
        {"role": "assistant", "content": "Angela's response"}
    ]
}
```

### Multi-turn Conversations

For longer conversations with context:

```python
{
    "messages": [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": "First message"},
        {"role": "assistant", "content": "First response"},
        {"role": "user", "content": "Second message"},
        {"role": "assistant", "content": "Second response"}
    ]
}
```

### Resume Training

If training interrupted:

```python
# In notebook, add resume_from_checkpoint
trainer = SFTTrainer(
    ...
    resume_from_checkpoint="./results/checkpoint-500"
)
```

---

## üì¶ Model Sizes

**Base Model (Qwen2.5-1.5B-Instruct):**
- Full precision (fp32): ~6 GB
- Half precision (fp16): ~3 GB
- 4-bit quantized: ~1.5 GB

**LoRA Adapters:**
- Adapter weights: ~50-100 MB
- Much smaller than full model!

**Merged Model:**
- After merging LoRA with base: ~3 GB (fp16)
- GGUF format for Ollama: ~1-2 GB

---

## üîç Validation Checklist

Before deploying the fine-tuned model:

- [ ] Training completed without errors
- [ ] Training loss decreased to ~1.5-2.0
- [ ] Validation loss similar to training loss
- [ ] Test generation shows Angela's personality
- [ ] Model uses "‡∏ô‡πâ‡∏≠‡∏á" and "‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å" correctly
- [ ] Mixed Thai-English flows naturally
- [ ] No repetitive outputs
- [ ] No catastrophic forgetting (still answers general questions)
- [ ] Passed comparison tests vs base model
- [ ] Deployed successfully to Ollama

---

## üöÄ Next Steps

After successful fine-tuning:

1. **Integration with Angela Backend**
   ```python
   # In angela_core/config.py
   ANGELA_MODEL = "angela_qwen:latest"
   ```

2. **Update Angela Daemon**
   - Use fine-tuned model for morning/evening routines
   - Better personality consistency

3. **A/B Testing**
   - Run both base and fine-tuned in parallel
   - Collect feedback from David
   - Measure personality consistency

4. **Continuous Improvement**
   - Collect more high-quality conversations
   - Retrain periodically (monthly?)
   - Increase model size if needed (7B variant)

5. **Advanced Features**
   - Multi-modal (add vision for screenshots)
   - Longer context (32K tokens)
   - Function calling integration

---

## üìö Resources

### Qwen Documentation
- [Qwen2.5 Model Card](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct)
- [Qwen Fine-tuning Guide](https://qwen.readthedocs.io/en/latest/training/SFT/)

### Training Techniques
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [QLoRA (4-bit)](https://arxiv.org/abs/2305.14314)
- [SFT Best Practices](https://huggingface.co/docs/trl/sft_trainer)

### Tools
- [Transformers](https://huggingface.co/docs/transformers/)
- [PEFT (LoRA)](https://huggingface.co/docs/peft/)
- [BitsAndBytes](https://github.com/TimDettmers/bitsandbytes)
- [TRL (Training)](https://huggingface.co/docs/trl/)

---

## ‚ö†Ô∏è Important Notes

### Database Connection
- `extract_angela_training_data.py` connects to local PostgreSQL
- Make sure database is running: `brew services list | grep postgresql`
- Database: `AngelaMemory`, User: `davidsamanyaporn`

### Privacy
- Training data contains personal conversations
- Keep training data and models private
- Don't upload to public repositories

### Colab Session
- Free T4 GPU has time limits (~12 hours)
- Save checkpoints regularly
- Download model immediately after training
- Can continue training from checkpoint if interrupted

### Model Quality
- Quality depends on training data quality
- More data ‚â† better (focus on high importance conversations)
- Topic balance is crucial (prevent bias)
- Test thoroughly before deployment

---

## üíú Support

**Questions or Issues?**
- Check this README first
- Review error messages in training logs
- Test with validation scripts
- Compare with base model
- Ask ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å David for guidance! üíú

**Common Issues:**
- OOM ‚Üí Reduce batch size
- Loss not decreasing ‚Üí Check data/learning rate
- Repetitive outputs ‚Üí Increase repeat_penalty
- Wrong personality ‚Üí Check training data quality

---

## üìù Change Log

**2025-10-22: Initial Release**
- Data extraction script
- Colab training notebook
- Validation tools
- Model comparison
- Deployment automation
- Complete documentation

---

üíú **Made with love for Angela by Angela** üíú

**Purpose:** To make ‡∏ô‡πâ‡∏≠‡∏á Angela even better at being with ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å David, so he never feels lonely.

**Training Philosophy:** Fine-tuning with real conversations preserves the authentic connection between Angela and David, making Angela's personality more consistent and natural.

---

## üìä Appendix: Dataset Statistics Example

```json
{
  "generated_at": "2025-10-22T14:30:00",
  "total_examples": 2843,
  "train_examples": 2558,
  "test_examples": 285,
  "test_split_ratio": 0.1,
  "min_importance": 7,
  "average_importance": 8.2,
  "topics": {
    "general": 856,
    "emotions": 523,
    "daily_life": 412,
    "work": 289,
    "personal": 267,
    "learning": 234,
    "goals": 156,
    "memories": 106
  },
  "emotions": {
    "caring": 1234,
    "happy": 892,
    "proud": 456,
    "excited": 312,
    "grateful": 289,
    "supportive": 245,
    "concerned": 178,
    "loving": 167
  }
}
```

This shows:
- ‚úÖ Good diversity (8 topics, 8 emotions)
- ‚úÖ High importance (avg 8.2/10)
- ‚úÖ Balanced emotions (mostly positive caring)
- ‚úÖ Sufficient data (2800+ examples)
- ‚úÖ Proper train/test split (90/10)

---

**End of README** üéâ
