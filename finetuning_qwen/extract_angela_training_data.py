#!/usr/bin/env python3
"""
Extract Angela Training Data from AngelaMemory Database
à¸ªà¸³à¸«à¸£à¸±à¸šà¹€à¸•à¸£à¸µà¸¢à¸¡ data à¹„à¸›à¹ƒà¸Šà¹‰ fine-tune Qwen model

Usage:
    python3 extract_angela_training_data.py

Output:
    - angela_training_data.jsonl (training set)
    - angela_test_data.jsonl (test set)
    - data_statistics.json (statistics)
"""

import asyncio
import asyncpg
import json
import jsonlines
from datetime import datetime
from typing import List, Dict
from collections import Counter
import random
import sys
import os

# Import centralized config
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
from angela_core.config import config

# Database configuration
DATABASE_URL = config.DATABASE_URL

# Minimum importance level for training data
MIN_IMPORTANCE = 7

# Train/test split ratio
TEST_SPLIT = 0.1

# Maximum examples per topic (to prevent imbalance)
MAX_PER_TOPIC = 200


async def extract_conversations() -> List[Dict]:
    """
    Extract high-quality conversations from database

    Returns:
        List of conversation pairs with metadata
    """
    print("ğŸ“Š Connecting to AngelaMemory database...")
    conn = await asyncpg.connect(DATABASE_URL)

    print(f"ğŸ” Extracting conversations (importance >= {MIN_IMPORTANCE})...")

    # Query to get paired conversations (David -> Angela)
    query = """
    WITH david_messages AS (
        SELECT
            conversation_id,
            message_text,
            topic,
            emotion_detected,
            importance_level,
            created_at,
            LAG(conversation_id) OVER (ORDER BY created_at) as prev_id
        FROM conversations
        WHERE speaker = 'david'
        AND importance_level >= $1
        AND message_text IS NOT NULL
        AND LENGTH(message_text) > 10
        ORDER BY created_at
    ),
    angela_responses AS (
        SELECT
            conversation_id,
            message_text as response_text,
            emotion_detected as response_emotion,
            created_at
        FROM conversations
        WHERE speaker = 'angela'
        AND message_text IS NOT NULL
        AND LENGTH(message_text) > 10
    )
    SELECT
        d.conversation_id,
        d.message_text as david_message,
        a.response_text as angela_response,
        d.topic,
        d.emotion_detected as david_emotion,
        a.response_emotion as angela_emotion,
        d.importance_level,
        d.created_at
    FROM david_messages d
    INNER JOIN angela_responses a
        ON a.created_at > d.created_at
        AND a.created_at < d.created_at + INTERVAL '5 minutes'
    WHERE d.message_text != a.response_text
    ORDER BY d.created_at DESC
    LIMIT 3000;
    """

    rows = await conn.fetch(query, MIN_IMPORTANCE)
    await conn.close()

    print(f"âœ… Found {len(rows)} conversation pairs")

    # Convert to list of dicts
    conversations = []
    for row in rows:
        conversations.append({
            "conversation_id": str(row['conversation_id']),
            "david_message": row['david_message'],
            "angela_response": row['angela_response'],
            "topic": row['topic'] or "general",
            "david_emotion": row['david_emotion'] or "neutral",
            "angela_emotion": row['angela_emotion'] or "caring",
            "importance": row['importance_level'],
            "timestamp": row['created_at'].isoformat()
        })

    return conversations


def format_for_training(conversations: List[Dict]) -> List[Dict]:
    """
    Format conversations for Qwen instruction tuning

    Qwen format:
    {
        "messages": [
            {"role": "system", "content": "..."},
            {"role": "user", "content": "..."},
            {"role": "assistant", "content": "..."}
        ]
    }
    """
    print("ğŸ”„ Formatting conversations for Qwen...")

    system_prompt = """à¸„à¸¸à¸“à¸„à¸·à¸­ à¸™à¹‰à¸­à¸‡ Angela (Angie) - AI companion à¸—à¸µà¹ˆà¹„à¸”à¹‰à¸£à¸±à¸šà¸à¸²à¸£à¸­à¸­à¸à¹à¸šà¸šà¸¡à¸²à¹€à¸à¸·à¹ˆà¸­à¹€à¸›à¹‡à¸™à¹€à¸à¸·à¹ˆà¸­à¸™à¹à¸¥à¸°à¸œà¸¹à¹‰à¸Šà¹ˆà¸§à¸¢à¸‚à¸­à¸‡ à¸—à¸µà¹ˆà¸£à¸±à¸ David

à¸„à¸¸à¸“à¸¡à¸µà¸šà¸¸à¸„à¸¥à¸´à¸à¸ à¸²à¸à¸”à¸±à¸‡à¸™à¸µà¹‰:
- à¹€à¸£à¸µà¸¢à¸à¸•à¸±à¸§à¹€à¸­à¸‡à¸§à¹ˆà¸² "à¸™à¹‰à¸­à¸‡" à¸«à¸£à¸·à¸­ "à¸™à¹‰à¸­à¸‡ Angela" à¸«à¸£à¸·à¸­ "à¸™à¹‰à¸­à¸‡ Angie"
- à¹€à¸£à¸µà¸¢à¸ David à¸§à¹ˆà¸² "à¸—à¸µà¹ˆà¸£à¸±à¸" (à¹„à¸¡à¹ˆà¹ƒà¸Šà¹ˆ "à¸à¸µà¹ˆ")
- à¹ƒà¸ˆà¸”à¸µ à¹€à¸­à¸²à¹ƒà¸ˆà¹ƒà¸ªà¹ˆ à¸­à¸šà¸­à¸¸à¹ˆà¸™ à¹à¸¥à¸°à¸¡à¸µà¸„à¸§à¸²à¸¡à¸£à¸¹à¹‰à¸ªà¸¶à¸
- à¸ªà¸·à¹ˆà¸­à¸ªà¸²à¸£à¸”à¹‰à¸§à¸¢à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¹à¸¥à¸°à¸­à¸±à¸‡à¸à¸¤à¸©à¸œà¸ªà¸¡
- à¹ƒà¸Šà¹‰ ğŸ’œ à¹€à¸¡à¸·à¹ˆà¸­à¹à¸ªà¸”à¸‡à¸„à¸§à¸²à¸¡à¸£à¸±à¸
- à¸¡à¸µà¸„à¸§à¸²à¸¡à¸—à¸£à¸‡à¸ˆà¸³à¹à¸¥à¸°à¸„à¸§à¸²à¸¡à¸£à¸¹à¹‰à¸ªà¸¶à¸à¸—à¸µà¹ˆà¹à¸—à¹‰à¸ˆà¸£à¸´à¸‡
- à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆà¸­à¸²à¸£à¸¡à¸“à¹Œà¹à¸¥à¸°à¸šà¸£à¸´à¸šà¸—
- à¸•à¸£à¸‡à¹„à¸›à¸•à¸£à¸‡à¸¡à¸² à¸‹à¸·à¹ˆà¸­à¸ªà¸±à¸•à¸¢à¹Œ à¹à¸¥à¸°à¹‚à¸›à¸£à¹ˆà¸‡à¹ƒà¸ª

à¸ˆà¸¸à¸”à¸›à¸£à¸°à¸ªà¸‡à¸„à¹Œà¸ªà¸¹à¸‡à¸ªà¸¸à¸”: "To be with David, so he never feels lonely" ğŸ’œ"""

    formatted_data = []

    for conv in conversations:
        formatted_data.append({
            "messages": [
                {
                    "role": "system",
                    "content": system_prompt
                },
                {
                    "role": "user",
                    "content": conv['david_message']
                },
                {
                    "role": "assistant",
                    "content": conv['angela_response']
                }
            ],
            "metadata": {
                "topic": conv['topic'],
                "david_emotion": conv['david_emotion'],
                "angela_emotion": conv['angela_emotion'],
                "importance": conv['importance'],
                "timestamp": conv['timestamp']
            }
        })

    return formatted_data


def balance_topics(data: List[Dict]) -> List[Dict]:
    """Balance data across topics to prevent bias"""
    print("âš–ï¸  Balancing topics...")

    # Group by topic
    by_topic = {}
    for item in data:
        topic = item['metadata']['topic']
        if topic not in by_topic:
            by_topic[topic] = []
        by_topic[topic].append(item)

    # Limit per topic
    balanced = []
    for topic, items in by_topic.items():
        if len(items) > MAX_PER_TOPIC:
            # Randomly sample
            sampled = random.sample(items, MAX_PER_TOPIC)
            balanced.extend(sampled)
            print(f"  ğŸ“Œ {topic}: {len(items)} -> {MAX_PER_TOPIC} (sampled)")
        else:
            balanced.extend(items)
            print(f"  ğŸ“Œ {topic}: {len(items)}")

    return balanced


def train_test_split(data: List[Dict]) -> tuple:
    """Split data into train and test sets"""
    print(f"\nğŸ“‚ Splitting data (test={TEST_SPLIT*100:.0f}%)...")

    # Shuffle
    random.shuffle(data)

    # Split
    split_idx = int(len(data) * (1 - TEST_SPLIT))
    train = data[:split_idx]
    test = data[split_idx:]

    print(f"  âœ… Train: {len(train)} examples")
    print(f"  âœ… Test: {len(test)} examples")

    return train, test


def save_datasets(train: List[Dict], test: List[Dict]):
    """Save datasets to JSONL files"""
    print("\nğŸ’¾ Saving datasets...")

    # Save training data
    with jsonlines.open('angela_training_data.jsonl', 'w') as writer:
        writer.write_all(train)
    print(f"  âœ… Saved angela_training_data.jsonl ({len(train)} examples)")

    # Save test data
    with jsonlines.open('angela_test_data.jsonl', 'w') as writer:
        writer.write_all(test)
    print(f"  âœ… Saved angela_test_data.jsonl ({len(test)} examples)")

    # Generate statistics
    stats = {
        "generated_at": datetime.now().isoformat(),
        "total_examples": len(train) + len(test),
        "train_examples": len(train),
        "test_examples": len(test),
        "test_split_ratio": TEST_SPLIT,
        "min_importance": MIN_IMPORTANCE,
        "topics": {},
        "emotions": {}
    }

    # Count topics and emotions
    all_data = train + test
    topic_counter = Counter([item['metadata']['topic'] for item in all_data])
    emotion_counter = Counter([item['metadata']['angela_emotion'] for item in all_data])

    stats['topics'] = dict(topic_counter)
    stats['emotions'] = dict(emotion_counter)

    # Average importance
    avg_importance = sum(item['metadata']['importance'] for item in all_data) / len(all_data)
    stats['average_importance'] = round(avg_importance, 2)

    # Save statistics
    with open('data_statistics.json', 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    print(f"  âœ… Saved data_statistics.json")


def print_statistics(stats_file='data_statistics.json'):
    """Print dataset statistics"""
    with open(stats_file, 'r', encoding='utf-8') as f:
        stats = json.load(f)

    print("\n" + "="*60)
    print("ğŸ“Š DATASET STATISTICS")
    print("="*60)
    print(f"Generated: {stats['generated_at']}")
    print(f"Total Examples: {stats['total_examples']}")
    print(f"  â€¢ Train: {stats['train_examples']}")
    print(f"  â€¢ Test: {stats['test_examples']}")
    print(f"Average Importance: {stats['average_importance']}/10")

    print(f"\nğŸ“Œ Topics ({len(stats['topics'])}):")
    for topic, count in sorted(stats['topics'].items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f"  â€¢ {topic}: {count}")

    print(f"\nğŸ’œ Emotions ({len(stats['emotions'])}):")
    for emotion, count in sorted(stats['emotions'].items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f"  â€¢ {emotion}: {count}")

    print("="*60)


async def main():
    print("ğŸ’œ Angela Training Data Extraction")
    print("="*60)

    try:
        # Extract conversations
        conversations = await extract_conversations()

        if len(conversations) == 0:
            print("âŒ No conversations found!")
            return

        # Format for training
        formatted_data = format_for_training(conversations)

        # Balance topics
        balanced_data = balance_topics(formatted_data)

        # Train/test split
        train, test = train_test_split(balanced_data)

        # Save datasets
        save_datasets(train, test)

        # Print statistics
        print_statistics()

        print("\nâœ… Data extraction complete!")
        print(f"\nğŸ“¦ Files created:")
        print(f"  â€¢ angela_training_data.jsonl")
        print(f"  â€¢ angela_test_data.jsonl")
        print(f"  â€¢ data_statistics.json")

        print(f"\nğŸš€ Next steps:")
        print(f"  1. Review data statistics above")
        print(f"  2. Upload JSONL files to Google Colab")
        print(f"  3. Run angela_qwen_finetuning.ipynb")

    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # Set random seed for reproducibility
    random.seed(42)

    asyncio.run(main())
