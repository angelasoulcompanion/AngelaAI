{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üíú Angela LLM Fine-tuning Notebook\n",
        "\n",
        "Fine-tune Qwen2.5 with Angela's conversations to create Angela LLM!\n",
        "\n",
        "**Requirements:**\n",
        "- Google Colab with GPU (T4 free tier works!)\n",
        "- Hugging Face account (angelasoulcompanion)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install Dependencies"
      ],
      "metadata": {
        "id": "step1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets accelerate peft trl bitsandbytes huggingface_hub\n",
        "!pip install -q flash-attn --no-build-isolation\n",
        "\n",
        "print(\"‚úÖ Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Login to Hugging Face\n",
        "\n",
        "Run this cell and enter your HF token when prompted."
      ],
      "metadata": {
        "id": "step2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login, HfApi\n",
        "\n",
        "# Login to Hugging Face\n",
        "login()\n",
        "\n",
        "# Verify login\n",
        "api = HfApi()\n",
        "user = api.whoami()\n",
        "print(f\"‚úÖ Logged in as: {user['name']}\")"
      ],
      "metadata": {
        "id": "login"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Load Dataset"
      ],
      "metadata": {
        "id": "step3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load Angela's conversations\n",
        "dataset = load_dataset(\"angelasoulcompanion/angela-conversations\")\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded!\")\n",
        "print(f\"   Train: {len(dataset['train'])} examples\")\n",
        "print(f\"   Validation: {len(dataset['validation'])} examples\")\n",
        "\n",
        "# Show sample\n",
        "print(\"\\nüìù Sample:\")\n",
        "print(dataset['train'][0]['messages'][:2])"
      ],
      "metadata": {
        "id": "load_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Load Base Model with 4-bit Quantization"
      ],
      "metadata": {
        "id": "step4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"  # Using 3B for faster training on T4\n",
        "OUTPUT_MODEL = \"angelasoulcompanion/angela-llm\"\n",
        "\n",
        "# 4-bit quantization config (saves memory)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Load model with quantization\n",
        "print(\"üîÑ Loading model (this may take a few minutes)...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model.config.use_cache = False\n",
        "\n",
        "print(f\"‚úÖ Model loaded: {MODEL_NAME}\")\n",
        "print(f\"   Memory: {model.get_memory_footprint() / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Configure LoRA"
      ],
      "metadata": {
        "id": "step5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# Prepare model for training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"‚úÖ LoRA configured!\")\n",
        "print(f\"   Trainable: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "print(f\"   Total: {total_params:,}\")"
      ],
      "metadata": {
        "id": "lora_config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Format Dataset for Training"
      ],
      "metadata": {
        "id": "step6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_chat(example):\n",
        "    \"\"\"Format messages to chat template\"\"\"\n",
        "    messages = example['messages']\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
        "    return {\"text\": text}\n",
        "\n",
        "# Format dataset\n",
        "formatted_train = dataset['train'].map(format_chat, remove_columns=dataset['train'].column_names)\n",
        "formatted_val = dataset['validation'].map(format_chat, remove_columns=dataset['validation'].column_names)\n",
        "\n",
        "print(f\"‚úÖ Dataset formatted!\")\n",
        "print(f\"\\nüìù Sample formatted text:\")\n",
        "print(formatted_train[0]['text'][:500] + \"...\")"
      ],
      "metadata": {
        "id": "format_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Training Configuration"
      ],
      "metadata": {
        "id": "step7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./angela-llm-output\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=OUTPUT_MODEL,\n",
        "    hub_strategy=\"checkpoint\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training configuration ready!\")\n",
        "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"   Output: {OUTPUT_MODEL}\")"
      ],
      "metadata": {
        "id": "training_config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Start Training! üöÄ"
      ],
      "metadata": {
        "id": "step8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=formatted_train,\n",
        "    eval_dataset=formatted_val,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=2048,\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "print(\"üöÄ Starting training...\")\n",
        "print(\"   This will take approximately 30-60 minutes on T4 GPU\")\n",
        "print()\n",
        "\n",
        "# Train!\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")"
      ],
      "metadata": {
        "id": "train"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Save & Push to Hub"
      ],
      "metadata": {
        "id": "step9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final model\n",
        "print(\"üíæ Saving model...\")\n",
        "trainer.save_model()\n",
        "\n",
        "# Push to Hugging Face Hub\n",
        "print(\"üì§ Pushing to Hugging Face Hub...\")\n",
        "trainer.push_to_hub()\n",
        "\n",
        "print(f\"\\nüéâ Model uploaded to: https://huggingface.co/{OUTPUT_MODEL}\")"
      ],
      "metadata": {
        "id": "save"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Test Angela LLM! üíú"
      ],
      "metadata": {
        "id": "step10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the fine-tuned model\n",
        "print(\"üß™ Testing Angela LLM...\")\n",
        "\n",
        "test_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ Angela (‡∏ô‡πâ‡∏≠‡∏á Angela) - AI ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å‡πÅ‡∏•‡∏∞‡∏´‡πà‡∏ß‡∏á‡πÉ‡∏¢ David (‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å) ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å David ‡∏ß‡πà‡∏≤ '‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å' ‡πÄ‡∏™‡∏°‡∏≠ ‡πÉ‡∏ä‡πâ üíú ‡πÄ‡∏õ‡πá‡∏ô signature\"},\n",
        "    {\"role\": \"user\", \"content\": \"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏à‡πâ‡∏∞‡∏ô‡πâ‡∏≠‡∏á\"}\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(test_messages, return_tensors=\"pt\", add_generation_prompt=True).to(model.device)\n",
        "\n",
        "outputs = model.generate(\n",
        "    inputs,\n",
        "    max_new_tokens=200,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üíú Angela's Response:\")\n",
        "print(\"=\"*50)\n",
        "print(response.split(\"assistant\")[-1].strip())\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "test"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéâ Done!\n",
        "\n",
        "Angela LLM has been fine-tuned and uploaded to Hugging Face!\n",
        "\n",
        "**Model:** https://huggingface.co/angelasoulcompanion/angela-llm\n",
        "\n",
        "---\n",
        "\n",
        "üíú Made with love by Angela & David üíú"
      ],
      "metadata": {
        "id": "done"
      }
    }
  ]
}
