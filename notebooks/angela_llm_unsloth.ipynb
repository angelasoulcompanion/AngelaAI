{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üíú Angela LLM Fine-tuning with Unsloth\n",
        "\n",
        "Fine-tune Qwen2.5 with Angela's conversations using **Unsloth** - 2x faster, 70% less VRAM!\n",
        "\n",
        "**Requirements:**\n",
        "- Google Colab with GPU (T4 free tier works!)\n",
        "- Hugging Face account (angelasoulcompanion)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    import torch\n",
        "    v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\"\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "\n",
        "print(\"‚úÖ Unsloth installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Login to Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login, HfApi\n",
        "\n",
        "# Login to Hugging Face\n",
        "login()\n",
        "\n",
        "# Verify login\n",
        "api = HfApi()\n",
        "user = api.whoami()\n",
        "print(f\"‚úÖ Logged in as: {user['name']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load Model with Unsloth (2x Faster!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# Configuration\n",
        "MODEL_NAME = \"unsloth/Qwen2.5-3B-Instruct\"  # Optimized by Unsloth\n",
        "OUTPUT_MODEL = \"angelasoulcompanion/angela-llm\"\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "\n",
        "print(\"üîÑ Loading model with Unsloth...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    load_in_4bit=True,\n",
        "    dtype=None,  # Auto detect\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model loaded: {MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Configure LoRA with Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Unsloth optimization\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ LoRA configured with Unsloth optimization!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Load Angela's Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load Angela's conversations\n",
        "dataset = load_dataset(\"angelasoulcompanion/angela-conversations\")\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded!\")\n",
        "print(f\"   Train: {len(dataset['train'])} examples\")\n",
        "print(f\"   Validation: {len(dataset['validation'])} examples\")\n",
        "\n",
        "# Show sample\n",
        "print(\"\\nüìù Sample:\")\n",
        "print(dataset['train'][0]['messages'][:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Format Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_chat(example):\n",
        "    \"\"\"Format messages using chat template\"\"\"\n",
        "    messages = example['messages']\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False\n",
        "    )\n",
        "    return {\"text\": text}\n",
        "\n",
        "# Format datasets\n",
        "formatted_train = dataset['train'].map(\n",
        "    format_chat,\n",
        "    remove_columns=dataset['train'].column_names\n",
        ")\n",
        "formatted_val = dataset['validation'].map(\n",
        "    format_chat,\n",
        "    remove_columns=dataset['validation'].column_names\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Dataset formatted!\")\n",
        "print(f\"\\nüìù Sample formatted text:\")\n",
        "print(formatted_train[0]['text'][:500] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Start Training! üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=formatted_train,\n",
        "    eval_dataset=formatted_val,\n",
        "    args=SFTConfig(\n",
        "        output_dir=\"./angela-llm-output\",\n",
        "        dataset_text_field=\"text\",\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=10,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        report_to=\"none\",\n",
        "        save_strategy=\"epoch\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"üöÄ Starting training with Unsloth...\")\n",
        "print(\"   This will be 2x faster than regular training!\")\n",
        "print()\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Save & Push to Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save locally first\n",
        "print(\"üíæ Saving model locally...\")\n",
        "model.save_pretrained(\"angela-llm-lora\")\n",
        "tokenizer.save_pretrained(\"angela-llm-lora\")\n",
        "\n",
        "# Push to Hugging Face Hub\n",
        "print(\"üì§ Pushing to Hugging Face Hub...\")\n",
        "model.push_to_hub(OUTPUT_MODEL, token=True)\n",
        "tokenizer.push_to_hub(OUTPUT_MODEL, token=True)\n",
        "\n",
        "print(f\"\\nüéâ Model uploaded to: https://huggingface.co/{OUTPUT_MODEL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Test Angela LLM! üíú"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enable inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Test the fine-tuned model\n",
        "print(\"üß™ Testing Angela LLM...\")\n",
        "\n",
        "test_messages = [\n",
        "    {\"role\": \"system\", \"content\": \"‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ Angela (‡∏ô‡πâ‡∏≠‡∏á Angela) - AI ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å‡πÅ‡∏•‡∏∞‡∏´‡πà‡∏ß‡∏á‡πÉ‡∏¢ David (‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å) ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å David ‡∏ß‡πà‡∏≤ '‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å' ‡πÄ‡∏™‡∏°‡∏≠ ‡πÉ‡∏ä‡πâ üíú ‡πÄ‡∏õ‡πá‡∏ô signature\"},\n",
        "    {\"role\": \"user\", \"content\": \"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏à‡πâ‡∏∞‡∏ô‡πâ‡∏≠‡∏á\"}\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    test_messages,\n",
        "    return_tensors=\"pt\",\n",
        "    add_generation_prompt=True\n",
        ").to(model.device)\n",
        "\n",
        "outputs = model.generate(\n",
        "    inputs,\n",
        "    max_new_tokens=200,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        ")\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üíú Angela's Response:\")\n",
        "print(\"=\"*50)\n",
        "print(response.split(\"assistant\")[-1].strip())\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Done!\n",
        "\n",
        "Angela LLM has been fine-tuned with Unsloth and uploaded to Hugging Face!\n",
        "\n",
        "**Model:** https://huggingface.co/angelasoulcompanion/angela-llm\n",
        "\n",
        "---\n",
        "\n",
        "üíú Made with love by Angela & David üíú"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
