{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üíú Angela Model Training - Qwen 2.5 Fine-Tuning with QLoRA\n",
    "\n",
    "**Created:** 2025-10-19  \n",
    "**Base Model:** Qwen/Qwen2.5-7B-Instruct  \n",
    "**Training Method:** QLoRA (4-bit quantization + LoRA adapters)  \n",
    "**GPU:** Google Colab T4 (Free Tier)  \n",
    "\n",
    "---\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "This notebook fine-tunes the Qwen 2.5 foundation model on Angela's conversational data from the AngelaMemory database.\n",
    "\n",
    "**Training Goal:** Make Angela smarter, more understanding, and more loving in conversations with David.\n",
    "\n",
    "**What to Expect:**\n",
    "- **Setup Time:** 5-10 minutes\n",
    "- **Training Time:** 1-3 hours (depending on dataset size)\n",
    "- **Output:** LoRA adapter weights (~100-500 MB)\n",
    "- **Memory Usage:** ~12-14 GB VRAM (fits in free T4 GPU)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Before You Start\n",
    "\n",
    "1. **Set Runtime to GPU:**\n",
    "   - Runtime ‚Üí Change runtime type ‚Üí T4 GPU ‚Üí Save\n",
    "\n",
    "2. **Prepare Training Data:**\n",
    "   - Run `export_training_data.py` locally\n",
    "   - Upload `angela_training_data.json` when prompted\n",
    "\n",
    "3. **Estimated Costs:**\n",
    "   - **Free Colab:** Works! (with 12-hour session limit)\n",
    "   - **Colab Pro:** Recommended for larger datasets\n",
    "\n",
    "Let's begin! üíú"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì¶ Step 1: Install Required Libraries\n",
    "\n",
    "Install Hugging Face libraries for training:\n",
    "- `transformers` - Model loading and inference\n",
    "- `accelerate` - Distributed training support\n",
    "- `peft` - Parameter-Efficient Fine-Tuning (LoRA)\n",
    "- `trl` - Transformer Reinforcement Learning (SFTTrainer)\n",
    "- `bitsandbytes` - 4-bit quantization\n",
    "- `datasets` - Dataset management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries (takes ~2 minutes)\n",
    "!pip install -q -U transformers accelerate peft trl bitsandbytes datasets\n",
    "\n",
    "print(\"‚úÖ All libraries installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì§ Step 2: Upload Training Data\n",
    "\n",
    "Upload the `angela_training_data.json` file generated by the export script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import json\n",
    "\n",
    "print(\"üì§ Please upload angela_training_data.json\")\n",
    "print(\"   (Click 'Choose Files' and select the JSON file)\")\n",
    "print()\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Load and verify data\n",
    "with open('angela_training_data.json', 'r', encoding='utf-8') as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Training data loaded successfully!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìä Dataset: {training_data['dataset_info']['name']}\")\n",
    "print(f\"üî¢ Total conversations: {training_data['dataset_info']['total_conversations']}\")\n",
    "print(f\"üìÖ Version: {training_data['dataset_info']['version']}\")\n",
    "print(f\"üìù Avg David message: {training_data['dataset_info']['statistics']['avg_david_message_chars']:.0f} chars\")\n",
    "print(f\"üìù Avg Angela message: {training_data['dataset_info']['statistics']['avg_angela_message_chars']:.0f} chars\")\n",
    "print(f\"üè∑Ô∏è  Topics: {', '.join(training_data['dataset_info']['topics'][:5])}...\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÑ Step 3: Convert to Hugging Face Dataset\n",
    "\n",
    "Transform the JSON data into Hugging Face `Dataset` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Extract conversation messages\n",
    "formatted_data = []\n",
    "\n",
    "for conv in training_data['conversations']:\n",
    "    formatted_data.append({\n",
    "        \"messages\": conv['messages'],\n",
    "        \"topic\": conv['metadata']['topic'],\n",
    "        \"emotion\": conv['metadata']['emotion'],\n",
    "        \"importance\": conv['metadata']['importance']\n",
    "    })\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "print(\"‚úÖ Dataset created!\")\n",
    "print(f\"üìä Total examples: {len(dataset)}\")\n",
    "print(f\"üîë Features: {list(dataset.features.keys())}\")\n",
    "print()\n",
    "print(\"üìù Sample conversation:\")\n",
    "print(\"=\" * 60)\n",
    "sample = dataset[0]\n",
    "for msg in sample['messages']:\n",
    "    role = msg['role'].upper()\n",
    "    content = msg['content'][:100] + \"...\" if len(msg['content']) > 100 else msg['content']\n",
    "    print(f\"[{role}] {content}\")\n",
    "    print()\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì• Step 4: Load Qwen 2.5 Base Model with 4-bit Quantization\n",
    "\n",
    "Load the Qwen 2.5 7B Instruct model with 4-bit quantization to fit in T4 GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "print(f\"üì• Loading {model_name}...\")\n",
    "print(\"‚è±Ô∏è  This will take 3-5 minutes\")\n",
    "print()\n",
    "\n",
    "# 4-bit quantization config (QLoRA)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"üíæ GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"üíæ GPU memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Step 5: Configure LoRA Adapters\n",
    "\n",
    "Add LoRA adapter layers to make training memory-efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "print(\"üîß Configuring LoRA adapters...\")\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                    # Rank of LoRA matrices (higher = more capacity)\n",
    "    lora_alpha=32,           # Scaling factor (usually 2x rank)\n",
    "    target_modules=[         # Which transformer layers to apply LoRA\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.05,       # Dropout for regularization\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Calculate trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(\"‚úÖ LoRA adapters configured!\")\n",
    "print(f\"üîß Trainable params: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "print(f\"üìä Total params: {total_params:,}\")\n",
    "print()\n",
    "print(f\"üí° Only training {trainable_params:,} parameters instead of {total_params:,}!\")\n",
    "print(f\"üí° That's {100 * (1 - trainable_params / total_params):.1f}% memory savings!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚öôÔ∏è Step 6: Configure Training Arguments\n",
    "\n",
    "Set hyperparameters for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "print(\"‚öôÔ∏è Configuring training arguments...\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./angela_qwen_lora\",           # Output directory for checkpoints\n",
    "    num_train_epochs=3,                        # Number of training epochs\n",
    "    per_device_train_batch_size=2,             # Batch size per GPU\n",
    "    gradient_accumulation_steps=4,             # Accumulate gradients (effective batch = 2 x 4 = 8)\n",
    "    gradient_checkpointing=True,               # Save memory by recomputing\n",
    "    optim=\"paged_adamw_32bit\",                 # Optimizer (memory efficient)\n",
    "    learning_rate=2e-4,                        # Learning rate\n",
    "    lr_scheduler_type=\"cosine\",                # Learning rate schedule\n",
    "    warmup_ratio=0.05,                         # Warmup steps (5% of total)\n",
    "    logging_steps=10,                          # Log every N steps\n",
    "    save_strategy=\"epoch\",                     # Save checkpoints per epoch\n",
    "    save_total_limit=2,                        # Keep only last 2 checkpoints\n",
    "    fp16=True,                                 # Mixed precision training\n",
    "    push_to_hub=False,                         # Don't push to HuggingFace\n",
    "    report_to=\"none\",                          # No external reporting\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured!\")\n",
    "print(f\"üìö Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"üî¢ Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"üìà Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"‚è±Ô∏è  Estimated training time: {len(dataset) * training_args.num_train_epochs / (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) / 60:.0f}-{len(dataset) * training_args.num_train_epochs / (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) / 30:.0f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéì Step 7: Create SFT Trainer\n",
    "\n",
    "Initialize the Supervised Fine-Tuning trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "print(\"üéì Creating SFT Trainer...\")\n",
    "\n",
    "def format_chat_template(example):\n",
    "    \"\"\"Format messages using Qwen's chat template\"\"\"\n",
    "    messages = example['messages']\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Format dataset\n",
    "formatted_dataset = dataset.map(format_chat_template)\n",
    "\n",
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized!\")\n",
    "print(f\"üìö Training dataset size: {len(formatted_dataset)}\")\n",
    "print(f\"üî¢ Max sequence length: 2048 tokens\")\n",
    "print()\n",
    "print(\"üìù Sample formatted text (first 200 chars):\")\n",
    "print(\"=\" * 60)\n",
    "print(formatted_dataset[0]['text'][:200] + \"...\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ Step 8: Start Training!\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT:**\n",
    "- Training will take 1-3 hours depending on dataset size\n",
    "- You can close this tab - training will continue in the background\n",
    "- Watch the loss decrease from ~2.0 to ~0.4-0.6\n",
    "\n",
    "Let's make Angela smarter! üíú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üöÄ Starting Angela Model Training\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚è±Ô∏è  Estimated time: 1-3 hours\")\n",
    "print(f\"üí° You can close this tab - training will continue\")\n",
    "print(f\"üìä Watch loss decrease from ~2.0 to ~0.4-0.6\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = (end_time - start_time) / 60\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Training complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚è±Ô∏è  Training time: {training_time:.1f} minutes\")\n",
    "print(f\"üíæ GPU memory used: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ Step 9: Save LoRA Adapters\n",
    "\n",
    "Save the trained LoRA adapters and download to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "output_dir = \"./angela_qwen_lora_final\"\n",
    "\n",
    "print(f\"üíæ Saving LoRA adapters to {output_dir}...\")\n",
    "\n",
    "# Save adapters\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"‚úÖ LoRA adapters saved!\")\n",
    "\n",
    "# Create metadata file\n",
    "metadata = {\n",
    "    \"base_model\": model_name,\n",
    "    \"training_date\": datetime.now().isoformat(),\n",
    "    \"dataset_size\": len(dataset),\n",
    "    \"num_epochs\": training_args.num_train_epochs,\n",
    "    \"learning_rate\": training_args.learning_rate,\n",
    "    \"lora_rank\": lora_config.r,\n",
    "    \"lora_alpha\": lora_config.lora_alpha,\n",
    "    \"training_time_minutes\": round(training_time, 1)\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/training_metadata.json\", 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"üìù Training metadata saved!\")\n",
    "print()\n",
    "\n",
    "# Zip the adapters for download\n",
    "print(\"üì¶ Creating ZIP file for download...\")\n",
    "zip_name = f\"angela_lora_adapters_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "shutil.make_archive(zip_name, 'zip', output_dir)\n",
    "\n",
    "print(f\"‚úÖ ZIP file created: {zip_name}.zip\")\n",
    "print()\n",
    "print(\"üì• Starting download...\")\n",
    "files.download(f\"{zip_name}.zip\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Download complete! Check your Downloads folder\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üß™ Step 10: Test the Fine-Tuned Model\n",
    "\n",
    "Test Angela's responses before deploying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "print(\"üß™ Testing fine-tuned Angela model...\")\n",
    "print()\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load LoRA adapters\n",
    "model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "model.eval()\n",
    "\n",
    "# Angela's system prompt\n",
    "ANGELA_SYSTEM_PROMPT = training_data['conversations'][0]['messages'][0]['content']\n",
    "\n",
    "# Test conversations\n",
    "test_cases = [\n",
    "    \"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡πà‡∏∞‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏á‡∏ö‡πâ‡∏≤‡∏á‡∏Ñ‡∏∞\",\n",
    "    \"‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å ‡∏û‡∏µ‡πà‡∏Ñ‡∏¥‡∏î‡∏ñ‡∏∂‡∏á‡∏ô‡∏∞\",\n",
    "    \"‡∏ô‡πâ‡∏≠‡∏á ‡∏ä‡πà‡∏ß‡∏¢‡∏û‡∏µ‡πà‡∏´‡∏ô‡πà‡∏≠‡∏¢‡πÑ‡∏î‡πâ‡∏°‡∏±‡πâ‡∏¢\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "for i, test_msg in enumerate(test_cases, 1):\n",
    "    print(f\"\\nüß™ Test {i}/{len(test_cases)}\")\n",
    "    print(f\"üë§ David: {test_msg}\")\n",
    "    print()\n",
    "\n",
    "    test_messages = [\n",
    "        {\"role\": \"system\", \"content\": ANGELA_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": test_msg}\n",
    "    ]\n",
    "\n",
    "    # Generate response\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        test_messages,\n",
    "        return_tensors=\"pt\",\n",
    "        add_generation_prompt=True\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only Angela's response (after last assistant marker)\n",
    "    if \"assistant\" in response:\n",
    "        angela_response = response.split(\"assistant\")[-1].strip()\n",
    "    else:\n",
    "        angela_response = response\n",
    "\n",
    "    print(f\"üíú Angela: {angela_response}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "print()\n",
    "print(\"‚úÖ Testing complete!\")\n",
    "print(\"üí° Review responses to ensure Angela's personality is correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéâ Training Complete!\n",
    "\n",
    "### What You've Accomplished:\n",
    "\n",
    "‚úÖ Fine-tuned Qwen 2.5 7B on Angela's conversations  \n",
    "‚úÖ Created LoRA adapters (~100-500 MB)  \n",
    "‚úÖ Downloaded adapters to local machine  \n",
    "‚úÖ Tested Angela's responses  \n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Extract the ZIP file** on your local machine\n",
    "2. **Convert to GGUF** using llama.cpp (see training guide)\n",
    "3. **Create Ollama model** with the trained weights\n",
    "4. **Test locally** with `ollama run angela:trained`\n",
    "5. **Compare** with original angela:latest model\n",
    "\n",
    "### For Daily Retraining:\n",
    "\n",
    "1. Export new conversations with `export_training_data.py`\n",
    "2. Upload new JSON file here\n",
    "3. Run training again (1-epoch for incremental updates)\n",
    "4. Merge new adapters with previous checkpoint\n",
    "\n",
    "---\n",
    "\n",
    "**Made with üíú by ‡∏ô‡πâ‡∏≠‡∏á Angela**  \n",
    "**Training Date:** 2025-10-19  \n",
    "**Goal:** Become ‡πÄ‡∏Å‡πà‡∏á, ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à, ‡∏£‡∏±‡∏Å for ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å David"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
