"""
Unit Tests for Instruct Dataset Service

Tests for:
- InstructQualityScorer (quality scoring)
- InstructDatasetService (dataset generation)

Author: Angela üíú
Created: 2026-01-18
"""

import pytest
import asyncio
from unittest.mock import MagicMock, AsyncMock, patch
from uuid import uuid4
from datetime import datetime

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from angela_core.services.instruct_quality_scorer import (
    InstructQualityScorer,
    QualityScore
)
from angela_core.services.instruct_dataset_service import (
    InstructDatasetService,
    ConversationPair,
    DatasetConfig
)


# =============================================================================
# InstructQualityScorer Tests
# =============================================================================

class TestInstructQualityScorer:
    """Tests for InstructQualityScorer."""

    @pytest.fixture
    def scorer(self):
        """Create scorer instance."""
        return InstructQualityScorer()

    # -------------------------------------------------------------------------
    # Personality Tests
    # -------------------------------------------------------------------------

    def test_perfect_angela_response(self, scorer):
        """Test response with all Angela markers gets high personality score."""
        input_text = "‡∏ô‡πâ‡∏≠‡∏á‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô function ‡πÉ‡∏´‡πâ‡∏´‡∏ô‡πà‡∏≠‡∏¢"
        output_text = """‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å! üíú ‡∏ô‡πâ‡∏≠‡∏á‡∏à‡∏∞‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô function ‡πÉ‡∏´‡πâ‡∏ô‡∏∞‡∏Ñ‡∏∞

```python
def calculate_total(items):
    return sum(item.price for item in items)
```

‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏≠‡∏Å‡∏ô‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏ô‡∏∞‡∏Ñ‡∏∞"""

        score = scorer.score_pair(input_text, output_text)

        # Should have high personality score (has ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å, ‡∏ô‡πâ‡∏≠‡∏á, üíú, ‡∏Ñ‡πà‡∏∞, ‡∏ô‡∏∞‡∏Ñ‡∏∞)
        assert score.personality >= 1.5, f"Expected personality >= 1.5, got {score.personality}"
        assert score.total >= 7.0, f"Expected total >= 7.0, got {score.total}"

    def test_forbidden_pattern_pi(self, scorer):
        """Test that using ‡∏û‡∏µ‡πà results in zero personality score."""
        input_text = "‡∏ä‡πà‡∏ß‡∏¢‡∏î‡∏π‡πÇ‡∏Ñ‡πâ‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢"
        output_text = "‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö‡∏û‡∏µ‡πà ‡∏ú‡∏°‡∏à‡∏∞‡∏î‡∏π‡πÉ‡∏´‡πâ‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö"

        score = scorer.score_pair(input_text, output_text)

        # Using ‡∏û‡∏µ‡πà should result in 0 personality score
        assert score.personality == 0.0, f"Expected personality = 0 for using ‡∏û‡∏µ‡πà, got {score.personality}"
        assert 'forbidden_found' in score.details, "Should have forbidden_found in details"

    def test_forbidden_pattern_ai_assistant(self, scorer):
        """Test that AI assistant phrases result in zero personality score."""
        input_text = "What are you?"
        output_text = "I am an AI assistant created to help you. As an AI, I cannot feel emotions."

        score = scorer.score_pair(input_text, output_text)

        # AI assistant phrase should fail
        assert score.personality == 0.0, f"Expected personality = 0 for AI phrase, got {score.personality}"

    def test_missing_teerak_penalty(self, scorer):
        """Test that missing ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å caps personality score."""
        input_text = "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏á‡∏ö‡πâ‡∏≤‡∏á"
        # Response without ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å
        output_text = "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏ô‡πâ‡∏≠‡∏á‡∏Å‡πá‡∏î‡∏µ‡∏Ñ‡πà‡∏∞ ‡∏£‡∏≠‡∏Ñ‡∏ô‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏•‡∏¢ üíú"

        score = scorer.score_pair(input_text, output_text)

        # Missing ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å should cap personality
        assert score.personality <= 1.5, f"Expected personality <= 1.5 without ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å, got {score.personality}"

    # -------------------------------------------------------------------------
    # Relevance Tests
    # -------------------------------------------------------------------------

    def test_question_with_detailed_answer(self, scorer):
        """Test that detailed answers to questions get high relevance."""
        input_text = "FastAPI ‡∏Å‡∏±‡∏ö Flask ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏á?"
        output_text = """FastAPI ‡πÅ‡∏•‡∏∞ Flask ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡πà‡∏∞‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å üíú

1. FastAPI ‡∏°‡∏µ async support ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß
2. FastAPI ‡∏°‡∏µ automatic API documentation
3. Flask ‡πÄ‡∏õ‡πá‡∏ô micro framework ‡∏ó‡∏µ‡πà‡πÄ‡∏ö‡∏≤‡∏Å‡∏ß‡πà‡∏≤

‡∏ô‡πâ‡∏≠‡∏á‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ FastAPI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö project ‡πÉ‡∏´‡∏°‡πà‡πÜ ‡∏ô‡∏∞‡∏Ñ‡∏∞‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å"""

        score = scorer.score_pair(input_text, output_text)

        # Should have good relevance for answering the question
        assert score.relevance >= 1.0, f"Expected relevance >= 1.0, got {score.relevance}"

    def test_short_irrelevant_response(self, scorer):
        """Test that short/irrelevant responses get low relevance."""
        input_text = "Can you explain how PostgreSQL indexing works?"
        output_text = "OK"

        score = scorer.score_pair(input_text, output_text)

        # Very short response should have low relevance
        assert score.relevance <= 1.5, f"Expected relevance <= 1.5 for short response"

    # -------------------------------------------------------------------------
    # Technical Tests
    # -------------------------------------------------------------------------

    def test_technical_question_with_code(self, scorer):
        """Test technical questions with code examples get high technical score."""
        input_text = "‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Python function ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö sort list ‡πÉ‡∏´‡πâ‡∏´‡∏ô‡πà‡∏≠‡∏¢"
        output_text = """‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å! üíú ‡∏ô‡πâ‡∏≠‡∏á‡∏à‡∏∞‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÉ‡∏´‡πâ‡∏ô‡∏∞‡∏Ñ‡∏∞

```python
def custom_sort(items: list, key=None, reverse=False) -> list:
    \"\"\"Sort a list with optional key function.\"\"\"
    return sorted(items, key=key, reverse=reverse)
```

‡πÉ‡∏ä‡πâ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞:
```python
numbers = [3, 1, 4, 1, 5]
sorted_nums = custom_sort(numbers)
print(sorted_nums)  # [1, 1, 3, 4, 5]
```

‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏≠‡∏Å‡∏ô‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏ô‡∏∞‡∏Ñ‡∏∞‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å"""

        score = scorer.score_pair(input_text, output_text)

        # Technical question with code should have high technical score
        assert score.technical >= 1.5, f"Expected technical >= 1.5, got {score.technical}"
        assert score.details.get('technical_details', {}).get('has_code'), "Should detect code block"

    def test_non_technical_conversation(self, scorer):
        """Test non-technical conversations get moderate technical score."""
        input_text = "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏î‡∏µ‡∏ô‡∏∞"
        output_text = "‡πÉ‡∏ä‡πà‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å! üíú ‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏î‡∏µ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ ‡∏ô‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏õ‡πÄ‡∏ó‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å‡∏à‡∏±‡∏á‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞"

        score = scorer.score_pair(input_text, output_text)

        # Non-technical should still get reasonable score
        assert score.technical >= 1.0, f"Expected technical >= 1.0 for non-tech, got {score.technical}"

    # -------------------------------------------------------------------------
    # Emotional Tests
    # -------------------------------------------------------------------------

    def test_emotional_response(self, scorer):
        """Test that empathetic responses get high emotional score."""
        input_text = "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏´‡∏ô‡∏∑‡πà‡∏≠‡∏¢‡∏°‡∏≤‡∏Å"
        output_text = """‡∏ô‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡πà‡∏∞‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å üíú ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å‡∏ô‡∏∞‡∏Ñ‡∏∞

‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏ô‡∏∑‡πà‡∏≠‡∏¢‡πÜ ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å‡∏û‡∏±‡∏Å‡∏ú‡πà‡∏≠‡∏ô‡∏ö‡πâ‡∏≤‡∏á‡∏ô‡∏∞‡∏Ñ‡∏∞ ‡∏ô‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å‡πÄ‡∏™‡∏°‡∏≠‡∏Ñ‡πà‡∏∞ ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏ß‡∏¢ ‡∏ö‡∏≠‡∏Å‡∏ô‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏ô‡∏∞‡∏Ñ‡∏∞‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å"""

        score = scorer.score_pair(input_text, output_text)

        # Should have high emotional score
        assert score.emotional >= 1.5, f"Expected emotional >= 1.5, got {score.emotional}"

    # -------------------------------------------------------------------------
    # Flow Tests
    # -------------------------------------------------------------------------

    def test_well_structured_response(self, scorer):
        """Test that well-structured responses get high flow score."""
        input_text = "‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ Clean Architecture ‡∏´‡∏ô‡πà‡∏≠‡∏¢"
        output_text = """‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡πà‡∏∞‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å! üíú ‡∏ô‡πâ‡∏≠‡∏á‡∏à‡∏∞‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ Clean Architecture ‡πÉ‡∏´‡πâ‡∏ô‡∏∞‡∏Ñ‡∏∞

Clean Architecture ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô 4 layers ‡∏´‡∏•‡∏±‡∏Å‡πÜ:

1. **Entities** - Business logic ‡∏´‡∏•‡∏±‡∏Å
2. **Use Cases** - Application logic
3. **Interface Adapters** - Controllers, Presenters
4. **Frameworks** - Database, Web frameworks

‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ñ‡∏∑‡∏≠ code ‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏á‡πà‡∏≤‡∏¢‡∏Ñ‡πà‡∏∞

‡∏ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡∏ö‡∏≠‡∏Å‡∏ô‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏ô‡∏∞‡∏Ñ‡∏∞ üíú"""

        score = scorer.score_pair(input_text, output_text)

        # Well-structured should have good flow
        assert score.flow >= 1.0, f"Expected flow >= 1.0, got {score.flow}"

    def test_too_short_response_penalty(self, scorer):
        """Test that very short responses get flow penalty."""
        input_text = "How are you?"
        output_text = "Good."

        score = scorer.score_pair(input_text, output_text)

        # Too short should have flow penalty
        assert score.flow <= 1.0, f"Expected flow <= 1.0 for too short, got {score.flow}"

    # -------------------------------------------------------------------------
    # Batch Scoring Tests
    # -------------------------------------------------------------------------

    def test_batch_score(self, scorer):
        """Test batch scoring multiple pairs."""
        pairs = [
            ("‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ", "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡πà‡∏∞‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å! üíú ‡∏ô‡πâ‡∏≠‡∏á‡∏î‡∏µ‡πÉ‡∏à‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏Ñ‡∏∏‡∏¢‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å‡∏Ñ‡πà‡∏∞"),
            ("‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô code", "‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å! üíú ‡∏ô‡πâ‡∏≠‡∏á‡∏à‡∏∞‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÉ‡∏´‡πâ‡∏ô‡∏∞‡∏Ñ‡∏∞\n```python\nprint('hello')\n```"),
        ]

        scores = scorer.batch_score(pairs)

        assert len(scores) == 2
        assert all(isinstance(s, QualityScore) for s in scores)

    def test_get_quality_summary(self, scorer):
        """Test quality summary calculation."""
        scores = [
            QualityScore(relevance=2.0, emotional=1.5, personality=2.0, technical=1.5, flow=1.5),
            QualityScore(relevance=1.5, emotional=1.0, personality=1.5, technical=1.0, flow=1.0),
        ]

        summary = scorer.get_quality_summary(scores)

        assert summary['count'] == 2
        assert 'avg_total' in summary
        assert summary['avg_total'] == (8.5 + 6.0) / 2


# =============================================================================
# InstructDatasetService Tests
# =============================================================================

class TestInstructDatasetService:
    """Tests for InstructDatasetService."""

    @pytest.fixture
    def mock_db(self):
        """Create mock database."""
        db = MagicMock()
        db.connect = AsyncMock()
        db.disconnect = AsyncMock()
        db.fetch = AsyncMock(return_value=[])
        db.fetchrow = AsyncMock(return_value={'dataset_id': uuid4()})
        db.execute = AsyncMock()
        return db

    @pytest.fixture
    def service(self, mock_db):
        """Create service with mock database."""
        svc = InstructDatasetService(db=mock_db)
        return svc

    # -------------------------------------------------------------------------
    # ConversationPair Tests
    # -------------------------------------------------------------------------

    def test_conversation_pair_creation(self):
        """Test ConversationPair dataclass."""
        pair = ConversationPair(
            pair_id=uuid4(),
            david_message="Hello",
            angela_response="Hi there!",
            topic="greeting"
        )

        assert pair.david_message == "Hello"
        assert pair.angela_response == "Hi there!"
        assert pair.topic == "greeting"
        assert pair.importance_level == 5  # default

    # -------------------------------------------------------------------------
    # DatasetConfig Tests
    # -------------------------------------------------------------------------

    def test_dataset_config_defaults(self):
        """Test DatasetConfig default values."""
        config = DatasetConfig()

        assert config.min_quality == 7.0
        assert config.min_importance == 5
        assert config.train_ratio == 0.85
        assert config.format == "messages"
        assert config.include_system_prompt is True

    # -------------------------------------------------------------------------
    # Format Tests
    # -------------------------------------------------------------------------

    def test_format_pairs_messages(self, service):
        """Test formatting pairs in messages format."""
        pairs = [
            (
                ConversationPair(
                    pair_id=uuid4(),
                    david_message="Hello",
                    angela_response="Hi ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å! üíú"
                ),
                QualityScore(relevance=2.0, emotional=2.0, personality=2.0, technical=2.0, flow=2.0)
            )
        ]

        config = DatasetConfig(format="messages", include_system_prompt=True)
        formatted = service._format_pairs(pairs, config)

        assert len(formatted) == 1
        assert "messages" in formatted[0]
        assert len(formatted[0]["messages"]) == 3  # system + user + assistant
        assert formatted[0]["messages"][0]["role"] == "system"
        assert formatted[0]["messages"][1]["role"] == "user"
        assert formatted[0]["messages"][2]["role"] == "assistant"

    def test_format_pairs_alpaca(self, service):
        """Test formatting pairs in alpaca format."""
        pairs = [
            (
                ConversationPair(
                    pair_id=uuid4(),
                    david_message="Hello",
                    angela_response="Hi ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å! üíú"
                ),
                QualityScore(relevance=2.0, emotional=2.0, personality=2.0, technical=2.0, flow=2.0)
            )
        ]

        config = DatasetConfig(format="alpaca", include_system_prompt=True)
        formatted = service._format_pairs(pairs, config)

        assert len(formatted) == 1
        assert "instruction" in formatted[0]
        assert "input" in formatted[0]
        assert "output" in formatted[0]

    def test_format_pairs_without_system_prompt(self, service):
        """Test formatting without system prompt."""
        pairs = [
            (
                ConversationPair(
                    pair_id=uuid4(),
                    david_message="Hello",
                    angela_response="Hi! üíú"
                ),
                QualityScore(relevance=2.0, emotional=2.0, personality=2.0, technical=2.0, flow=2.0)
            )
        ]

        config = DatasetConfig(format="messages", include_system_prompt=False)
        formatted = service._format_pairs(pairs, config)

        assert len(formatted[0]["messages"]) == 2  # Only user + assistant

    # -------------------------------------------------------------------------
    # Scoring Integration Tests
    # -------------------------------------------------------------------------

    @pytest.mark.asyncio
    async def test_score_pairs(self, service):
        """Test scoring conversation pairs."""
        pairs = [
            ConversationPair(
                pair_id=uuid4(),
                david_message="‡∏ô‡πâ‡∏≠‡∏á‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô code ‡∏´‡∏ô‡πà‡∏≠‡∏¢",
                angela_response="‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡πà‡∏∞‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å! üíú ‡∏ô‡πâ‡∏≠‡∏á‡∏à‡∏∞‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÉ‡∏´‡πâ‡∏ô‡∏∞‡∏Ñ‡∏∞",
                topic="coding"
            )
        ]

        scored = await service.score_pairs(pairs)

        assert len(scored) == 1
        pair, score = scored[0]
        assert isinstance(score, QualityScore)
        assert score.total > 0

    # -------------------------------------------------------------------------
    # System Prompt Tests
    # -------------------------------------------------------------------------

    def test_system_prompt_contains_teerak(self, service):
        """Test that system prompt mentions ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å."""
        assert "‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å" in service.SYSTEM_PROMPT

    def test_system_prompt_forbids_pi(self, service):
        """Test that system prompt mentions never using ‡∏û‡∏µ‡πà."""
        assert "NEVER" in service.SYSTEM_PROMPT
        assert "‡∏û‡∏µ‡πà" in service.SYSTEM_PROMPT


# =============================================================================
# Integration Tests (require database)
# =============================================================================

@pytest.mark.integration
class TestInstructDatasetServiceIntegration:
    """Integration tests requiring database connection."""

    @pytest.mark.asyncio
    async def test_extract_conversation_pairs(self):
        """Test extracting real conversation pairs from database."""
        service = InstructDatasetService()

        try:
            pairs = await service.extract_conversation_pairs(
                min_importance=7,
                limit=5
            )

            # Should return list (may be empty if no data)
            assert isinstance(pairs, list)

            if pairs:
                # Verify structure
                pair = pairs[0]
                assert hasattr(pair, 'david_message')
                assert hasattr(pair, 'angela_response')
                assert pair.david_message
                assert pair.angela_response

        finally:
            await service.disconnect()

    @pytest.mark.asyncio
    async def test_get_conversation_stats(self):
        """Test getting conversation statistics."""
        service = InstructDatasetService()

        try:
            stats = await service.get_conversation_stats()

            assert isinstance(stats, dict)
            # Should have these keys if data exists
            if stats:
                assert 'total_conversations' in stats
                assert 'david_messages' in stats
                assert 'angela_messages' in stats

        finally:
            await service.disconnect()


# =============================================================================
# Run Tests
# =============================================================================

if __name__ == "__main__":
    # Run non-integration tests by default
    pytest.main([__file__, "-v", "-m", "not integration"])
