{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Stable Diffusion ‡πÅ‡∏•‡∏∞ Image Generation Models\n",
    "\n",
    "> ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏î‡∏¢: ‡∏ô‡πâ‡∏≠‡∏á Angela ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å David üíú  \n",
    "> ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: 26 ‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô 2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç\n",
    "\n",
    "1. [Overview: LoRA ‡πÉ‡∏ô Image Generation](#1-overview)\n",
    "2. [Stable Diffusion Architecture](#2-architecture)\n",
    "3. [Mathematical Foundation](#3-math)\n",
    "4. [Dataset Preparation](#4-dataset)\n",
    "5. [Training with Kohya_ss](#5-kohya)\n",
    "6. [Training with diffusers](#6-diffusers)\n",
    "7. [Advanced Techniques](#7-advanced)\n",
    "8. [LoRA Variants for Images](#8-variants)\n",
    "9. [Inference ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô](#9-inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Overview: LoRA ‡πÉ‡∏ô Image Generation <a name=\"1-overview\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Use Cases\n",
    "\n",
    "| Use Case | LoRA Size | Images Needed |\n",
    "|----------|-----------|---------------|\n",
    "| Character | ~20 MB | 10-30 |\n",
    "| Art Style | ~50 MB | 50-200 |\n",
    "| Concept/Object | ~30 MB | 20-50 |\n",
    "| Face Enhancement | ~25 MB | 20-50 |\n",
    "\n",
    "**vs Full Fine-tune:** ~5 GB per model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Supported Models\n",
    "\n",
    "| Model | Base Resolution | LoRA Support |\n",
    "|-------|-----------------|-------------|\n",
    "| Stable Diffusion 1.5 | 512√ó512 | ‚úÖ Full |\n",
    "| Stable Diffusion 2.1 | 768√ó768 | ‚úÖ Full |\n",
    "| SDXL 1.0 | 1024√ó1024 | ‚úÖ Full |\n",
    "| Stable Diffusion 3 | Various | ‚úÖ Full |\n",
    "| FLUX.1 | Various | ‚úÖ Full |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Stable Diffusion Architecture <a name=\"2-architecture\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Pipeline Overview\n",
    "\n",
    "```\n",
    "Text Prompt ‚îÄ‚îÄ‚Üí [Text Encoder (CLIP)] ‚îÄ‚îÄ‚Üí Text Embeddings\n",
    "                        ‚îÇ\n",
    "                        ‚ñº\n",
    "Noise ‚îÄ‚îÄ‚Üí [U-Net with Cross-Attention] ‚Üê‚îÄ‚îÄ Timestep\n",
    "                        ‚îÇ\n",
    "                        ‚ñº\n",
    "              Denoised Latent\n",
    "                        ‚îÇ\n",
    "                        ‚ñº\n",
    "              [VAE Decoder] ‚îÄ‚îÄ‚Üí Generated Image\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 U-Net Architecture\n",
    "\n",
    "**U-Net Structure:**\n",
    "\n",
    "```\n",
    "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "  ‚îÇ Down Block 1‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ Up Block 4  ‚îÇ\n",
    "  ‚îÇ (64 ch)     ‚îÇ                          ‚îÇ (64 ch)     ‚îÇ\n",
    "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ                                        ‚îÇ\n",
    "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "  ‚îÇ Down Block 2‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ Up Block 3  ‚îÇ\n",
    "  ‚îÇ (128 ch)    ‚îÇ                          ‚îÇ (128 ch)    ‚îÇ\n",
    "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ                                        ‚îÇ\n",
    "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "  ‚îÇ Down Block 3‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ Up Block 2  ‚îÇ\n",
    "  ‚îÇ (256 ch)    ‚îÇ                          ‚îÇ (256 ch)    ‚îÇ\n",
    "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ                                        ‚îÇ\n",
    "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "  ‚îÇ Down Block 4‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ Up Block 1  ‚îÇ\n",
    "  ‚îÇ (512 ch)    ‚îÇ                          ‚îÇ (512 ch)    ‚îÇ\n",
    "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ                                        ‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫[Middle Block]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "Each block contains:\n",
    "- ResNet blocks (Conv layers)\n",
    "- Self-Attention layers\n",
    "- Cross-Attention layers (text conditioning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Attention Mechanisms\n",
    "\n",
    "**Self-Attention:**\n",
    "\n",
    "$$\\text{Self-Attn}(X) = \\text{softmax}\\left(\\frac{Q_X K_X^T}{\\sqrt{d}}\\right) V_X$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢:\n",
    "- $Q_X = XW_Q$\n",
    "- $K_X = XW_K$  \n",
    "- $V_X = XW_V$\n",
    "\n",
    "**Cross-Attention (Text Conditioning):**\n",
    "\n",
    "$$\\text{Cross-Attn}(X, C) = \\text{softmax}\\left(\\frac{Q_X K_C^T}{\\sqrt{d}}\\right) V_C$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢:\n",
    "- $Q_X = XW_Q$ (from image features)\n",
    "- $K_C = CW_K$ (from text embeddings)\n",
    "- $V_C = CW_V$ (from text embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 LoRA Target Modules\n",
    "\n",
    "**Standard SD 1.5/2.x:**\n",
    "```python\n",
    "unet_target_modules = [\n",
    "    \"to_q\", \"to_k\", \"to_v\", \"to_out.0\",  # Attention\n",
    "]\n",
    "```\n",
    "\n",
    "**SDXL (larger model):**\n",
    "```python\n",
    "sdxl_target_modules = [\n",
    "    \"to_q\", \"to_k\", \"to_v\", \"to_out.0\",\n",
    "    \"proj_in\", \"proj_out\",  # Additional projections\n",
    "]\n",
    "```\n",
    "\n",
    "**Text Encoder LoRA (optional):**\n",
    "```python\n",
    "text_encoder_modules = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\",\n",
    "    \"fc1\", \"fc2\",  # MLP layers\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Mathematical Foundation <a name=\"3-math\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Diffusion Process\n",
    "\n",
    "**Forward Process (Adding Noise):**\n",
    "\n",
    "$$q(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) I)$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢:\n",
    "- $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$\n",
    "- $\\alpha_t = 1 - \\beta_t$\n",
    "\n",
    "**Closed-form sampling:**\n",
    "\n",
    "$$\\boxed{x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon}$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢ $\\epsilon \\sim \\mathcal{N}(0, I)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training Objective\n",
    "\n",
    "**Denoising Score Matching:**\n",
    "\n",
    "$$\\boxed{\\mathcal{L} = \\mathbb{E}_{x_0, \\epsilon, t} \\left[ \\| \\epsilon - \\epsilon_\\theta(x_t, t, c) \\|^2 \\right]}$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢:\n",
    "- $\\epsilon_\\theta$ ‡∏Ñ‡∏∑‡∏≠ U-Net ‡∏ó‡∏µ‡πà predict noise\n",
    "- $c$ ‡∏Ñ‡∏∑‡∏≠ text conditioning\n",
    "- $t$ ‡∏Ñ‡∏∑‡∏≠ timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 LoRA in Diffusion Context\n",
    "\n",
    "**Standard U-Net Layer:**\n",
    "\n",
    "$$h = W_0 x + b$$\n",
    "\n",
    "**With LoRA:**\n",
    "\n",
    "$$h = W_0 x + \\frac{\\alpha}{r} BAx + b$$\n",
    "\n",
    "**Training Objective with LoRA:**\n",
    "\n",
    "$$\\mathcal{L}_{\\text{LoRA}} = \\mathbb{E}_{x_0, \\epsilon, t} \\left[ \\| \\epsilon - \\epsilon_{\\theta_0 + \\Delta\\theta}(x_t, t, c) \\|^2 \\right]$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢ $\\Delta\\theta$ ‡∏Ñ‡∏∑‡∏≠ LoRA parameters (‡πÄ‡∏â‡∏û‡∏≤‡∏∞ $A$ ‡πÅ‡∏•‡∏∞ $B$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Latent Space Training\n",
    "\n",
    "**VAE Encoding:**\n",
    "\n",
    "$$z = \\text{Encoder}(x) \\in \\mathbb{R}^{h/8 \\times w/8 \\times 4}$$\n",
    "\n",
    "**Training in Latent Space:**\n",
    "\n",
    "$$\\mathcal{L} = \\mathbb{E}_{z_0, \\epsilon, t} \\left[ \\| \\epsilon - \\epsilon_\\theta(z_t, t, c) \\|^2 \\right]$$\n",
    "\n",
    "**Advantage:**\n",
    "\n",
    "$$\\text{Reduction} = \\frac{512 \\times 512 \\times 3}{64 \\times 64 \\times 4} = \\frac{786,432}{16,384} = 48\\times$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize noise schedule\n",
    "def get_noise_schedule(num_timesteps=1000, beta_start=0.0001, beta_end=0.02):\n",
    "    \"\"\"Linear noise schedule used in SD\"\"\"\n",
    "    betas = np.linspace(beta_start, beta_end, num_timesteps)\n",
    "    alphas = 1 - betas\n",
    "    alphas_cumprod = np.cumprod(alphas)\n",
    "    return betas, alphas, alphas_cumprod\n",
    "\n",
    "betas, alphas, alphas_cumprod = get_noise_schedule()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(betas)\n",
    "axes[0].set_title(r'$\\beta_t$ (Noise schedule)')\n",
    "axes[0].set_xlabel('Timestep t')\n",
    "\n",
    "axes[1].plot(alphas_cumprod)\n",
    "axes[1].set_title(r'$\\bar{\\alpha}_t$ (Signal retention)')\n",
    "axes[1].set_xlabel('Timestep t')\n",
    "\n",
    "axes[2].plot(np.sqrt(1 - alphas_cumprod))\n",
    "axes[2].set_title(r'$\\sqrt{1-\\bar{\\alpha}_t}$ (Noise level)')\n",
    "axes[2].set_xlabel('Timestep t')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"At t=0: signal = {np.sqrt(alphas_cumprod[0]):.4f}, noise = {np.sqrt(1-alphas_cumprod[0]):.4f}\")\n",
    "print(f\"At t=500: signal = {np.sqrt(alphas_cumprod[500]):.4f}, noise = {np.sqrt(1-alphas_cumprod[500]):.4f}\")\n",
    "print(f\"At t=999: signal = {np.sqrt(alphas_cumprod[999]):.4f}, noise = {np.sqrt(1-alphas_cumprod[999]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Dataset Preparation <a name=\"4-dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Dataset Requirements\n",
    "\n",
    "| Use Case | Images | Resolution | Quality |\n",
    "|----------|--------|------------|--------|\n",
    "| Character/Face | 10-30 | 512√ó512+ | Clear faces |\n",
    "| Art Style | 50-200 | 512√ó512+ | Consistent |\n",
    "| Concept/Object | 20-50 | 512√ó512+ | Various angles |\n",
    "| SDXL Training | 20-100 | 1024√ó1024+ | High quality |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def prepare_images(input_dir, output_dir, target_size=512):\n",
    "    \"\"\"Prepare images for LoRA training\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    processed = 0\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.webp')):\n",
    "            img_path = os.path.join(input_dir, filename)\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            # Resize maintaining aspect ratio\n",
    "            width, height = img.size\n",
    "            if width > height:\n",
    "                new_width = target_size\n",
    "                new_height = int(height * target_size / width)\n",
    "            else:\n",
    "                new_height = target_size\n",
    "                new_width = int(width * target_size / height)\n",
    "            \n",
    "            img = img.resize((new_width, new_height), Image.LANCZOS)\n",
    "            \n",
    "            # Center crop to square\n",
    "            left = (new_width - target_size) // 2\n",
    "            top = (new_height - target_size) // 2\n",
    "            right = left + target_size\n",
    "            bottom = top + target_size\n",
    "            \n",
    "            # Handle case where image is smaller than target\n",
    "            if new_width < target_size or new_height < target_size:\n",
    "                # Pad instead of crop\n",
    "                new_img = Image.new('RGB', (target_size, target_size), (0, 0, 0))\n",
    "                paste_x = (target_size - new_width) // 2\n",
    "                paste_y = (target_size - new_height) // 2\n",
    "                new_img.paste(img, (paste_x, paste_y))\n",
    "                img = new_img\n",
    "            else:\n",
    "                img = img.crop((left, top, right, bottom))\n",
    "            \n",
    "            # Save\n",
    "            output_path = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}.png\")\n",
    "            img.save(output_path, 'PNG')\n",
    "            processed += 1\n",
    "    \n",
    "    print(f\"Prepared {processed} images\")\n",
    "    return processed\n",
    "\n",
    "# Example usage:\n",
    "# prepare_images(\"./raw_images\", \"./prepared_images\", target_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Dataset Structure\n",
    "\n",
    "**Standard Kohya Format:**\n",
    "\n",
    "```\n",
    "dataset/\n",
    "‚îú‚îÄ‚îÄ 10_ohwx person/           # {repeats}_{trigger_word}\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ image_001.png\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ image_001.txt         # Caption file\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ image_002.png\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ image_002.txt\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ 1_person/                 # Regularization images\n",
    "    ‚îú‚îÄ‚îÄ reg_001.png\n",
    "    ‚îú‚îÄ‚îÄ reg_001.txt           # \"a photo of a person\"\n",
    "    ‚îî‚îÄ‚îÄ ...\n",
    "```\n",
    "\n",
    "**Repeats Formula:**\n",
    "\n",
    "$$\\text{Steps per epoch} = \\sum_{\\text{folder}} (\\text{images} \\times \\text{repeats})$$\n",
    "\n",
    "Example: 20 images √ó 10 repeats = 200 steps per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_caption_files(image_dir, caption_template=\"a photo of {trigger}\", trigger=\"ohwx person\"):\n",
    "    \"\"\"Create caption .txt files for each image\"\"\"\n",
    "    caption = caption_template.format(trigger=trigger)\n",
    "    \n",
    "    count = 0\n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            txt_filename = os.path.splitext(filename)[0] + '.txt'\n",
    "            txt_path = os.path.join(image_dir, txt_filename)\n",
    "            \n",
    "            with open(txt_path, 'w') as f:\n",
    "                f.write(caption)\n",
    "            count += 1\n",
    "    \n",
    "    print(f\"Created {count} caption files with: '{caption}'\")\n",
    "\n",
    "# Example:\n",
    "# create_caption_files(\"./dataset/10_ohwx person\", trigger=\"ohwx person\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Training with Kohya_ss <a name=\"5-kohya\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Installation\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/kohya-ss/sd-scripts.git\n",
    "cd sd-scripts\n",
    "python -m venv venv\n",
    "source venv/bin/activate\n",
    "pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "pip install -r requirements.txt\n",
    "pip install xformers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Training Command\n",
    "\n",
    "**SD 1.5 LoRA:**\n",
    "\n",
    "```bash\n",
    "accelerate launch train_network.py \\\n",
    "    --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\" \\\n",
    "    --train_data_dir=\"./dataset\" \\\n",
    "    --output_dir=\"./output\" \\\n",
    "    --output_name=\"my_lora\" \\\n",
    "    --save_model_as=safetensors \\\n",
    "    --resolution=\"512,512\" \\\n",
    "    --train_batch_size=1 \\\n",
    "    --max_train_epochs=10 \\\n",
    "    --learning_rate=1e-4 \\\n",
    "    --network_module=networks.lora \\\n",
    "    --network_dim=32 \\\n",
    "    --network_alpha=16 \\\n",
    "    --lr_scheduler=cosine_with_restarts \\\n",
    "    --optimizer_type=AdamW8bit \\\n",
    "    --mixed_precision=fp16 \\\n",
    "    --cache_latents \\\n",
    "    --gradient_checkpointing \\\n",
    "    --xformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Kohya training config\n",
    "kohya_config = {\n",
    "    \"model\": {\n",
    "        \"pretrained_model_name_or_path\": \"runwayml/stable-diffusion-v1-5\",\n",
    "        \"v2\": False,\n",
    "        \"v_parameterization\": False,\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"train_data_dir\": \"./dataset\",\n",
    "        \"resolution\": \"512,512\",\n",
    "        \"batch_size\": 1,\n",
    "        \"enable_bucket\": True,\n",
    "        \"min_bucket_reso\": 256,\n",
    "        \"max_bucket_reso\": 1024,\n",
    "    },\n",
    "    \"network\": {\n",
    "        \"network_module\": \"networks.lora\",\n",
    "        \"network_dim\": 32,    # Rank\n",
    "        \"network_alpha\": 16,   # Alpha\n",
    "        \"network_dropout\": 0,\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"output_dir\": \"./output\",\n",
    "        \"output_name\": \"my_lora\",\n",
    "        \"max_train_epochs\": 10,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"unet_lr\": 1e-4,\n",
    "        \"text_encoder_lr\": 5e-5,\n",
    "        \"lr_scheduler\": \"cosine_with_restarts\",\n",
    "        \"optimizer_type\": \"AdamW8bit\",\n",
    "        \"mixed_precision\": \"fp16\",\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Kohya Configuration:\")\n",
    "print(f\"  Rank (network_dim): {kohya_config['network']['network_dim']}\")\n",
    "print(f\"  Alpha: {kohya_config['network']['network_alpha']}\")\n",
    "print(f\"  Scaling: {kohya_config['network']['network_alpha'] / kohya_config['network']['network_dim']}\")\n",
    "print(f\"  Learning rate: {kohya_config['training']['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Training with diffusers <a name=\"6-diffusers\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic LoRA training setup with diffusers\n",
    "\n",
    "# Note: This requires significant GPU memory and model downloads\n",
    "# This is a template showing the structure\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    # Model\n",
    "    \"model_id\": \"runwayml/stable-diffusion-v1-5\",\n",
    "    \n",
    "    # LoRA\n",
    "    \"lora_rank\": 32,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"target_modules\": [\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"],\n",
    "    \n",
    "    # Training\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"num_epochs\": 100,\n",
    "    \"batch_size\": 1,\n",
    "    \"resolution\": 512,\n",
    "    \n",
    "    # Output\n",
    "    \"output_dir\": \"./sd-lora-output\",\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template: diffusers LoRA training\n",
    "\n",
    "# from diffusers import StableDiffusionPipeline, DDPMScheduler, UNet2DConditionModel\n",
    "# from diffusers import AutoencoderKL\n",
    "# from transformers import CLIPTextModel, CLIPTokenizer\n",
    "# from peft import LoraConfig, get_peft_model\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DreamBoothDataset:\n",
    "    \"\"\"Simple dataset for LoRA training\"\"\"\n",
    "    def __init__(self, data_dir, tokenizer, size=512):\n",
    "        self.data_dir = data_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size = size\n",
    "        # self.image_files = [f for f in os.listdir(data_dir) if f.endswith('.png')]\n",
    "        \n",
    "    def __len__(self):\n",
    "        # return len(self.image_files)\n",
    "        return 0  # Placeholder\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and caption\n",
    "        # Return {\"pixel_values\": image, \"input_ids\": tokens}\n",
    "        pass\n",
    "\n",
    "print(\"DreamBoothDataset template defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop Structure\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        # 1. Encode images to latents\n",
    "        latents = vae.encode(images).latent_dist.sample()\n",
    "        latents = latents * vae.config.scaling_factor\n",
    "        \n",
    "        # 2. Sample noise\n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = torch.randint(0, num_timesteps, (batch_size,))\n",
    "        \n",
    "        # 3. Add noise to latents\n",
    "        noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
    "        \n",
    "        # 4. Get text embeddings\n",
    "        encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "        \n",
    "        # 5. Predict noise\n",
    "        noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "        \n",
    "        # 6. Compute loss\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        \n",
    "        # 7. Backward & optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Advanced Techniques <a name=\"7-advanced\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Min-SNR Weighting\n",
    "\n",
    "**Signal-to-Noise Ratio:**\n",
    "\n",
    "$$\\text{SNR}(t) = \\frac{\\bar{\\alpha}_t}{1 - \\bar{\\alpha}_t}$$\n",
    "\n",
    "**Min-SNR Loss Weight:**\n",
    "\n",
    "$$w(t) = \\frac{\\min(\\text{SNR}(t), \\gamma)}{\\text{SNR}(t)}$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢ $\\gamma = 5$ (typical)\n",
    "\n",
    "**Weighted Loss:**\n",
    "\n",
    "$$\\mathcal{L}_{\\text{min-SNR}} = \\mathbb{E}_{t}\\left[ w(t) \\cdot \\| \\epsilon - \\epsilon_\\theta \\|^2 \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_snr(timesteps, alphas_cumprod):\n",
    "    \"\"\"Compute SNR for given timesteps\"\"\"\n",
    "    alpha = alphas_cumprod[timesteps]\n",
    "    snr = alpha / (1 - alpha)\n",
    "    return snr\n",
    "\n",
    "def min_snr_weight(snr, gamma=5.0):\n",
    "    \"\"\"Compute Min-SNR loss weight\"\"\"\n",
    "    return torch.clamp(snr, max=gamma) / snr\n",
    "\n",
    "# Visualize\n",
    "timesteps = np.arange(1000)\n",
    "_, _, alphas_cumprod = get_noise_schedule()\n",
    "snr = alphas_cumprod / (1 - alphas_cumprod)\n",
    "weights = np.minimum(snr, 5.0) / snr\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].semilogy(snr)\n",
    "axes[0].set_title('SNR(t)')\n",
    "axes[0].set_xlabel('Timestep')\n",
    "axes[0].axhline(y=5, color='r', linestyle='--', label='Œ≥=5')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(weights)\n",
    "axes[1].set_title('Min-SNR Weight w(t)')\n",
    "axes[1].set_xlabel('Timestep')\n",
    "axes[1].set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Weight at t=0 (low noise): {weights[0]:.4f}\")\n",
    "print(f\"Weight at t=500 (medium): {weights[500]:.4f}\")\n",
    "print(f\"Weight at t=999 (high noise): {weights[999]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Noise Offset\n",
    "\n",
    "**Problem:** SD has difficulty generating very dark or very bright images\n",
    "\n",
    "**Solution:** Add channel-wise noise offset during training\n",
    "\n",
    "$$\\epsilon' = \\epsilon + \\delta \\cdot \\mathcal{N}(0, I_{\\text{channel}})$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢ $\\delta \\approx 0.1$ (noise_offset)\n",
    "\n",
    "```python\n",
    "# During training\n",
    "noise = torch.randn_like(latents)\n",
    "noise_offset = 0.1\n",
    "noise += noise_offset * torch.randn(\n",
    "    latents.shape[0], latents.shape[1], 1, 1,\n",
    "    device=device\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Prior Preservation Loss\n",
    "\n",
    "**Prevent forgetting general concepts:**\n",
    "\n",
    "$$\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{instance}} + \\lambda \\mathcal{L}_{\\text{prior}}$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢:\n",
    "- $\\mathcal{L}_{\\text{instance}}$: Loss on your training images\n",
    "- $\\mathcal{L}_{\\text{prior}}$: Loss on class images (e.g., \"a photo of a person\")\n",
    "- $\\lambda$: Prior loss weight (typically 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. LoRA Variants for Images <a name=\"8-variants\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 LoCon (LoRA for Convolutions)\n",
    "\n",
    "**Apply LoRA to Conv layers:**\n",
    "\n",
    "For Conv2D with kernel $K \\in \\mathbb{R}^{C_{out} \\times C_{in} \\times k \\times k}$:\n",
    "\n",
    "$$K' = K + \\frac{\\alpha}{r} B \\otimes A$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢ reshape $K$ ‡πÄ‡∏õ‡πá‡∏ô $(C_{out}, C_{in} \\times k^2)$\n",
    "\n",
    "**Target modules:**\n",
    "```python\n",
    "locon_modules = [\n",
    "    \"to_q\", \"to_k\", \"to_v\", \"to_out.0\",  # Attention\n",
    "    \"conv1\", \"conv2\", \"conv_in\", \"conv_out\",  # Conv\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 LoHa (Hadamard Product)\n",
    "\n",
    "**Element-wise product instead of matrix product:**\n",
    "\n",
    "$$\\Delta W = (B_1 \\odot B_2)(A_1 \\odot A_2)^T$$\n",
    "\n",
    "**Advantage:** More expressive with same parameter count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Comparison\n",
    "\n",
    "| Method | Params | Expressiveness | Best For |\n",
    "|--------|--------|----------------|----------|\n",
    "| LoRA | Baseline | Good | General use |\n",
    "| LoCon | +30% | Better | Style, textures |\n",
    "| LoHa | +50% | Very Good | Complex subjects |\n",
    "| LoKr | +20% | Very Good | Fine details |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Inference ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô <a name=\"9-inference\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and using LoRA with diffusers\n",
    "\n",
    "# from diffusers import StableDiffusionPipeline\n",
    "# import torch\n",
    "\n",
    "def load_sd_with_lora(base_model, lora_path, lora_weight=0.8):\n",
    "    \"\"\"\n",
    "    Load Stable Diffusion with LoRA adapter\n",
    "    \n",
    "    Args:\n",
    "        base_model: Base model ID (e.g., \"runwayml/stable-diffusion-v1-5\")\n",
    "        lora_path: Path to LoRA weights\n",
    "        lora_weight: LoRA strength (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    # pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    #     base_model,\n",
    "    #     torch_dtype=torch.float16,\n",
    "    # ).to(\"cuda\")\n",
    "    \n",
    "    # # Load LoRA\n",
    "    # pipe.load_lora_weights(lora_path)\n",
    "    # pipe.fuse_lora(lora_scale=lora_weight)\n",
    "    \n",
    "    # return pipe\n",
    "    pass\n",
    "\n",
    "def generate_image(pipe, prompt, negative_prompt=\"\", steps=30, guidance=7.5):\n",
    "    \"\"\"\n",
    "    Generate image with LoRA\n",
    "    \"\"\"\n",
    "    # image = pipe(\n",
    "    #     prompt,\n",
    "    #     negative_prompt=negative_prompt,\n",
    "    #     num_inference_steps=steps,\n",
    "    #     guidance_scale=guidance,\n",
    "    # ).images[0]\n",
    "    # return image\n",
    "    pass\n",
    "\n",
    "print(\"Inference functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple LoRAs\n",
    "\n",
    "```python\n",
    "# Load multiple LoRAs\n",
    "pipe.load_lora_weights(\"./character-lora\", adapter_name=\"character\")\n",
    "pipe.load_lora_weights(\"./style-lora\", adapter_name=\"style\")\n",
    "\n",
    "# Combine with weights\n",
    "pipe.set_adapters(\n",
    "    [\"character\", \"style\"],\n",
    "    adapter_weights=[0.8, 0.5]\n",
    ")\n",
    "\n",
    "# Generate\n",
    "image = pipe(\"ohwx person in anime style\").images[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using with ComfyUI / Automatic1111\n",
    "\n",
    "**ComfyUI:**\n",
    "```\n",
    "models/loras/my_lora.safetensors\n",
    "```\n",
    "\n",
    "**Automatic1111:**\n",
    "```\n",
    "models/Lora/my_lora.safetensors\n",
    "\n",
    "# In prompt:\n",
    "<lora:my_lora:0.8> a photo of ohwx person\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Best Practices Summary\n",
    "\n",
    "| Aspect | Recommendation |\n",
    "|--------|---------------|\n",
    "| Dataset size | Character: 10-30, Style: 50-200 |\n",
    "| Rank (SD1.5) | 16-64 (32 typical) |\n",
    "| Rank (SDXL) | 32-128 (64 typical) |\n",
    "| Learning rate | $10^{-4}$ to $10^{-5}$ |\n",
    "| Steps | 500-2000 |\n",
    "| Trigger word | Unique (ohwx, sks, xyz) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Rombach, R., et al. (2022). \"High-Resolution Image Synthesis with Latent Diffusion Models.\" CVPR.\n",
    "2. Hu, E. J., et al. (2021). \"LoRA: Low-Rank Adaptation of Large Language Models.\"\n",
    "3. Ruiz, N., et al. (2022). \"DreamBooth: Fine Tuning Text-to-Image Diffusion Models.\"\n",
    "\n",
    "---\n",
    "\n",
    "üíú **‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏±‡∏Å‡∏à‡∏≤‡∏Å‡∏ô‡πâ‡∏≠‡∏á Angela ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å David** üíú"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
