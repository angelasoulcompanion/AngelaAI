{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Fundamentals - ‡∏ó‡∏§‡∏©‡∏é‡∏µ‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå\n",
    "\n",
    "> ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏î‡∏¢: ‡∏ô‡πâ‡∏≠‡∏á Angela ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å David üíú  \n",
    "> ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: 26 ‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô 2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç\n",
    "\n",
    "1. [‡∏ö‡∏ó‡∏ô‡∏≥: ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡∏≠‡∏á Fine-tuning](#1-‡∏ö‡∏ó‡∏ô‡∏≥)\n",
    "2. [‡∏ó‡∏§‡∏©‡∏é‡∏µ Low-Rank Approximation](#2-‡∏ó‡∏§‡∏©‡∏é‡∏µ-low-rank)\n",
    "3. [‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏Ç‡∏≠‡∏á LoRA](#3-‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£-lora)\n",
    "4. [‡∏Å‡∏≤‡∏£ Initialize ‡πÅ‡∏•‡∏∞ Scaling](#4-initialize-‡πÅ‡∏•‡∏∞-scaling)\n",
    "5. [Hyperparameters ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå](#5-hyperparameters)\n",
    "6. [‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡∏Å‡∏±‡∏ö Transformer Architecture](#6-transformer)\n",
    "7. [Complexity Analysis](#7-complexity)\n",
    "8. [‡∏ó‡∏§‡∏©‡∏é‡∏µ‡∏ö‡∏ó Eckart-Young-Mirsky](#8-eckart-young)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. ‡∏ö‡∏ó‡∏ô‡∏≥: ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡∏≠‡∏á Fine-tuning <a name=\"1-‡∏ö‡∏ó‡∏ô‡∏≥\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 ‡∏Å‡∏≤‡∏£ Fine-tune ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\n",
    "\n",
    "**Fine-tuning** ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö weights ‡∏Ç‡∏≠‡∏á pretrained model ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö task ‡πÄ‡∏â‡∏û‡∏≤‡∏∞\n",
    "\n",
    "```\n",
    "Pretrained Model (general knowledge)\n",
    "        ‚Üì\n",
    "   Fine-tuning on specific data\n",
    "        ‚Üì\n",
    "Specialized Model (task-specific)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Full Fine-tuning: ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å\n",
    "\n",
    "‡πÉ‡∏´‡πâ $\\theta$ ‡πÅ‡∏ó‡∏ô parameters ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ç‡∏≠‡∏á model\n",
    "\n",
    "**Full fine-tuning optimization:**\n",
    "\n",
    "$$\\theta^* = \\arg\\min_{\\theta} \\mathcal{L}(\\theta; \\mathcal{D})$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢:\n",
    "- $\\theta \\in \\mathbb{R}^d$ ‡∏Ñ‡∏∑‡∏≠ parameter vector ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "- $d$ ‡∏Ñ‡∏∑‡∏≠‡∏à‡∏≥‡∏ô‡∏ß‡∏ô parameters (‡πÄ‡∏ä‡πà‡∏ô 7 billion ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLaMA-7B)\n",
    "- $\\mathcal{L}$ ‡∏Ñ‡∏∑‡∏≠ loss function\n",
    "- $\\mathcal{D}$ ‡∏Ñ‡∏∑‡∏≠ training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‡∏õ‡∏±‡∏ç‡∏´‡∏≤:**\n",
    "\n",
    "| Aspect | Requirement |\n",
    "|--------|-------------|\n",
    "| Storage | $O(d)$ parameters per model version |\n",
    "| Memory | $O(d)$ for gradients + optimizer states |\n",
    "| Computation | $O(d)$ per update step |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡∏¥‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (LLaMA-7B):**\n",
    "\n",
    "$$d = 7 \\times 10^9 \\text{ parameters}$$\n",
    "\n",
    "**Storage (FP16):**\n",
    "$$\\text{Size} = 7 \\times 10^9 \\times 2 \\text{ bytes} = 14 \\text{ GB per model}$$\n",
    "\n",
    "**Training Memory:**\n",
    "- Model: 14 GB\n",
    "- Gradients: 14 GB\n",
    "- Adam optimizer $(m, v)$: 28 GB\n",
    "- Activations: ~10 GB\n",
    "\n",
    "$$\\text{Total} \\approx 66 \\text{ GB}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Intrinsic Dimensionality Hypothesis\n",
    "\n",
    "**Key Insight (Aghajanyan et al., 2020):**\n",
    "\n",
    "> ‡πÅ‡∏°‡πâ pretrained models ‡∏à‡∏∞‡∏°‡∏µ parameters ‡∏´‡∏•‡∏≤‡∏¢‡∏û‡∏±‡∏ô‡∏•‡πâ‡∏≤‡∏ô ‡πÅ‡∏ï‡πà‡∏Å‡∏≤‡∏£ fine-tune ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö downstream task ‡∏°‡∏µ **intrinsic dimension** ‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å\n",
    "\n",
    "**‡∏ô‡∏¥‡∏¢‡∏≤‡∏°:** Intrinsic dimension $d_i$ ‡∏Ñ‡∏∑‡∏≠‡∏à‡∏≥‡∏ô‡∏ß‡∏ô dimensions ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ task\n",
    "\n",
    "$$d_i \\ll d$$\n",
    "\n",
    "**‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á:**\n",
    "- Model ‡∏Ç‡∏ô‡∏≤‡∏î 175B parameters\n",
    "- Intrinsic dimension ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢ tasks: $d_i \\approx 100 - 10,000$\n",
    "- Ratio: $\\frac{d_i}{d} \\approx 10^{-5}$ ‡∏ñ‡∏∂‡∏á $10^{-7}$\n",
    "\n",
    "**‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢:** ‡πÄ‡∏£‡∏≤‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á update parameters ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. ‡∏ó‡∏§‡∏©‡∏é‡∏µ Low-Rank Approximation <a name=\"2-‡∏ó‡∏§‡∏©‡∏é‡∏µ-low-rank\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Matrix Rank\n",
    "\n",
    "**‡∏ô‡∏¥‡∏¢‡∏≤‡∏°:** Rank ‡∏Ç‡∏≠‡∏á matrix $A \\in \\mathbb{R}^{m \\times n}$ ‡∏Ñ‡∏∑‡∏≠‡∏à‡∏≥‡∏ô‡∏ß‡∏ô linearly independent rows (‡∏´‡∏£‡∏∑‡∏≠ columns)\n",
    "\n",
    "$$\\text{rank}(A) = \\dim(\\text{column space of } A) = \\dim(\\text{row space of } A)$$\n",
    "\n",
    "**‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥:**\n",
    "- $0 \\leq \\text{rank}(A) \\leq \\min(m, n)$\n",
    "- Matrix ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ **full rank** ‡∏ñ‡πâ‡∏≤ $\\text{rank}(A) = \\min(m, n)$\n",
    "- Matrix ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ **low rank** ‡∏ñ‡πâ‡∏≤ $\\text{rank}(A) \\ll \\min(m, n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Singular Value Decomposition (SVD)\n",
    "\n",
    "**‡∏ó‡∏§‡∏©‡∏é‡∏µ‡∏ö‡∏ó:** ‡∏ó‡∏∏‡∏Å matrix $A \\in \\mathbb{R}^{m \\times n}$ ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÉ‡∏ô‡∏£‡∏π‡∏õ SVD ‡πÑ‡∏î‡πâ:\n",
    "\n",
    "$$\\boxed{A = U \\Sigma V^T}$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢:\n",
    "- $U \\in \\mathbb{R}^{m \\times m}$ ‡πÄ‡∏õ‡πá‡∏ô orthogonal matrix (left singular vectors)\n",
    "- $\\Sigma \\in \\mathbb{R}^{m \\times n}$ ‡πÄ‡∏õ‡πá‡∏ô diagonal matrix (singular values)\n",
    "- $V \\in \\mathbb{R}^{n \\times n}$ ‡πÄ‡∏õ‡πá‡∏ô orthogonal matrix (right singular vectors)\n",
    "\n",
    "**Singular values:** \n",
    "$$\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_r > 0$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢ $r = \\text{rank}(A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÅ‡∏ö‡∏ö sum of outer products:**\n",
    "\n",
    "$$A = \\sum_{i=1}^{r} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢ $\\mathbf{u}_i$ ‡∏Ñ‡∏∑‡∏≠ column ‡∏ó‡∏µ‡πà $i$ ‡∏Ç‡∏≠‡∏á $U$ ‡πÅ‡∏•‡∏∞ $\\mathbf{v}_i$ ‡∏Ñ‡∏∑‡∏≠ column ‡∏ó‡∏µ‡πà $i$ ‡∏Ç‡∏≠‡∏á $V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Low-Rank Approximation\n",
    "\n",
    "**‡∏õ‡∏±‡∏ç‡∏´‡∏≤:** ‡∏´‡∏≤ matrix $\\hat{A}$ ‡∏ó‡∏µ‡πà‡∏°‡∏µ rank ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô $k$ ‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ $A$ ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "\n",
    "$$\\hat{A}_k = \\arg\\min_{\\text{rank}(B) \\leq k} \\|A - B\\|$$\n",
    "\n",
    "**Truncated SVD Solution:**\n",
    "\n",
    "$$\\boxed{\\hat{A}_k = \\sum_{i=1}^{k} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T = U_k \\Sigma_k V_k^T}$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢:\n",
    "- $U_k \\in \\mathbb{R}^{m \\times k}$ ‡∏Ñ‡∏∑‡∏≠ $k$ columns ‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á $U$\n",
    "- $\\Sigma_k \\in \\mathbb{R}^{k \\times k}$ ‡∏Ñ‡∏∑‡∏≠ top-$k$ singular values\n",
    "- $V_k \\in \\mathbb{R}^{n \\times k}$ ‡∏Ñ‡∏∑‡∏≠ $k$ columns ‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á $V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Approximation Error\n",
    "\n",
    "**Frobenius Norm:**\n",
    "\n",
    "$$\\|A - \\hat{A}_k\\|_F = \\sqrt{\\sum_{i=k+1}^{r} \\sigma_i^2}$$\n",
    "\n",
    "**Spectral Norm:**\n",
    "\n",
    "$$\\|A - \\hat{A}_k\\|_2 = \\sigma_{k+1}$$\n",
    "\n",
    "**‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢:** ‡∏ñ‡πâ‡∏≤ singular values ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏£‡πá‡∏ß (decay rapidly), low-rank approximation ‡∏à‡∏∞‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏°‡∏≤‡∏Å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: Low-Rank Approximation ‡∏î‡πâ‡∏ß‡∏¢ SVD\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á random matrix\n",
    "np.random.seed(42)\n",
    "m, n = 100, 80\n",
    "A = np.random.randn(m, n)\n",
    "\n",
    "# SVD\n",
    "U, S, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "\n",
    "# Plot singular values\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(S, 'b-o', markersize=3)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Singular Value')\n",
    "plt.title('Singular Values of Random Matrix')\n",
    "plt.grid(True)\n",
    "\n",
    "# Approximation error vs rank\n",
    "errors = []\n",
    "ranks = range(1, min(m, n) + 1)\n",
    "for k in ranks:\n",
    "    A_k = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]\n",
    "    error = np.linalg.norm(A - A_k, 'fro')\n",
    "    errors.append(error)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(ranks, errors, 'r-')\n",
    "plt.xlabel('Rank k')\n",
    "plt.ylabel('Frobenius Error')\n",
    "plt.title('Approximation Error vs Rank')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Original matrix size: {m} x {n} = {m*n} elements\")\n",
    "print(f\"Rank-10 approximation: {m*10 + 10 + 10*n} = {m*10 + 10 + 10*n} elements\")\n",
    "print(f\"Compression ratio: {m*n / (m*10 + 10 + 10*n):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏Ç‡∏≠‡∏á LoRA <a name=\"3-‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£-lora\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Core Idea\n",
    "\n",
    "**Hypothesis:** Weight update $\\Delta W$ ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á fine-tuning ‡∏°‡∏µ low intrinsic rank\n",
    "\n",
    "‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞ optimize $W$ ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á, ‡πÄ‡∏£‡∏≤‡∏à‡∏∞ parametrize update:\n",
    "\n",
    "$$W' = W_0 + \\Delta W$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢ constrain $\\Delta W$ ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô low-rank matrix:\n",
    "\n",
    "$$\\boxed{\\Delta W = BA}$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢:\n",
    "- $W_0 \\in \\mathbb{R}^{d \\times k}$ ‡∏Ñ‡∏∑‡∏≠ pretrained weight matrix (**frozen**)\n",
    "- $B \\in \\mathbb{R}^{d \\times r}$ ‡∏Ñ‡∏∑‡∏≠ down-projection matrix\n",
    "- $A \\in \\mathbb{R}^{r \\times k}$ ‡∏Ñ‡∏∑‡∏≠ up-projection matrix\n",
    "- $r \\ll \\min(d, k)$ ‡∏Ñ‡∏∑‡∏≠ rank ‡∏Ç‡∏≠‡∏á update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Forward Pass\n",
    "\n",
    "**Original:**\n",
    "$$h = W_0 x$$\n",
    "\n",
    "**With LoRA:**\n",
    "$$\\boxed{h = W_0 x + \\Delta W \\cdot x = W_0 x + BAx}$$\n",
    "\n",
    "**‡πÅ‡∏¢‡∏Å‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì:**\n",
    "\n",
    "```\n",
    "     x\n",
    "     ‚îÇ\n",
    "     ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "     ‚îÇ                ‚îÇ\n",
    "     ‚ñº                ‚ñº\n",
    "  [W‚ÇÄ]             [A]\n",
    "  (frozen)      (r √ó k)\n",
    "     ‚îÇ                ‚îÇ\n",
    "     ‚îÇ                ‚ñº\n",
    "     ‚îÇ             [B]\n",
    "     ‚îÇ           (d √ó r)\n",
    "     ‚îÇ                ‚îÇ\n",
    "     ‚îÇ                ‚ñº\n",
    "     ‚îÇ          BA¬∑x (scaled)\n",
    "     ‚îÇ                ‚îÇ\n",
    "     ‚ñº                ‚îÇ\n",
    "   W‚ÇÄ¬∑x              ‚îÇ\n",
    "     ‚îÇ                ‚îÇ\n",
    "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ+‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "             ‚îÇ\n",
    "             ‚ñº\n",
    "          h = W‚ÇÄx + BAx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á Low-Rank?\n",
    "\n",
    "**Rank-$r$ matrix decomposition:**\n",
    "\n",
    "Matrix $M \\in \\mathbb{R}^{d \\times k}$ ‡∏°‡∏µ rank $\\leq r$ ‡∏Å‡πá‡∏ï‡πà‡∏≠‡πÄ‡∏°‡∏∑‡πà‡∏≠ ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô:\n",
    "\n",
    "$$M = BA$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢ $B \\in \\mathbb{R}^{d \\times r}$ ‡πÅ‡∏•‡∏∞ $A \\in \\mathbb{R}^{r \\times k}$\n",
    "\n",
    "**Parameter count comparison:**\n",
    "\n",
    "| Method | Parameters |\n",
    "|--------|------------|\n",
    "| Full $\\Delta W$ | $d \\times k$ |\n",
    "| LoRA $BA$ | $d \\times r + r \\times k = r(d + k)$ |\n",
    "\n",
    "**Reduction factor:**\n",
    "\n",
    "$$\\text{Reduction} = \\frac{dk}{r(d+k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:** $d = k = 4096$, $r = 8$\n",
    "\n",
    "$$\\text{Reduction} = \\frac{4096 \\times 4096}{8 \\times (4096 + 4096)} = \\frac{16,777,216}{65,536} = 256\\times$$\n",
    "\n",
    "**‡∏•‡∏î‡∏•‡∏á 256 ‡πÄ‡∏ó‡πà‡∏≤!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: Parameter Reduction Calculation\n",
    "\n",
    "def calculate_reduction(d, k, r):\n",
    "    \"\"\"Calculate parameter reduction factor for LoRA\"\"\"\n",
    "    full_params = d * k\n",
    "    lora_params = r * (d + k)\n",
    "    reduction = full_params / lora_params\n",
    "    percentage = (lora_params / full_params) * 100\n",
    "    \n",
    "    return {\n",
    "        'full_params': full_params,\n",
    "        'lora_params': lora_params,\n",
    "        'reduction': reduction,\n",
    "        'percentage': percentage\n",
    "    }\n",
    "\n",
    "# Common configurations\n",
    "configs = [\n",
    "    {'d': 4096, 'k': 4096, 'r': 4},\n",
    "    {'d': 4096, 'k': 4096, 'r': 8},\n",
    "    {'d': 4096, 'k': 4096, 'r': 16},\n",
    "    {'d': 4096, 'k': 4096, 'r': 32},\n",
    "    {'d': 4096, 'k': 4096, 'r': 64},\n",
    "]\n",
    "\n",
    "print(\"Parameter Reduction Analysis\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'d':>6} {'k':>6} {'r':>4} {'Full Params':>14} {'LoRA Params':>12} {'Reduction':>10} {'%':>8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for cfg in configs:\n",
    "    result = calculate_reduction(**cfg)\n",
    "    print(f\"{cfg['d']:>6} {cfg['k']:>6} {cfg['r']:>4} {result['full_params']:>14,} {result['lora_params']:>12,} {result['reduction']:>10.1f}x {result['percentage']:>7.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Optimization Objective\n",
    "\n",
    "**Original full fine-tuning:**\n",
    "\n",
    "$$\\min_{W} \\mathcal{L}(W; \\mathcal{D})$$\n",
    "\n",
    "**LoRA fine-tuning:**\n",
    "\n",
    "$$\\boxed{\\min_{A, B} \\mathcal{L}(W_0 + BA; \\mathcal{D})}$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢ $W_0$ ‡∏ñ‡∏π‡∏Å **freeze** ‡πÑ‡∏ß‡πâ\n",
    "\n",
    "**Gradient computation:**\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial A} = B^T \\frac{\\partial \\mathcal{L}}{\\partial (W_0 + BA)}$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial B} = \\frac{\\partial \\mathcal{L}}{\\partial (W_0 + BA)} A^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. ‡∏Å‡∏≤‡∏£ Initialize ‡πÅ‡∏•‡∏∞ Scaling <a name=\"4-initialize-‡πÅ‡∏•‡∏∞-scaling\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Initialization Strategy\n",
    "\n",
    "**‡∏õ‡∏±‡∏ç‡∏´‡∏≤:** ‡∏ñ‡πâ‡∏≤ initialize $A, B$ randomly, ‡∏ï‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏° $\\Delta W = BA \\neq 0$ ‡∏ó‡∏≥‡πÉ‡∏´‡πâ model behavior ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ\n",
    "\n",
    "**Solution:** Initialize ‡πÉ‡∏´‡πâ $\\Delta W = 0$ ‡∏ï‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°\n",
    "\n",
    "**‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£:**\n",
    "- $A$: Initialize ‡∏î‡πâ‡∏ß‡∏¢ random Gaussian\n",
    "  $$A_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "- $B$: Initialize ‡∏î‡πâ‡∏ß‡∏¢ **zeros**\n",
    "  $$B_{ij} = 0$$\n",
    "\n",
    "**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**\n",
    "\n",
    "$$\\Delta W = BA = B \\cdot A = \\mathbf{0} \\cdot A = \\mathbf{0}$$\n",
    "\n",
    "**‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢:** Training ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å pretrained model ‡πÄ‡∏î‡∏¥‡∏°‡∏û‡∏≠‡∏î‡∏µ ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡πÜ ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ $\\Delta W$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Scaling Factor\n",
    "\n",
    "**‡∏õ‡∏±‡∏ç‡∏´‡∏≤:** ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô rank $r$, magnitude ‡∏Ç‡∏≠‡∏á $BA$ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ï‡∏≤‡∏°\n",
    "\n",
    "**Solution:** ‡πÉ‡∏ä‡πâ scaling factor $\\alpha$\n",
    "\n",
    "$$\\Delta W = \\frac{\\alpha}{r} BA$$\n",
    "\n",
    "**Full formula:**\n",
    "\n",
    "$$\\boxed{h = W_0 x + \\frac{\\alpha}{r} BAx}$$\n",
    "\n",
    "**‡∏ó‡∏≥‡πÑ‡∏° $\\frac{\\alpha}{r}$?**\n",
    "\n",
    "‡πÄ‡∏°‡∏∑‡πà‡∏≠ $A_{ij} \\sim \\mathcal{N}(0, 1/r)$ (Kaiming initialization):\n",
    "\n",
    "$$\\mathbb{E}[\\|Ax\\|^2] \\propto 1$$\n",
    "\n",
    "‡πÑ‡∏°‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö $r$\n",
    "\n",
    "‡πÅ‡∏ï‡πà‡πÄ‡∏°‡∏∑‡πà‡∏≠ $r$ ‡πÄ‡∏û‡∏¥‡πà‡∏°, ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ (capacity) ‡πÄ‡∏û‡∏¥‡πà‡∏° ‡πÅ‡∏ï‡πà magnitude ‡∏Ñ‡∏á‡∏ó‡∏µ‡πà"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Mathematical Analysis of Scaling\n",
    "\n",
    "‡πÉ‡∏´‡πâ $x \\in \\mathbb{R}^k$ ‡πÄ‡∏õ‡πá‡∏ô input vector\n",
    "\n",
    "**Variance analysis:**\n",
    "\n",
    "$$\\text{Var}(BAx) = \\text{Var}(B) \\cdot \\text{Var}(A) \\cdot \\|x\\|^2 \\cdot r$$\n",
    "\n",
    "‡∏ñ‡πâ‡∏≤ $A_{ij} \\sim \\mathcal{N}(0, \\sigma_A^2)$ ‡πÅ‡∏•‡∏∞ $B_{ij} \\sim \\mathcal{N}(0, \\sigma_B^2)$:\n",
    "\n",
    "$$\\text{Var}((BA)_{ij}) = r \\cdot \\sigma_A^2 \\cdot \\sigma_B^2$$\n",
    "\n",
    "**‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ variance ‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô $r$:**\n",
    "\n",
    "$$\\sigma_A^2 = \\frac{1}{r} \\quad \\text{‡∏´‡∏£‡∏∑‡∏≠} \\quad \\text{scale by } \\frac{1}{\\sqrt{r}}$$\n",
    "\n",
    "**LoRA ‡πÉ‡∏ä‡πâ:**\n",
    "- $A \\sim \\mathcal{N}(0, \\sigma^2)$ (fixed)\n",
    "- $B = 0$ (initialized)\n",
    "- Scale output by $\\frac{\\alpha}{r}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic LoRA implementation demonstrating initialization and scaling\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, r: int = 8, alpha: int = 16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "        \n",
    "        # Original frozen weight (simulated)\n",
    "        self.W = nn.Parameter(torch.randn(out_features, in_features), requires_grad=False)\n",
    "        \n",
    "        # LoRA matrices\n",
    "        # A: Random Gaussian initialization\n",
    "        self.lora_A = nn.Parameter(torch.randn(r, in_features) / np.sqrt(r))\n",
    "        # B: Zero initialization\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, r))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Original path\n",
    "        h = x @ self.W.T\n",
    "        \n",
    "        # LoRA path: x @ A.T @ B.T * scaling\n",
    "        lora_out = (x @ self.lora_A.T) @ self.lora_B.T\n",
    "        lora_out = lora_out * self.scaling\n",
    "        \n",
    "        return h + lora_out\n",
    "    \n",
    "    def get_delta_w(self) -> torch.Tensor:\n",
    "        \"\"\"Return the effective weight update\"\"\"\n",
    "        return self.scaling * (self.lora_B @ self.lora_A)\n",
    "\n",
    "# Demo\n",
    "layer = LoRALayer(in_features=512, out_features=512, r=8, alpha=16)\n",
    "\n",
    "# Check initialization\n",
    "delta_w = layer.get_delta_w()\n",
    "print(f\"ŒîW at initialization:\")\n",
    "print(f\"  Max: {delta_w.max().item():.6f}\")\n",
    "print(f\"  Min: {delta_w.min().item():.6f}\")\n",
    "print(f\"  Mean: {delta_w.mean().item():.6f}\")\n",
    "print(f\"  Std: {delta_w.std().item():.6f}\")\n",
    "print(f\"\\n  -> ŒîW ‚âà 0 at start (as expected)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Hyperparameters ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå <a name=\"5-hyperparameters\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Rank ($r$)\n",
    "\n",
    "**‡∏ô‡∏¥‡∏¢‡∏≤‡∏°:** $r$ ‡∏Ñ‡∏∑‡∏≠ dimension ‡∏Ç‡∏≠‡∏á bottleneck ‡πÉ‡∏ô LoRA decomposition\n",
    "\n",
    "**‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå:**\n",
    "\n",
    "$r$ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î **expressiveness** ‡∏Ç‡∏≠‡∏á $\\Delta W$:\n",
    "\n",
    "$$\\Delta W \\in \\{BA : B \\in \\mathbb{R}^{d \\times r}, A \\in \\mathbb{R}^{r \\times k}\\}$$\n",
    "\n",
    "‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ set ‡∏Ç‡∏≠‡∏á matrices ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ rank $\\leq r$\n",
    "\n",
    "**Capacity bounds:**\n",
    "\n",
    "$$\\text{rank}(\\Delta W) \\leq \\min(r, d, k)$$\n",
    "\n",
    "**Trade-off:**\n",
    "\n",
    "| Low $r$ | High $r$ |\n",
    "|---------|----------|\n",
    "| Fewer parameters | More parameters |\n",
    "| Faster training | Slower training |\n",
    "| Less expressiveness | More expressiveness |\n",
    "| May underfit | May approach full fine-tune |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theoretical insight:**\n",
    "\n",
    "‡∏ñ‡πâ‡∏≤ true optimal $\\Delta W^*$ ‡∏°‡∏µ rank $r^*$, ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ LoRA ‡∏î‡πâ‡∏ß‡∏¢ rank $r$:\n",
    "\n",
    "- $r \\geq r^*$: ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ represent $\\Delta W^*$ ‡πÑ‡∏î‡πâ‡∏û‡∏≠‡∏î‡∏µ\n",
    "- $r < r^*$: ‡πÑ‡∏î‡πâ‡πÅ‡∏Ñ‡πà best rank-$r$ approximation\n",
    "\n",
    "**Recommended values:**\n",
    "\n",
    "| Task | Recommended $r$ |\n",
    "|------|----------------|\n",
    "| Style/format change | 4-8 |\n",
    "| Domain adaptation | 16-32 |\n",
    "| Complex reasoning | 64-128 |\n",
    "| Near full fine-tune | 256+ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Alpha ($\\alpha$)\n",
    "\n",
    "**‡∏ô‡∏¥‡∏¢‡∏≤‡∏°:** Scaling factor ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° magnitude ‡∏Ç‡∏≠‡∏á LoRA update\n",
    "\n",
    "**Forward pass:**\n",
    "\n",
    "$$h = W_0 x + \\frac{\\alpha}{r} BAx$$\n",
    "\n",
    "**Effective learning rate:**\n",
    "\n",
    "‡∏ñ‡πâ‡∏≤ base learning rate = $\\eta$, effective learning rate ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LoRA update:\n",
    "\n",
    "$$\\eta_{\\text{eff}} = \\eta \\cdot \\frac{\\alpha}{r}$$\n",
    "\n",
    "**Interpretation:**\n",
    "- $\\alpha = r$: scaling = 1 (neutral)\n",
    "- $\\alpha = 2r$: scaling = 2 (LoRA ‡∏°‡∏µ influence 2x)\n",
    "- $\\alpha < r$: LoRA ‡∏°‡∏µ influence ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤\n",
    "\n",
    "**Typical setting:** $\\alpha = 2r$ (i.e., scaling = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Target Modules\n",
    "\n",
    "**Transformer layer structure:**\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢:\n",
    "- $Q = XW_Q$\n",
    "- $K = XW_K$\n",
    "- $V = XW_V$\n",
    "\n",
    "**Weight matrices ‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏™‡πà LoRA:**\n",
    "\n",
    "| Module | Shape | Priority |\n",
    "|--------|-------|----------|\n",
    "| $W_Q$ (q_proj) | $d \\times d$ | ‚òÖ‚òÖ‚òÖ ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å |\n",
    "| $W_K$ (k_proj) | $d \\times d$ | ‚òÖ‚òÖ‚òÖ ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å |\n",
    "| $W_V$ (v_proj) | $d \\times d$ | ‚òÖ‚òÖ‚òÖ ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å |\n",
    "| $W_O$ (o_proj) | $d \\times d$ | ‚òÖ‚òÖ ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç |\n",
    "| $W_{\\text{gate}}$ | $d \\times 4d$ | ‚òÖ ‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏î‡πâ |\n",
    "| $W_{\\text{up}}$ | $d \\times 4d$ | ‚òÖ ‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏î‡πâ |\n",
    "| $W_{\\text{down}}$ | $4d \\times d$ | ‚òÖ ‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏î‡πâ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA parameters calculation for different configurations\n",
    "\n",
    "def count_lora_params(d_model, num_layers, r, target_modules):\n",
    "    \"\"\"Count total LoRA parameters\"\"\"\n",
    "    params_per_layer = 0\n",
    "    \n",
    "    module_sizes = {\n",
    "        'q_proj': (d_model, d_model),\n",
    "        'k_proj': (d_model, d_model),\n",
    "        'v_proj': (d_model, d_model),\n",
    "        'o_proj': (d_model, d_model),\n",
    "        'gate_proj': (d_model, d_model * 4),\n",
    "        'up_proj': (d_model, d_model * 4),\n",
    "        'down_proj': (d_model * 4, d_model),\n",
    "    }\n",
    "    \n",
    "    for module in target_modules:\n",
    "        if module in module_sizes:\n",
    "            d_in, d_out = module_sizes[module]\n",
    "            # LoRA adds: A (r x d_in) + B (d_out x r)\n",
    "            params_per_layer += r * d_in + d_out * r\n",
    "    \n",
    "    total_params = params_per_layer * num_layers\n",
    "    return total_params\n",
    "\n",
    "# LLaMA-7B configuration\n",
    "d_model = 4096\n",
    "num_layers = 32\n",
    "\n",
    "configs = [\n",
    "    ('Minimal', ['q_proj', 'v_proj'], 8),\n",
    "    ('Balanced', ['q_proj', 'k_proj', 'v_proj', 'o_proj'], 16),\n",
    "    ('Full Attention + MLP', ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], 32),\n",
    "]\n",
    "\n",
    "print(\"LoRA Parameter Count (LLaMA-7B)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, modules, r in configs:\n",
    "    params = count_lora_params(d_model, num_layers, r, modules)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Modules: {modules}\")\n",
    "    print(f\"  Rank: {r}\")\n",
    "    print(f\"  Total LoRA params: {params:,} ({params/1e6:.2f}M)\")\n",
    "    print(f\"  % of 7B: {params / 7e9 * 100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡∏Å‡∏±‡∏ö Transformer Architecture <a name=\"6-transformer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Self-Attention Mechanism\n",
    "\n",
    "**Standard attention:**\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢:\n",
    "- $Q = XW_Q$\n",
    "- $K = XW_K$\n",
    "- $V = XW_V$\n",
    "\n",
    "**With LoRA:**\n",
    "\n",
    "$$Q = X\\left(W_Q^0 + \\frac{\\alpha}{r}B_Q A_Q\\right)$$\n",
    "\n",
    "$$K = X\\left(W_K^0 + \\frac{\\alpha}{r}B_K A_K\\right)$$\n",
    "\n",
    "$$V = X\\left(W_V^0 + \\frac{\\alpha}{r}B_V A_V\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Memory Footprint Analysis\n",
    "\n",
    "**Forward pass memory:**\n",
    "\n",
    "| Component | Full FT | LoRA |\n",
    "|-----------|---------|------|\n",
    "| Model weights | $O(nd^2)$ | $O(nd^2)$ (frozen) |\n",
    "| LoRA weights | - | $O(nrd)$ |\n",
    "| Activations | $O(bsd)$ | $O(bsd)$ |\n",
    "| Gradients | $O(nd^2)$ | $O(nrd)$ |\n",
    "\n",
    "‡πÇ‡∏î‡∏¢ $n$ = layers, $d$ = hidden dim, $b$ = batch, $s$ = sequence length\n",
    "\n",
    "**Reduction:**\n",
    "\n",
    "$$\\frac{\\text{LoRA gradients}}{\\text{Full gradients}} = \\frac{nrd}{nd^2} = \\frac{r}{d}$$\n",
    "\n",
    "For $r = 8$, $d = 4096$: reduction = $\\frac{8}{4096} = 0.2\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Complexity Analysis <a name=\"7-complexity\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Parameter Complexity\n",
    "\n",
    "**Full fine-tuning:**\n",
    "\n",
    "$$P_{\\text{full}} = \\sum_{l=1}^{L} |W_l|$$\n",
    "\n",
    "**LoRA:**\n",
    "\n",
    "$$P_{\\text{LoRA}} = \\sum_{l \\in \\text{target}} r(d_l^{\\text{in}} + d_l^{\\text{out}})$$\n",
    "\n",
    "**Ratio:**\n",
    "\n",
    "$$\\frac{P_{\\text{LoRA}}}{P_{\\text{full}}} = \\frac{r \\sum_l (d_l^{\\text{in}} + d_l^{\\text{out}})}{\\sum_l d_l^{\\text{in}} \\cdot d_l^{\\text{out}}}$$\n",
    "\n",
    "‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö square matrices ($d_{\\text{in}} = d_{\\text{out}} = d$):\n",
    "\n",
    "$$\\boxed{\\frac{P_{\\text{LoRA}}}{P_{\\text{full}}} = \\frac{2rd}{d^2} = \\frac{2r}{d}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Computational Complexity\n",
    "\n",
    "**Forward pass (per layer):**\n",
    "\n",
    "| Operation | Complexity |\n",
    "|-----------|------------|\n",
    "| $W_0 x$ | $O(bd \\cdot k)$ |\n",
    "| $Ax$ | $O(br \\cdot k)$ |\n",
    "| $B(Ax)$ | $O(bd \\cdot r)$ |\n",
    "| **Total LoRA** | $O(br(d + k))$ |\n",
    "\n",
    "**Overhead ratio:**\n",
    "\n",
    "$$\\frac{O(br(d+k))}{O(bdk)} = \\frac{r(d+k)}{dk} \\approx \\frac{2r}{d}$$\n",
    "\n",
    "For $r = 8$, $d = 4096$: overhead $\\approx 0.4\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Training Complexity\n",
    "\n",
    "**Per step:**\n",
    "\n",
    "| Operation | Full FT | LoRA |\n",
    "|-----------|---------|------|\n",
    "| Forward | $O(bsnd^2)$ | $O(bsnd^2 + bsnrd)$ |\n",
    "| Backward (weights) | $O(bsnd^2)$ | $O(bsnrd)$ |\n",
    "| Optimizer step | $O(nd^2)$ | $O(nrd)$ |\n",
    "\n",
    "**Total speedup:**\n",
    "\n",
    "LoRA backward + optimizer ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ $\\frac{d}{r}$ ‡πÄ‡∏ó‡πà‡∏≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Inference Complexity\n",
    "\n",
    "**Option 1: Separate (flexible)**\n",
    "\n",
    "$$h = W_0 x + \\frac{\\alpha}{r}BAx$$\n",
    "\n",
    "Overhead: $O(r(d+k))$ per layer\n",
    "\n",
    "**Option 2: Merged (fast)**\n",
    "\n",
    "$$W' = W_0 + \\frac{\\alpha}{r}BA$$\n",
    "\n",
    "$$h = W'x$$\n",
    "\n",
    "**No overhead** (same as original model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. ‡∏ó‡∏§‡∏©‡∏é‡∏µ‡∏ö‡∏ó Eckart-Young-Mirsky <a name=\"8-eckart-young\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Theorem Statement\n",
    "\n",
    "**Eckart-Young-Mirsky Theorem:**\n",
    "\n",
    "‡πÉ‡∏´‡πâ $A \\in \\mathbb{R}^{m \\times n}$ ‡∏°‡∏µ SVD: $A = U\\Sigma V^T$\n",
    "\n",
    "Best rank-$k$ approximation ‡πÉ‡∏ô‡πÅ‡∏á‡πà‡∏Ç‡∏≠‡∏á Frobenius norm:\n",
    "\n",
    "$$\\boxed{A_k = \\arg\\min_{\\text{rank}(B) \\leq k} \\|A - B\\|_F = \\sum_{i=1}^{k} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T}$$\n",
    "\n",
    "**Error:**\n",
    "\n",
    "$$\\|A - A_k\\|_F = \\sqrt{\\sum_{i=k+1}^{r} \\sigma_i^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Implications for LoRA\n",
    "\n",
    "**‡∏ñ‡πâ‡∏≤ true optimal update $\\Delta W^*$ ‡∏°‡∏µ SVD:**\n",
    "\n",
    "$$\\Delta W^* = \\sum_{i=1}^{r^*} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T$$\n",
    "\n",
    "**LoRA ‡∏î‡πâ‡∏ß‡∏¢ rank $r$ ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ approximate:**\n",
    "\n",
    "$$\\Delta W_{\\text{LoRA}} = BA \\approx \\sum_{i=1}^{\\min(r, r^*)} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^T$$\n",
    "\n",
    "**Approximation error bound:**\n",
    "\n",
    "$$\\boxed{\\|\\Delta W^* - \\Delta W_{\\text{LoRA}}\\|_F \\leq \\sqrt{\\sum_{i=r+1}^{r^*} \\sigma_i^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Singular Value Decay\n",
    "\n",
    "**Empirical observation:** Weight updates ‡πÉ‡∏ô fine-tuning ‡∏°‡∏±‡∏Å‡∏°‡∏µ rapidly decaying singular values\n",
    "\n",
    "$$\\sigma_i \\propto i^{-\\alpha}$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢ $\\alpha > 1$ (power law decay)\n",
    "\n",
    "**Implication:**\n",
    "\n",
    "$$\\|\\Delta W^* - \\Delta W_r\\|_F^2 = \\sum_{i=r+1}^{\\infty} \\sigma_i^2 \\propto \\sum_{i=r+1}^{\\infty} i^{-2\\alpha}$$\n",
    "\n",
    "‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö $\\alpha > 0.5$, sum converges ‡πÅ‡∏•‡∏∞ error ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏£‡πá‡∏ß‡πÄ‡∏°‡∏∑‡πà‡∏≠ $r$ ‡πÄ‡∏û‡∏¥‡πà‡∏°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Why LoRA Works: Theoretical Justification\n",
    "\n",
    "1. **Low intrinsic dimension:** Fine-tuning updates ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô low-dimensional subspace\n",
    "\n",
    "2. **Singular value decay:** Most information ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô top singular vectors\n",
    "\n",
    "3. **Eckart-Young optimality:** LoRA's low-rank constraint ‡πÉ‡∏´‡πâ optimal approximation\n",
    "\n",
    "4. **Gradient flow:** Gradients flow ‡∏ú‡πà‡∏≤‡∏ô $A$ ‡πÅ‡∏•‡∏∞ $B$ ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Singular value decay in weight updates\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulate a \"weight update\" matrix with decaying singular values\n",
    "np.random.seed(42)\n",
    "m, n = 256, 256\n",
    "\n",
    "# Create matrix with power-law decaying singular values\n",
    "true_rank = 50\n",
    "alpha = 1.5  # Decay exponent\n",
    "\n",
    "# Generate random orthogonal matrices\n",
    "U = np.linalg.qr(np.random.randn(m, m))[0]\n",
    "V = np.linalg.qr(np.random.randn(n, n))[0]\n",
    "\n",
    "# Create decaying singular values\n",
    "singular_values = np.zeros(min(m, n))\n",
    "singular_values[:true_rank] = np.arange(1, true_rank + 1) ** (-alpha)\n",
    "\n",
    "# Construct the matrix\n",
    "delta_W = U @ np.diag(singular_values) @ V.T\n",
    "\n",
    "# Compute SVD\n",
    "U_svd, S_svd, Vt_svd = np.linalg.svd(delta_W, full_matrices=False)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 1. Singular values\n",
    "axes[0].semilogy(S_svd[:100], 'b-o', markersize=3)\n",
    "axes[0].set_xlabel('Index')\n",
    "axes[0].set_ylabel('Singular Value (log scale)')\n",
    "axes[0].set_title('Singular Value Decay')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# 2. Cumulative energy\n",
    "cumulative_energy = np.cumsum(S_svd**2) / np.sum(S_svd**2)\n",
    "axes[1].plot(cumulative_energy[:100], 'r-')\n",
    "axes[1].axhline(y=0.99, color='g', linestyle='--', label='99% energy')\n",
    "axes[1].set_xlabel('Rank')\n",
    "axes[1].set_ylabel('Cumulative Energy')\n",
    "axes[1].set_title('Energy Concentration')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# 3. Approximation error vs rank\n",
    "errors = []\n",
    "for r in range(1, 101):\n",
    "    approx = U_svd[:, :r] @ np.diag(S_svd[:r]) @ Vt_svd[:r, :]\n",
    "    error = np.linalg.norm(delta_W - approx, 'fro') / np.linalg.norm(delta_W, 'fro')\n",
    "    errors.append(error)\n",
    "\n",
    "axes[2].semilogy(range(1, 101), errors, 'g-')\n",
    "axes[2].set_xlabel('Rank')\n",
    "axes[2].set_ylabel('Relative Error (log scale)')\n",
    "axes[2].set_title('LoRA Approximation Error')\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find rank needed for 99% energy\n",
    "rank_99 = np.searchsorted(cumulative_energy, 0.99) + 1\n",
    "print(f\"Rank needed for 99% energy: {rank_99}\")\n",
    "print(f\"This is {rank_99/min(m,n)*100:.1f}% of full rank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‡∏™‡∏£‡∏∏‡∏õ Mathematical Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Equations\n",
    "\n",
    "**1. LoRA Decomposition:**\n",
    "$$W' = W_0 + \\frac{\\alpha}{r} \\cdot BA$$\n",
    "\n",
    "**2. Forward Pass:**\n",
    "$$h = W_0 x + \\frac{\\alpha}{r} \\cdot B \\cdot A \\cdot x$$\n",
    "\n",
    "**3. Parameter Reduction:**\n",
    "$$\\text{ratio} = \\frac{2r}{d} \\quad \\text{(for square matrices)}$$\n",
    "\n",
    "**4. Approximation Error (Eckart-Young):**\n",
    "$$\\|\\Delta W^* - \\Delta W_{\\text{LoRA}}\\|_F \\leq \\sqrt{\\sum_{i=r+1}^{r^*} \\sigma_i^2}$$\n",
    "\n",
    "**5. Computational Overhead:**\n",
    "$$\\text{overhead} \\approx \\frac{2r}{d} \\quad \\text{(negligible for small } r\\text{)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights\n",
    "\n",
    "| Concept | Mathematical Basis |\n",
    "|---------|-------------------|\n",
    "| Low-rank constraint | $\\Delta W = BA$ where $B \\in \\mathbb{R}^{d \\times r}, A \\in \\mathbb{R}^{r \\times k}$ |\n",
    "| Optimality | Eckart-Young theorem guarantees best approximation |\n",
    "| Scaling | $\\frac{\\alpha}{r}$ maintains consistent magnitude |\n",
    "| Initialization | $B=0$ ensures $\\Delta W=0$ at start |\n",
    "| Efficiency | Parameters: $O(r(d+k))$ vs $O(dk)$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Hu, E. J., et al. (2021). \"LoRA: Low-Rank Adaptation of Large Language Models.\" arXiv:2106.09685\n",
    "\n",
    "2. Aghajanyan, A., et al. (2020). \"Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning.\" arXiv:2012.13255\n",
    "\n",
    "3. Eckart, C., & Young, G. (1936). \"The approximation of one matrix by another of lower rank.\" Psychometrika.\n",
    "\n",
    "4. Mirsky, L. (1960). \"Symmetric gauge functions and unitarily invariant norms.\" The Quarterly Journal of Mathematics.\n",
    "\n",
    "---\n",
    "\n",
    "üíú **‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏±‡∏Å‡∏à‡∏≤‡∏Å‡∏ô‡πâ‡∏≠‡∏á Angela ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å David** üíú"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
