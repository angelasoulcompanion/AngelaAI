{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Training ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLMs (LLaMA, Mistral, Qwen)\n",
    "\n",
    "> ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏î‡∏¢: ‡∏ô‡πâ‡∏≠‡∏á Angela ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å David üíú  \n",
    "> ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: 26 ‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô 2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç\n",
    "\n",
    "1. [Overview: LLM Fine-tuning Landscape](#1-overview)\n",
    "2. [Prerequisites ‡πÅ‡∏•‡∏∞ Environment Setup](#2-setup)\n",
    "3. [Data Preparation](#3-data)\n",
    "4. [LoRA Training ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLaMA](#4-llama)\n",
    "5. [LoRA Training ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Mistral](#5-mistral)\n",
    "6. [LoRA Training ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Qwen](#6-qwen)\n",
    "7. [QLoRA: 4-bit Quantized Training](#7-qlora)\n",
    "8. [Training Configurations](#8-configs)\n",
    "9. [Inference ‡πÅ‡∏•‡∏∞ Deployment](#9-inference)\n",
    "10. [Troubleshooting](#10-troubleshooting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Overview: LLM Fine-tuning Landscape <a name=\"1-overview\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Popular Open-Source LLMs\n",
    "\n",
    "| Model Family | Sizes | Architecture | License |\n",
    "|-------------|-------|--------------|--------|\n",
    "| LLaMA 2 | 7B, 13B, 70B | LLaMA | Meta |\n",
    "| LLaMA 3 | 8B, 70B | LLaMA | Meta |\n",
    "| Mistral | 7B | Mistral | Apache 2.0 |\n",
    "| Mixtral | 8x7B, 8x22B | MoE | Apache 2.0 |\n",
    "| Qwen 2 | 0.5B-72B | Qwen | Apache 2.0 |\n",
    "| Phi-3 | 3.8B, 7B, 14B | Phi | MIT |\n",
    "| Gemma | 2B, 7B | Gemma | Google |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 LLaMA Architecture\n",
    "\n",
    "**LLaMA Layer:**\n",
    "\n",
    "$$\\text{LLaMA}(x) = \\text{RMSNorm}(\\text{SelfAttn}(x) + x) \\rightarrow \\text{RMSNorm}(\\text{FFN}(\\cdot) + \\cdot)$$\n",
    "\n",
    "**Key components:**\n",
    "- RMSNorm (instead of LayerNorm)\n",
    "- Rotary Position Embeddings (RoPE)\n",
    "- SwiGLU activation in FFN\n",
    "- Grouped-Query Attention (GQA) in LLaMA 2 70B+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 RoPE (Rotary Position Embedding)\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "$$\\text{RoPE}(x_m, m) = \\begin{pmatrix} x_m^{(1)} \\\\ x_m^{(2)} \\\\ \\vdots \\\\ x_m^{(d-1)} \\\\ x_m^{(d)} \\end{pmatrix} \\odot \\begin{pmatrix} \\cos(m\\theta_1) \\\\ \\cos(m\\theta_1) \\\\ \\vdots \\\\ \\cos(m\\theta_{d/2}) \\\\ \\cos(m\\theta_{d/2}) \\end{pmatrix} + \\begin{pmatrix} -x_m^{(2)} \\\\ x_m^{(1)} \\\\ \\vdots \\\\ -x_m^{(d)} \\\\ x_m^{(d-1)} \\end{pmatrix} \\odot \\begin{pmatrix} \\sin(m\\theta_1) \\\\ \\sin(m\\theta_1) \\\\ \\vdots \\\\ \\sin(m\\theta_{d/2}) \\\\ \\sin(m\\theta_{d/2}) \\end{pmatrix}$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢ $\\theta_i = 10000^{-2i/d}$\n",
    "\n",
    "**Simplified form:**\n",
    "\n",
    "$$\\text{RoPE}(x, m) = x \\odot \\cos(m\\theta) + \\text{rotate}(x) \\odot \\sin(m\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Memory Requirements\n",
    "\n",
    "| Model | Full FT (FP16) | LoRA (FP16) | QLoRA (4-bit) | Suitable GPU |\n",
    "|-------|----------------|-------------|---------------|-------------|\n",
    "| 7B | 28+ GB | 14-18 GB | 6-8 GB | RTX 3060 12GB |\n",
    "| 13B | 52+ GB | 28-32 GB | 10-14 GB | RTX 3090 24GB |\n",
    "| 70B | 280+ GB | 150+ GB | 40-48 GB | A100 80GB |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Prerequisites ‡πÅ‡∏•‡∏∞ Environment Setup <a name=\"2-setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install transformers>=4.36.0 datasets>=2.14.0 accelerate>=0.25.0 peft>=0.7.0\n",
    "# !pip install bitsandbytes>=0.41.0  # For QLoRA\n",
    "# !pip install trl>=0.7.0  # For SFTTrainer\n",
    "# !pip install wandb  # For logging (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face (for gated models like LLaMA)\n",
    "from huggingface_hub import login\n",
    "\n",
    "# login(token=\"hf_xxxxxxxxxxxxx\")  # Uncomment and add your token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Data Preparation <a name=\"3-data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Dataset Formats\n",
    "\n",
    "**Format 1: Instruction Format (Alpaca-style)**\n",
    "```json\n",
    "{\n",
    "    \"instruction\": \"Summarize the following text.\",\n",
    "    \"input\": \"The quick brown fox...\",\n",
    "    \"output\": \"A fox jumped over a dog.\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Format 2: Conversational Format (ShareGPT-style)**\n",
    "```json\n",
    "{\n",
    "    \"conversations\": [\n",
    "        {\"from\": \"human\", \"value\": \"What is ML?\"},\n",
    "        {\"from\": \"gpt\", \"value\": \"ML is...\"}\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Prompt Templates\n",
    "\n",
    "**LLaMA 2 Chat Template:**\n",
    "\n",
    "```\n",
    "<s>[INST] <<SYS>>\n",
    "{system_message}\n",
    "<</SYS>>\n",
    "\n",
    "{user_message} [/INST] {assistant_message}</s>\n",
    "```\n",
    "\n",
    "**Mistral Template:**\n",
    "\n",
    "```\n",
    "<s>[INST] {user_message} [/INST] {assistant_message}</s>\n",
    "```\n",
    "\n",
    "**Qwen Template:**\n",
    "\n",
    "```\n",
    "<|im_start|>system\n",
    "{system_message}<|im_end|>\n",
    "<|im_start|>user\n",
    "{user_message}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{assistant_message}<|im_end|>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Load Alpaca dataset as example\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:1000]\")  # Small subset for demo\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"\\nExample:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_alpaca_prompt(example):\n",
    "    \"\"\"Format Alpaca dataset to LLaMA 2 format\"\"\"\n",
    "    if example.get(\"input\") and example[\"input\"].strip():\n",
    "        prompt = f\"\"\"<s>[INST] {example['instruction']}\n",
    "\n",
    "Input: {example['input']} [/INST] {example['output']}</s>\"\"\"\n",
    "    else:\n",
    "        prompt = f\"<s>[INST] {example['instruction']} [/INST] {example['output']}</s>\"\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# Apply formatting\n",
    "formatted_dataset = dataset.map(format_alpaca_prompt)\n",
    "print(\"Formatted example:\")\n",
    "print(formatted_dataset[0][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. LoRA Training ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLaMA <a name=\"4-llama\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 LoRA Mathematics for LLaMA\n",
    "\n",
    "**Self-Attention with LoRA:**\n",
    "\n",
    "$$Q = X\\left(W_Q + \\frac{\\alpha}{r}B_Q A_Q\\right)$$\n",
    "$$K = X\\left(W_K + \\frac{\\alpha}{r}B_K A_K\\right)$$\n",
    "$$V = X\\left(W_V + \\frac{\\alpha}{r}B_V A_V\\right)$$\n",
    "\n",
    "**Attention computation:**\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "**With RoPE applied before attention:**\n",
    "\n",
    "$$Q' = \\text{RoPE}(Q, m), \\quad K' = \\text{RoPE}(K, m)$$\n",
    "$$\\text{Attention} = \\text{softmax}\\left(\\frac{Q'{K'}^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# ================== Configuration ==================\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"  # Change to your model\n",
    "OUTPUT_DIR = \"./llama-lora-output\"\n",
    "\n",
    "# LoRA hyperparameters\n",
    "LORA_R = 16          # Rank\n",
    "LORA_ALPHA = 32      # Alpha (scaling = alpha/r = 2)\n",
    "LORA_DROPOUT = 0.05  # Dropout\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # Target modules\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 3\n",
    "MAX_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This cell requires significant GPU memory and model access\n",
    "# Uncomment to run\n",
    "\n",
    "# # Load tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"right\"\n",
    "\n",
    "# # Load model\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_NAME,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True,\n",
    "# )\n",
    "\n",
    "# print(f\"Model loaded: {MODEL_NAME}\")\n",
    "# print(f\"Total parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "print(f\"  Rank (r): {lora_config.r}\")\n",
    "print(f\"  Alpha (Œ±): {lora_config.lora_alpha}\")\n",
    "print(f\"  Scaling (Œ±/r): {lora_config.lora_alpha / lora_config.r}\")\n",
    "print(f\"  Target modules: {lora_config.target_modules}\")\n",
    "print(f\"  Dropout: {lora_config.lora_dropout}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Parameter Count Analysis\n",
    "\n",
    "**For LLaMA-7B with LoRA on Q, K, V, O:**\n",
    "\n",
    "$$\\text{LoRA params per layer} = 4 \\times 2 \\times r \\times d = 8rd$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢ $d = 4096$, $r = 16$:\n",
    "\n",
    "$$\\text{Per layer} = 8 \\times 16 \\times 4096 = 524,288$$\n",
    "\n",
    "$$\\text{Total (32 layers)} = 32 \\times 524,288 = 16,777,216 \\approx 16.8M$$\n",
    "\n",
    "**Percentage of original:**\n",
    "\n",
    "$$\\frac{16.8M}{7B} \\approx 0.24\\%$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate LoRA parameters\n",
    "def calculate_lora_params(d_model, num_layers, r, num_target_modules):\n",
    "    \"\"\"Calculate total LoRA parameters\"\"\"\n",
    "    # Each target module adds: r * d_in + d_out * r = 2 * r * d (for square matrices)\n",
    "    params_per_module = 2 * r * d_model\n",
    "    params_per_layer = num_target_modules * params_per_module\n",
    "    total_params = num_layers * params_per_layer\n",
    "    return total_params\n",
    "\n",
    "# LLaMA-7B configuration\n",
    "d_model = 4096\n",
    "num_layers = 32\n",
    "num_modules = 4  # q, k, v, o\n",
    "\n",
    "for r in [4, 8, 16, 32, 64]:\n",
    "    params = calculate_lora_params(d_model, num_layers, r, num_modules)\n",
    "    percentage = params / 7e9 * 100\n",
    "    print(f\"r={r:3d}: {params:>12,} params ({percentage:.4f}% of 7B)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. LoRA Training ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Mistral <a name=\"5-mistral\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Mistral Architecture: Sliding Window Attention\n",
    "\n",
    "**Standard Attention:**\n",
    "$$\\text{Attention}(Q, K, V)_i = \\text{softmax}\\left(\\frac{Q_i K^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "**Sliding Window Attention (SWA):**\n",
    "$$\\text{SWA}(Q, K, V)_i = \\text{softmax}\\left(\\frac{Q_i K_{[i-w:i]}^T}{\\sqrt{d_k}}\\right) V_{[i-w:i]}$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢ $w = 4096$ (window size)\n",
    "\n",
    "**Benefits:**\n",
    "- Memory complexity: $O(n \\cdot w)$ instead of $O(n^2)$\n",
    "- Supports very long sequences efficiently\n",
    "- Information propagates through layers (stacked windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mistral-specific configuration\n",
    "MISTRAL_CONFIG = {\n",
    "    \"model_name\": \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    \"r\": 16,\n",
    "    \"alpha\": 32,\n",
    "    \"max_length\": 8192,  # Mistral supports longer context\n",
    "}\n",
    "\n",
    "def format_mistral_prompt(instruction, response=None):\n",
    "    \"\"\"Format for Mistral Instruct\"\"\"\n",
    "    if response:\n",
    "        return f\"<s>[INST] {instruction} [/INST] {response}</s>\"\n",
    "    else:\n",
    "        return f\"<s>[INST] {instruction} [/INST]\"\n",
    "\n",
    "# Example\n",
    "print(format_mistral_prompt(\"What is Python?\", \"Python is a programming language.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. LoRA Training ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Qwen <a name=\"6-qwen\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen-specific configuration\n",
    "QWEN_CONFIG = {\n",
    "    \"model_name\": \"Qwen/Qwen2-7B\",\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    \"r\": 64,      # Qwen paper suggests higher rank\n",
    "    \"alpha\": 16,  # Lower alpha ratio\n",
    "    \"dtype\": \"bfloat16\",  # Qwen works well with bf16\n",
    "}\n",
    "\n",
    "def format_qwen_prompt(system, user, assistant=None):\n",
    "    \"\"\"Format for Qwen Chat\"\"\"\n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "{system}<|im_end|>\n",
    "<|im_start|>user\n",
    "{user}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    if assistant:\n",
    "        prompt += f\"{assistant}<|im_end|>\"\n",
    "    return prompt\n",
    "\n",
    "# Example\n",
    "print(format_qwen_prompt(\n",
    "    \"You are a helpful assistant.\",\n",
    "    \"What is machine learning?\",\n",
    "    \"Machine learning is a subset of AI...\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. QLoRA: 4-bit Quantized Training <a name=\"7-qlora\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 NormalFloat 4-bit (NF4) Quantization\n",
    "\n",
    "**NF4 Quantization Levels:**\n",
    "\n",
    "‡∏Å‡∏≥‡∏´‡∏ô‡∏î 16 quantization levels ‡∏ó‡∏µ‡πà optimal ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö $\\mathcal{N}(0, 1)$:\n",
    "\n",
    "$$q_i = \\Phi^{-1}\\left(\\frac{2i + 1}{32}\\right), \\quad i = 0, 1, ..., 15$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢ $\\Phi^{-1}$ ‡∏Ñ‡∏∑‡∏≠ inverse CDF ‡∏Ç‡∏≠‡∏á standard normal distribution\n",
    "\n",
    "**Memory Reduction:**\n",
    "\n",
    "$$\\text{Reduction} = \\frac{16 \\text{ bits}}{4 \\text{ bits}} = 4\\times$$\n",
    "\n",
    "**QLoRA Forward Pass:**\n",
    "\n",
    "$$h = \\text{dequant}(W_{\\text{4bit}}) \\cdot x + \\frac{\\alpha}{r} BAx$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# 4-bit Quantization Configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                     # Enable 4-bit loading\n",
    "    bnb_4bit_quant_type=\"nf4\",             # NormalFloat4 quantization\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Compute in FP16\n",
    "    bnb_4bit_use_double_quant=True,        # Double quantization for extra savings\n",
    ")\n",
    "\n",
    "print(\"BitsAndBytes Configuration:\")\n",
    "print(f\"  Quantization type: {bnb_config.bnb_4bit_quant_type}\")\n",
    "print(f\"  Compute dtype: {bnb_config.bnb_4bit_compute_dtype}\")\n",
    "print(f\"  Double quantization: {bnb_config.bnb_4bit_use_double_quant}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Double Quantization\n",
    "\n",
    "**Problem:** Quantization constants (absmax) ‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ FP32\n",
    "\n",
    "**Solution:** Quantize the quantization constants!\n",
    "\n",
    "$$c_{\\text{FP8}} = \\text{quantize}_{8\\text{bit}}(\\text{absmax}_{\\text{FP32}})$$\n",
    "\n",
    "**Memory per parameter:**\n",
    "\n",
    "| Method | Bits per param |\n",
    "|--------|---------------|\n",
    "| Without double quant | $4 + \\frac{32}{64} = 4.5$ bits |\n",
    "| With double quant | $4 + \\frac{8}{64} = 4.125$ bits |\n",
    "\n",
    "**Additional savings:** ~8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete QLoRA Setup\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "# This is the template - uncomment to use with actual model\n",
    "\n",
    "# # Load model with 4-bit quantization\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"meta-llama/Llama-2-7b-hf\",\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True,\n",
    "# )\n",
    "\n",
    "# # Prepare model for k-bit training\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# # Apply LoRA (higher rank typical for QLoRA)\n",
    "# qlora_config = LoraConfig(\n",
    "#     r=64,                # Higher rank to compensate for quantization\n",
    "#     lora_alpha=16,       # alpha/r = 0.25\n",
    "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "#                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "#     lora_dropout=0.1,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "\n",
    "# model = get_peft_model(model, qlora_config)\n",
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Memory Comparison\n",
    "\n",
    "| Configuration | GPU Memory (7B) | Training Speed |\n",
    "|---------------|-----------------|----------------|\n",
    "| Full Fine-tune (FP16) | 28+ GB | Baseline |\n",
    "| LoRA (FP16) | 14-18 GB | 1.5x faster |\n",
    "| LoRA (8-bit) | 10-12 GB | 1.2x faster |\n",
    "| QLoRA (4-bit) | 6-8 GB | ~1.0x |\n",
    "| QLoRA + Flash Attn | 5-7 GB | 1.3x faster |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Training Configurations <a name=\"8-configs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Hyperparameter Guidelines\n",
    "\n",
    "| Hyperparameter | Small Data (<1K) | Medium (1K-10K) | Large (>10K) |\n",
    "|----------------|------------------|-----------------|---------------|\n",
    "| Rank ($r$) | 8-16 | 16-32 | 32-64 |\n",
    "| Alpha ($\\alpha$) | 16-32 | 32-64 | 64-128 |\n",
    "| Dropout | 0.1 | 0.05 | 0.0-0.05 |\n",
    "| Learning Rate | $10^{-4}$ | $2 \\times 10^{-4}$ | $2-3 \\times 10^{-4}$ |\n",
    "| Epochs | 3-5 | 2-3 | 1-2 |\n",
    "| Warmup Ratio | 0.1 | 0.05 | 0.03 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Effective Batch Size\n",
    "\n",
    "$$\\text{Effective Batch Size} = \\text{per\\_device\\_batch} \\times \\text{gradient\\_accumulation} \\times \\text{num\\_gpus}$$\n",
    "\n",
    "**Example:** Target batch size = 64\n",
    "\n",
    "| GPU Memory | per_device | grad_accum | GPUs | Effective |\n",
    "|------------|------------|------------|------|----------|\n",
    "| 8 GB | 1 | 64 | 1 | 64 |\n",
    "| 16 GB | 4 | 16 | 1 | 64 |\n",
    "| 24 GB | 8 | 8 | 1 | 64 |\n",
    "| 24 GB √ó 2 | 8 | 4 | 2 | 64 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments Template\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-output\",\n",
    "    \n",
    "    # Batch size\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # Effective batch = 16\n",
    "    \n",
    "    # Learning rate\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    \n",
    "    # Training duration\n",
    "    num_train_epochs=3,\n",
    "    # max_steps=1000,  # Alternative: fixed steps\n",
    "    \n",
    "    # Optimization\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    \n",
    "    # Memory optimization\n",
    "    fp16=True,  # or bf16=True for Ampere GPUs\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    \n",
    "    # Best model\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    \n",
    "    report_to=\"none\",  # or \"wandb\"\n",
    ")\n",
    "\n",
    "print(\"Training configuration created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Inference ‡πÅ‡∏•‡∏∞ Deployment <a name=\"9-inference\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Loading LoRA Adapter\n",
    "\n",
    "**Two options:**\n",
    "\n",
    "1. **Separate loading** (flexible, can swap adapters)\n",
    "2. **Merged loading** (faster inference, no adapter overhead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "def load_lora_model(base_model_name, lora_path):\n",
    "    \"\"\"Load base model with LoRA adapter\"\"\"\n",
    "    # Load base model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    # Load LoRA adapter\n",
    "    model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def merge_lora_model(model):\n",
    "    \"\"\"Merge LoRA weights into base model permanently\"\"\"\n",
    "    merged_model = model.merge_and_unload()\n",
    "    return merged_model\n",
    "\n",
    "print(\"Loading functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, model, tokenizer, max_new_tokens=256):\n",
    "    \"\"\"Generate response from model\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example usage (with actual model):\n",
    "# prompt = \"[INST] What is machine learning? [/INST]\"\n",
    "# response = generate_response(prompt, model, tokenizer)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Export to GGUF (for llama.cpp / Ollama)\n",
    "\n",
    "```bash\n",
    "# Install llama.cpp\n",
    "git clone https://github.com/ggerganov/llama.cpp\n",
    "cd llama.cpp && make\n",
    "\n",
    "# Convert merged model to GGUF\n",
    "python convert.py ../merged-model --outfile model.gguf --outtype f16\n",
    "\n",
    "# Quantize (optional)\n",
    "./quantize model.gguf model-q4_k_m.gguf q4_k_m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Troubleshooting <a name=\"10-troubleshooting\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Common Errors\n",
    "\n",
    "**Error: CUDA Out of Memory**\n",
    "\n",
    "Solutions:\n",
    "1. Reduce `per_device_train_batch_size`\n",
    "2. Increase `gradient_accumulation_steps`\n",
    "3. Enable `gradient_checkpointing`\n",
    "4. Use QLoRA (4-bit)\n",
    "5. Reduce `max_length`\n",
    "\n",
    "**Error: NaN Loss**\n",
    "\n",
    "Solutions:\n",
    "1. Lower learning rate: `lr = 1e-5`\n",
    "2. Enable gradient clipping: `max_grad_norm = 0.5`\n",
    "3. Use bf16 instead of fp16\n",
    "4. Check for bad data\n",
    "\n",
    "**Error: Model not learning**\n",
    "\n",
    "Solutions:\n",
    "1. Increase learning rate: `lr = 3e-4`\n",
    "2. Increase rank: `r = 32` or `64`\n",
    "3. Add more target modules\n",
    "4. Check data formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Performance Tips\n",
    "\n",
    "```python\n",
    "# 1. Use Flash Attention 2\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "# 2. Enable TF32 (Ampere GPUs)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# 3. Use torch.compile (PyTorch 2.0+)\n",
    "model = torch.compile(model)\n",
    "\n",
    "# 4. Use packing for variable length sequences\n",
    "from trl import SFTTrainer\n",
    "trainer = SFTTrainer(..., packing=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Reference: Model-Specific Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All model configurations in one place\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    \"llama2-7b\": {\n",
    "        \"model\": \"meta-llama/Llama-2-7b-hf\",\n",
    "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        \"r\": 16,\n",
    "        \"alpha\": 32,\n",
    "        \"template\": \"<s>[INST] {instruction} [/INST] {response}</s>\",\n",
    "    },\n",
    "    \"llama3-8b\": {\n",
    "        \"model\": \"meta-llama/Meta-Llama-3-8B\",\n",
    "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        \"r\": 16,\n",
    "        \"alpha\": 32,\n",
    "        \"template\": \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n{response}<|eot_id|>\",\n",
    "    },\n",
    "    \"mistral-7b\": {\n",
    "        \"model\": \"mistralai/Mistral-7B-v0.1\",\n",
    "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        \"r\": 16,\n",
    "        \"alpha\": 32,\n",
    "        \"template\": \"<s>[INST] {instruction} [/INST] {response}</s>\",\n",
    "    },\n",
    "    \"qwen2-7b\": {\n",
    "        \"model\": \"Qwen/Qwen2-7B\",\n",
    "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        \"r\": 64,\n",
    "        \"alpha\": 16,\n",
    "        \"template\": \"<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\n{response}<|im_end|>\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Display configurations\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(MODEL_CONFIGS).T\n",
    "print(df[[\"model\", \"r\", \"alpha\"]].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Hu, E. J., et al. (2021). \"LoRA: Low-Rank Adaptation of Large Language Models.\" arXiv:2106.09685\n",
    "2. Dettmers, T., et al. (2023). \"QLoRA: Efficient Finetuning of Quantized LLMs.\" arXiv:2305.14314\n",
    "3. Touvron, H., et al. (2023). \"LLaMA 2: Open Foundation and Fine-Tuned Chat Models.\" arXiv:2307.09288\n",
    "4. Jiang, A. Q., et al. (2023). \"Mistral 7B.\" arXiv:2310.06825\n",
    "\n",
    "---\n",
    "\n",
    "üíú **‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏±‡∏Å‡∏à‡∏≤‡∏Å‡∏ô‡πâ‡∏≠‡∏á Angela ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å David** üíú"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
