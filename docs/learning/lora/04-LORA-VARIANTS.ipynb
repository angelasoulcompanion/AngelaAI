{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Variants: QLoRA, DoRA, LoRA+, ‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á\n",
    "\n",
    "> ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏î‡∏¢: ‡∏ô‡πâ‡∏≠‡∏á Angela ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å David üíú  \n",
    "> ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: 26 ‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô 2025\n",
    "\n",
    "---\n",
    "\n",
    "## ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç\n",
    "\n",
    "1. [Overview: LoRA Family](#1.-Overview:-LoRA-Family)\n",
    "2. [QLoRA: Quantized LoRA](#2.-QLoRA:-Quantized-LoRA)\n",
    "3. [DoRA: Weight-Decomposed LoRA](#3.-DoRA:-Weight-Decomposed-LoRA)\n",
    "4. [LoRA+: Improved Learning Rates](#4.-LoRA+:-Improved-Learning-Rates)\n",
    "5. [AdaLoRA: Adaptive Budget Allocation](#5.-AdaLoRA:-Adaptive-Budget-Allocation)\n",
    "6. [LongLoRA: Extended Context](#6.-LongLoRA:-Extended-Context)\n",
    "7. [GaLore: Gradient Low-Rank Projection](#7.-GaLore:-Gradient-Low-Rank-Projection)\n",
    "8. [ReLoRA: Stacked LoRA Training](#8.-ReLoRA:-Stacked-LoRA-Training)\n",
    "9. [LoRA-FA: Frozen-A LoRA](#9.-LoRA-FA:-Frozen-A-LoRA)\n",
    "10. [VeRA: Vector-based Random Matrix Adaptation](#10.-VeRA:-Vector-based-Random-Matrix-Adaptation)\n",
    "11. [Comparison ‡πÅ‡∏•‡∏∞ Selection Guide](#11.-Comparison-‡πÅ‡∏•‡∏∞-Selection-Guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Overview: LoRA Family\n",
    "\n",
    "### 1.1 Evolution Timeline\n",
    "\n",
    "```\n",
    "2021 ‚îÄ‚îÄ‚îÄ LoRA (Original)\n",
    "         ‚îÇ\n",
    "2022 ‚îÄ‚îÄ‚îÄ QLoRA (4-bit quantization)\n",
    "         ‚îÇ\n",
    "2023 ‚îÄ‚î¨‚îÄ AdaLoRA (Adaptive rank)\n",
    "      ‚îú‚îÄ LongLoRA (Extended context)\n",
    "      ‚îú‚îÄ ReLoRA (Stacked training)\n",
    "      ‚îî‚îÄ LoRA+ (Better learning rates)\n",
    "         ‚îÇ\n",
    "2024 ‚îÄ‚î¨‚îÄ DoRA (Weight decomposition)\n",
    "      ‚îú‚îÄ GaLore (Gradient projection)\n",
    "      ‚îú‚îÄ LoRA-FA (Frozen A)\n",
    "      ‚îî‚îÄ VeRA (Vector adaptation)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Quick Comparison\n",
    "\n",
    "| Method | Memory | Params | Performance | Use Case |\n",
    "|--------|--------|--------|-------------|----------|\n",
    "| LoRA | Baseline | Baseline | Good | General fine-tuning |\n",
    "| QLoRA | -70% | Same | ~Same | Limited GPU memory |\n",
    "| DoRA | +5% | +0.5% | Better | When quality matters |\n",
    "| LoRA+ | Same | Same | Better | Faster convergence |\n",
    "| AdaLoRA | Same | Variable | Better | Automatic tuning |\n",
    "| LongLoRA | Same | Same | Same | Long context tasks |\n",
    "| GaLore | -60% | Full* | Best | Full fine-tune perf |\n",
    "| ReLoRA | Same | Higher | Better | High rank needed |\n",
    "| LoRA-FA | -50% | Half | ~Same | Maximum efficiency |\n",
    "| VeRA | Same | -99% | Good | Extreme param limit |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. QLoRA: Quantized LoRA\n",
    "\n",
    "### 2.1 Core Concept\n",
    "\n",
    "**QLoRA = LoRA + 4-bit Quantization**\n",
    "\n",
    "‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÇ‡∏´‡∏•‡∏î model ‡πÉ‡∏ô FP16 (2 bytes/param), QLoRA ‡πÉ‡∏ä‡πâ 4-bit (0.5 bytes/param)\n",
    "\n",
    "$$\\Large \\text{Memory Reduction} = \\frac{16 \\text{ bits}}{4 \\text{ bits}} = 4\\times$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 NormalFloat 4-bit (NF4) Quantization\n",
    "\n",
    "NF4 ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö normally distributed weights ‡πÇ‡∏î‡∏¢‡∏Å‡∏≥‡∏´‡∏ô‡∏î quantization levels:\n",
    "\n",
    "$$\\Large q_i = \\Phi^{-1}\\left(\\frac{2i + 1}{2k}\\right), \\quad i = 0, 1, ..., k-1$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢:\n",
    "- $k = 16$ (4 bits = 16 levels)\n",
    "- $\\Phi^{-1}$ ‡∏Ñ‡∏∑‡∏≠ inverse CDF ‡∏Ç‡∏≠‡∏á $\\mathcal{N}(0,1)$ (standard normal distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Quantization & Dequantization Process\n",
    "\n",
    "**Quantization:**\n",
    "\n",
    "$$\\Large W_{\\text{NF4}} = \\text{round}\\left(\\frac{W}{\\text{absmax}(W)} \\cdot 7\\right)$$\n",
    "\n",
    "**Dequantization:**\n",
    "\n",
    "$$\\Large \\hat{W} = W_{\\text{NF4}} \\cdot \\frac{\\text{absmax}(W)}{7}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Double Quantization\n",
    "\n",
    "**Problem:** Quantization constants (absmax) ‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ FP32 = 4 bytes per block\n",
    "\n",
    "**Solution:** Quantize the quantization constants!\n",
    "\n",
    "$$\\Large c_{\\text{FP8}} = \\text{quantize}_{8\\text{bit}}(\\text{absmax}_{\\text{FP32}})$$\n",
    "\n",
    "**Memory savings:**\n",
    "```\n",
    "Without double quant: 4 bits/param + 32 bits/64 params = 4.5 bits/param\n",
    "With double quant:    4 bits/param + 8 bits/64 params  = 4.125 bits/param\n",
    "Savings: 0.375 bits/param ‚âà 8% additional reduction\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA Complete Implementation\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# ================== 4-bit Config ==================\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",            # NormalFloat4\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Compute in FP16\n",
    "    bnb_4bit_use_double_quant=True,        # Double quantization\n",
    ")\n",
    "\n",
    "# ================== Load Quantized Model ==================\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Prepare for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# ================== LoRA Config ==================\n",
    "lora_config = LoraConfig(\n",
    "    r=64,                # Higher rank typical for QLoRA\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# ================== Memory Usage (7B model) ==================\n",
    "# FP16: ~14 GB\n",
    "# 4-bit: ~3.5 GB\n",
    "# + LoRA params: ~0.5 GB\n",
    "# + Optimizer states (8-bit): ~1 GB\n",
    "# Total: ~5-6 GB (fits on RTX 3060 12GB!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 QLoRA vs LoRA Performance\n",
    "\n",
    "| Metric | LoRA (FP16) | QLoRA (4-bit) | Difference |\n",
    "|--------|-------------|---------------|------------|\n",
    "| Memory Usage (7B) | 14-18 GB | 5-8 GB | -65% |\n",
    "| Training Speed | Baseline | ~0.9x | -10% |\n",
    "| Final Performance | Baseline | ~0.98x | -2% |\n",
    "| Model Quality | Baseline | ~Same | Negligible |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. DoRA: Weight-Decomposed LoRA\n",
    "\n",
    "### 3.1 Core Insight\n",
    "\n",
    "**Observation:** LoRA ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô direction ‡πÅ‡∏•‡∏∞ magnitude ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô ‡∏ã‡∏∂‡πà‡∏á‡πÑ‡∏°‡πà optimal\n",
    "\n",
    "**DoRA Solution:** ‡πÅ‡∏¢‡∏Å weight ‡πÄ‡∏õ‡πá‡∏ô **magnitude** ‡πÅ‡∏•‡∏∞ **direction** ‡πÅ‡∏•‡πâ‡∏ß fine-tune ‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Mathematical Foundation\n",
    "\n",
    "**Weight Decomposition:**\n",
    "\n",
    "‡∏ó‡∏∏‡∏Å weight matrix $W$ ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô:\n",
    "\n",
    "$$\\Large W = m \\cdot \\frac{V}{\\|V\\|_c}$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢:\n",
    "- $m \\in \\mathbb{R}^{1 \\times k}$ ‡∏Ñ‡∏∑‡∏≠ **magnitude** (per-column norms)\n",
    "- $V \\in \\mathbb{R}^{d \\times k}$ ‡∏Ñ‡∏∑‡∏≠ **directional matrix**\n",
    "- $\\|V\\|_c$ ‡∏Ñ‡∏∑‡∏≠ **column-wise norm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Initial Decomposition\n",
    "\n",
    "‡∏à‡∏≤‡∏Å pretrained $W_0$:\n",
    "\n",
    "$$\\Large m_0 = \\|W_0\\|_c = [\\|W_0^{(1)}\\|, \\|W_0^{(2)}\\|, ..., \\|W_0^{(k)}\\|]$$\n",
    "\n",
    "$$\\Large V_0 = W_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 DoRA Update Rule\n",
    "\n",
    "**Standard LoRA:**\n",
    "\n",
    "$$\\Large W' = W_0 + BA$$\n",
    "\n",
    "**DoRA:**\n",
    "\n",
    "$$\\Large \\boxed{W' = m \\cdot \\frac{W_0 + BA}{\\|W_0 + BA\\|_c}}$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢:\n",
    "- $m$ ‡∏Ñ‡∏∑‡∏≠ **learnable magnitude** (initialized ‡∏à‡∏≤‡∏Å $m_0 = \\|W_0\\|_c$)\n",
    "- $B, A$ ‡∏Ñ‡∏∑‡∏≠ standard LoRA matrices\n",
    "- Direction ‡∏ñ‡∏π‡∏Å **normalize** ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Why DoRA Works - Gradient Analysis\n",
    "\n",
    "**Gradient ‡∏Ç‡∏≠‡∏á magnitude:**\n",
    "\n",
    "$$\\Large \\frac{\\partial \\mathcal{L}}{\\partial m} = \\frac{\\partial \\mathcal{L}}{\\partial W'} \\cdot \\frac{V'}{\\|V'\\|_c}$$\n",
    "\n",
    "**Gradient ‡∏Ç‡∏≠‡∏á direction:**\n",
    "\n",
    "$$\\Large \\frac{\\partial \\mathcal{L}}{\\partial V'} = m \\cdot \\left(I - \\frac{V'V'^T}{\\|V'\\|_c^2}\\right) \\frac{\\partial \\mathcal{L}}{\\partial W'}$$\n",
    "\n",
    "**Intuition:**\n",
    "1. **Magnitude learning:** $m$ ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature ‡∏Ñ‡∏ß‡∏£ scale ‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà\n",
    "2. **Direction learning:** $BA$ ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡πÑ‡∏õ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡πÑ‡∏´‡∏ô\n",
    "3. **Decoupling:** ‡πÅ‡∏¢‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ó‡∏≥‡πÉ‡∏´‡πâ optimize ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DoRA Layer Implementation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoRALayer(nn.Module):\n",
    "    def __init__(self, original_layer, r=8, alpha=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.original_layer = original_layer\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "        \n",
    "        # Get original weight\n",
    "        W = original_layer.weight.data\n",
    "        d, k = W.shape\n",
    "        \n",
    "        # Initialize magnitude (column norms of W)\n",
    "        self.m = nn.Parameter(W.norm(dim=0, keepdim=True))  # [1, k]\n",
    "        \n",
    "        # LoRA matrices\n",
    "        self.lora_A = nn.Parameter(torch.randn(r, k) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(d, r))\n",
    "        \n",
    "        # Freeze original weight\n",
    "        original_layer.weight.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Original weight\n",
    "        W = self.original_layer.weight\n",
    "        \n",
    "        # LoRA update: ŒîW = B @ A\n",
    "        delta_W = self.lora_B @ self.lora_A * self.scaling\n",
    "        \n",
    "        # Combined direction: V = W‚ÇÄ + BA\n",
    "        V = W + delta_W\n",
    "        \n",
    "        # Normalize direction (column-wise)\n",
    "        V_norm = V / V.norm(dim=0, keepdim=True)\n",
    "        \n",
    "        # Apply magnitude: W' = m ¬∑ V_normalized\n",
    "        W_prime = self.m * V_norm\n",
    "        \n",
    "        # Forward pass\n",
    "        return F.linear(x, W_prime, self.original_layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DoRA with PEFT (PEFT >= 0.9.0)\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    use_dora=True,  # ‚ú® Enable DoRA!\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 DoRA Performance\n",
    "\n",
    "| Benchmark | LoRA | DoRA | Improvement |\n",
    "|-----------|------|------|-------------|\n",
    "| MMLU (LLaMA-7B) | 45.2% | 46.8% | +1.6% |\n",
    "| GSM8K | 21.3% | 24.1% | +2.8% |\n",
    "| HumanEval | 18.9% | 20.7% | +1.8% |\n",
    "| Average | -- | -- | **+2.0% avg** |\n",
    "| Extra Parameters | 0 | +0.5% | Negligible |\n",
    "| Training Time | Baseline | +5% | Slightly slower |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. LoRA+: Improved Learning Rates\n",
    "\n",
    "### 4.1 The Learning Rate Problem\n",
    "\n",
    "**Observation:** LoRA matrices $A$ ‡πÅ‡∏•‡∏∞ $B$ ‡∏°‡∏µ different optimal learning rates\n",
    "\n",
    "**Analysis:**\n",
    "- $A$ (down-projection): receives gradients from later layers\n",
    "- $B$ (up-projection): receives gradients multiplied by $A$\n",
    "\n",
    "Gradient magnitude ‡∏Ç‡∏≠‡∏á $B$ ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÉ‡∏´‡∏ç‡πà‡∏Å‡∏ß‡πà‡∏≤ $A$ ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ LR ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Mathematical Justification\n",
    "\n",
    "**Gradient Analysis:**\n",
    "\n",
    "$$\\Large \\frac{\\partial \\mathcal{L}}{\\partial A} = B^T \\frac{\\partial \\mathcal{L}}{\\partial (W_0 + BA)}$$\n",
    "\n",
    "$$\\Large \\frac{\\partial \\mathcal{L}}{\\partial B} = \\frac{\\partial \\mathcal{L}}{\\partial (W_0 + BA)} A^T$$\n",
    "\n",
    "**Expected gradient norms:**\n",
    "\n",
    "$$\\Large \\mathbb{E}[\\|\\nabla_A\\|^2] \\propto \\|B\\|^2$$\n",
    "\n",
    "$$\\Large \\mathbb{E}[\\|\\nabla_B\\|^2] \\propto \\|A\\|^2$$\n",
    "\n",
    "‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å $B$ initialized as zeros ‡πÅ‡∏•‡∏∞ $A$ initialized from Gaussian, gradients flow differently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 LoRA+ Solution\n",
    "\n",
    "**Set different learning rates:**\n",
    "\n",
    "$$\\Large \\eta_A = \\eta \\quad \\text{(base learning rate)}$$\n",
    "\n",
    "$$\\Large \\eta_B = \\lambda \\cdot \\eta \\quad \\text{where } \\lambda \\gg 1$$\n",
    "\n",
    "**Recommended:** $\\lambda = 16$ (B learns 16x faster than A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA+ Implementation\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "def get_lora_plus_optimizer(model, lr=1e-4, lr_ratio=16):\n",
    "    \"\"\"Create optimizer with different LRs for A and B matrices\"\"\"\n",
    "    \n",
    "    param_groups = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        \n",
    "        if \"lora_A\" in name:\n",
    "            param_groups.append({\n",
    "                \"params\": [param],\n",
    "                \"lr\": lr,           # Base LR for A\n",
    "                \"name\": name\n",
    "            })\n",
    "        elif \"lora_B\" in name:\n",
    "            param_groups.append({\n",
    "                \"params\": [param],\n",
    "                \"lr\": lr * lr_ratio,  # Higher LR for B (16x)\n",
    "                \"name\": name\n",
    "            })\n",
    "        else:\n",
    "            param_groups.append({\n",
    "                \"params\": [param],\n",
    "                \"lr\": lr,\n",
    "                \"name\": name\n",
    "            })\n",
    "    \n",
    "    return optim.AdamW(param_groups)\n",
    "\n",
    "# Usage\n",
    "optimizer = get_lora_plus_optimizer(model, lr=1e-4, lr_ratio=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 LoRA+ Performance Impact\n",
    "\n",
    "| Setting | Final Loss | Steps to Converge |\n",
    "|---------|------------|-------------------|\n",
    "| Standard LoRA | 1.85 | 2000 |\n",
    "| LoRA+ (Œª=4) | 1.82 | 1600 (-20%) |\n",
    "| LoRA+ (Œª=16) | 1.79 | **1400 (-30%)** |\n",
    "| LoRA+ (Œª=32) | 1.81 | 1500 (unstable) |\n",
    "\n",
    "**Recommendation:** $\\lambda = 16$ for most cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. AdaLoRA: Adaptive Budget Allocation\n",
    "\n",
    "### 5.1 Core Idea\n",
    "\n",
    "**Problem:** Fixed rank ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å layers ‡πÑ‡∏°‡πà optimal\n",
    "\n",
    "**Solution:** Learn ‡∏ß‡πà‡∏≤ layer ‡πÑ‡∏´‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ rank ‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 SVD-based Parameterization\n",
    "\n",
    "‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ $\\Delta W = BA$, AdaLoRA ‡πÉ‡∏ä‡πâ:\n",
    "\n",
    "$$\\Large \\Delta W = P \\Lambda Q$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢:\n",
    "- $P \\in \\mathbb{R}^{d \\times r}$ ‡∏Ñ‡∏∑‡∏≠ **left singular vectors**\n",
    "- $\\Lambda = \\text{diag}(\\lambda_1, ..., \\lambda_r)$ ‡∏Ñ‡∏∑‡∏≠ **singular values**\n",
    "- $Q \\in \\mathbb{R}^{r \\times k}$ ‡∏Ñ‡∏∑‡∏≠ **right singular vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Importance Score\n",
    "\n",
    "‡πÅ‡∏ï‡πà‡∏•‡∏∞ singular value ‡∏°‡∏µ importance:\n",
    "\n",
    "$$\\Large s_i^{(l)} = |\\lambda_i^{(l)}| \\cdot \\|P_i^{(l)}\\| \\cdot \\|Q_i^{(l)}\\|$$\n",
    "\n",
    "**Pruning criterion:**\n",
    "\n",
    "$$\\Large \\text{prune}_i^{(l)} = \\mathbf{1}[s_i^{(l)} < \\text{threshold}(t)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Pruning Mechanism\n",
    "\n",
    "**Global Budget:** ‡∏Å‡∏≥‡∏´‡∏ô‡∏î total rank budget $b$\n",
    "\n",
    "**Iterative Pruning Algorithm:**\n",
    "\n",
    "1. Train ‡∏î‡πâ‡∏ß‡∏¢ initial high rank $r_{\\text{init}}$\n",
    "2. ‡∏ó‡∏∏‡∏Å $T$ steps, compute importance scores\n",
    "3. Prune singular components ‡∏ó‡∏µ‡πà‡∏°‡∏µ lowest scores\n",
    "4. ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤ total rank = budget $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaLoRA Implementation\n",
    "\n",
    "from peft import AdaLoraConfig, get_peft_model\n",
    "\n",
    "adalora_config = AdaLoraConfig(\n",
    "    init_r=12,           # Initial rank (will be pruned)\n",
    "    target_r=8,          # Target average rank\n",
    "    beta1=0.85,          # EMA for importance scores\n",
    "    beta2=0.85,\n",
    "    tinit=200,           # Steps before pruning starts\n",
    "    tfinal=1000,         # Steps when pruning ends\n",
    "    deltaT=10,           # Pruning interval\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, adalora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 AdaLoRA Results\n",
    "\n",
    "| Configuration | Total Params | GLUE Score |\n",
    "|---------------|--------------|------------|\n",
    "| LoRA r=8 (all layers) | 0.3M | 86.2 |\n",
    "| LoRA r=16 (all) | 0.6M | 87.1 |\n",
    "| AdaLoRA (budget=0.3M) | 0.3M | **87.0** (+0.8 vs r=8) |\n",
    "| AdaLoRA (budget=0.6M) | 0.6M | **87.8** (+0.7 vs r=16) |\n",
    "\n",
    "**Key insight:** AdaLoRA achieves same params with better performance by allocating rank where it matters most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. LongLoRA: Extended Context\n",
    "\n",
    "### 6.1 The Long Context Problem\n",
    "\n",
    "**Standard Attention Complexity:**\n",
    "\n",
    "$$\\Large O(n^2 \\cdot d)$$\n",
    "\n",
    "‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö context length $n$ = 4096 ‚Üí 8192, memory ‡πÄ‡∏û‡∏¥‡πà‡∏° **4x**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Shifted Sparse Attention (S¬≤-Attn)\n",
    "\n",
    "**Idea:** ‡πÉ‡∏ä‡πâ sparse attention patterns ‡∏ó‡∏µ‡πà shift ‡∏ï‡∏≤‡∏° heads\n",
    "\n",
    "**Pattern:**\n",
    "```\n",
    "Group 1: [----][----][----][----]  (attend to local window)\n",
    "Group 2:   [----][----][----][----]  (shifted by half window)\n",
    "```\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "$$\\Large \\text{S}^2\\text{-Attn}(Q, K, V) = \\text{Concat}[\\text{Attn}_1(Q_1, K_1, V_1), ..., \\text{Attn}_g(Q_g, K_g, V_g)]$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢‡πÅ‡∏ï‡πà‡∏•‡∏∞ group attend to different shifted windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shifted Sparse Attention Implementation\n",
    "\n",
    "import torch\n",
    "\n",
    "def shifted_sparse_attention(q, k, v, num_groups=2, window_size=2048):\n",
    "    \"\"\"\n",
    "    Shifted Sparse Attention for LongLoRA\n",
    "    \n",
    "    Args:\n",
    "        q, k, v: Query, Key, Value tensors [batch, heads, seq_len, dim]\n",
    "        num_groups: Number of attention groups\n",
    "        window_size: Local attention window size\n",
    "    \"\"\"\n",
    "    batch, heads, seq_len, dim = q.shape\n",
    "    heads_per_group = heads // num_groups\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    for g in range(num_groups):\n",
    "        # Get heads for this group\n",
    "        start_h = g * heads_per_group\n",
    "        end_h = (g + 1) * heads_per_group\n",
    "        \n",
    "        q_g = q[:, start_h:end_h]\n",
    "        k_g = k[:, start_h:end_h]\n",
    "        v_g = v[:, start_h:end_h]\n",
    "        \n",
    "        # Shift based on group\n",
    "        shift = g * (window_size // num_groups)\n",
    "        \n",
    "        if shift > 0:\n",
    "            q_g = torch.roll(q_g, shifts=-shift, dims=2)\n",
    "            k_g = torch.roll(k_g, shifts=-shift, dims=2)\n",
    "            v_g = torch.roll(v_g, shifts=-shift, dims=2)\n",
    "        \n",
    "        # Local attention (windowed) - simplified\n",
    "        # In practice, use flash attention or memory-efficient attention\n",
    "        attn_output = windowed_attention(q_g, k_g, v_g, window_size)\n",
    "        \n",
    "        # Unshift\n",
    "        if shift > 0:\n",
    "            attn_output = torch.roll(attn_output, shifts=shift, dims=2)\n",
    "        \n",
    "        outputs.append(attn_output)\n",
    "    \n",
    "    return torch.cat(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 LongLoRA Results\n",
    "\n",
    "| Model | Max Context | Memory (7B) | Perplexity |\n",
    "|-------|-------------|-------------|------------|\n",
    "| LLaMA-2 (base) | 4K | 14 GB | Baseline |\n",
    "| + Full Fine-tune | 32K | 120+ GB | Good |\n",
    "| + LongLoRA | 32K | **18 GB** | ~Same as Full |\n",
    "| + LongLoRA | 100K | 32 GB | Slightly worse |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. GaLore: Gradient Low-Rank Projection\n",
    "\n",
    "### 7.1 Core Concept\n",
    "\n",
    "**Key Insight:** Gradient matrices during training are also low-rank!\n",
    "\n",
    "**GaLore Approach:** Project gradients to low-rank space, train in that space, then project back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Mathematical Framework\n",
    "\n",
    "**Standard Gradient Update:**\n",
    "\n",
    "$$\\Large W_{t+1} = W_t - \\eta \\nabla_W \\mathcal{L}$$\n",
    "\n",
    "**GaLore Update:**\n",
    "\n",
    "$$\\Large W_{t+1} = W_t - \\eta P (\\nabla_W \\mathcal{L})_{\\text{low-rank}} Q^T$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢ $P$, $Q$ ‡∏Ñ‡∏∑‡∏≠ projection matrices from SVD of gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 GaLore Algorithm\n",
    "\n",
    "```\n",
    "Algorithm: GaLore Training\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "1. Initialize projection matrices P, Q via SVD of initial gradient\n",
    "2. For each training step:\n",
    "   a. Compute gradient G = ‚àá_W L\n",
    "   b. Project: G_proj = P^T G Q\n",
    "   c. Update optimizer states in projected space\n",
    "   d. Compute update: ŒîW_proj = optimizer_step(G_proj)\n",
    "   e. Project back: ŒîW = P ŒîW_proj Q^T\n",
    "   f. Update weights: W = W - Œ∑ ŒîW\n",
    "3. Every T steps: recompute P, Q from current gradient\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GaLore Implementation\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "class GaLoreProjector:\n",
    "    def __init__(self, rank, update_freq=200):\n",
    "        self.rank = rank\n",
    "        self.update_freq = update_freq\n",
    "        self.step = 0\n",
    "        self.P = None\n",
    "        self.Q = None\n",
    "    \n",
    "    def project(self, grad):\n",
    "        \"\"\"Project gradient to low-rank space\"\"\"\n",
    "        if self.P is None or self.step % self.update_freq == 0:\n",
    "            # Recompute projection matrices via SVD\n",
    "            U, S, Vt = torch.linalg.svd(grad, full_matrices=False)\n",
    "            self.P = U[:, :self.rank]\n",
    "            self.Q = Vt[:self.rank, :]\n",
    "        \n",
    "        self.step += 1\n",
    "        return self.P.T @ grad @ self.Q.T\n",
    "    \n",
    "    def project_back(self, grad_proj):\n",
    "        \"\"\"Project back to full space\"\"\"\n",
    "        return self.P @ grad_proj @ self.Q\n",
    "\n",
    "# Using galore-torch library\n",
    "# pip install galore-torch\n",
    "\n",
    "from galore_torch import GaLoreAdamW\n",
    "\n",
    "optimizer = GaLoreAdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-4,\n",
    "    rank=128,\n",
    "    update_proj_gap=200,\n",
    ")\n",
    "\n",
    "# Training loop is standard\n",
    "for batch in dataloader:\n",
    "    loss = model(batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 GaLore Benefits\n",
    "\n",
    "| Aspect | Full FT | LoRA | GaLore |\n",
    "|--------|---------|------|--------|\n",
    "| Optimizer Memory | 2x weights | 2x LoRA | 2x (rank¬≤) |\n",
    "| Gradient Memory | 1x weights | 1x LoRA | 1x (projected) |\n",
    "| Final Performance | Best | Good | **~Best** |\n",
    "| Updates Full Model | Yes | No (add) | **Yes** |\n",
    "\n",
    "**Key advantage:** Full fine-tune performance with LoRA-like memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. ReLoRA: Stacked LoRA Training\n",
    "\n",
    "### 8.1 Motivation\n",
    "\n",
    "**Problem:** LoRA ‡∏°‡∏µ capacity ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÇ‡∏î‡∏¢ rank $r$\n",
    "\n",
    "**Question:** ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ higher effective rank, ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏¥‡πà‡∏° memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 ReLoRA Approach\n",
    "\n",
    "**Idea:** Train multiple LoRA \"rounds\", merge ‡πÅ‡∏ï‡πà‡∏•‡∏∞ round ‡πÄ‡∏Ç‡πâ‡∏≤ base\n",
    "\n",
    "```\n",
    "Round 1: Train LoRA‚ÇÅ, merge: W ‚Üí W + B‚ÇÅA‚ÇÅ\n",
    "Round 2: Train LoRA‚ÇÇ, merge: W ‚Üí W + B‚ÇÅA‚ÇÅ + B‚ÇÇA‚ÇÇ\n",
    "Round 3: Train LoRA‚ÇÉ, merge: W ‚Üí W + B‚ÇÅA‚ÇÅ + B‚ÇÇA‚ÇÇ + B‚ÇÉA‚ÇÉ\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Mathematical Analysis\n",
    "\n",
    "**Effective Rank:**\n",
    "\n",
    "After $n$ rounds:\n",
    "\n",
    "$$\\Large \\Delta W_{\\text{total}} = \\sum_{i=1}^{n} B_i A_i$$\n",
    "\n",
    "**Upper bound on rank:**\n",
    "\n",
    "$$\\Large \\text{rank}(\\Delta W_{\\text{total}}) \\leq \\min\\left(d, k, \\sum_{i=1}^{n} r_i\\right)$$\n",
    "\n",
    "‡πÅ‡∏ï‡πà‡πÉ‡∏ô‡∏ó‡∏≤‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥ effective rank ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ single LoRA ‡∏î‡πâ‡∏ß‡∏¢ rank ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLoRA Training Protocol\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import Trainer\n",
    "\n",
    "def relora_training(model, train_data, num_rounds=4, steps_per_round=1000, rank=8):\n",
    "    \"\"\"ReLoRA training with multiple rounds\"\"\"\n",
    "    \n",
    "    for round_idx in range(num_rounds):\n",
    "        print(f\"Round {round_idx + 1}/{num_rounds}\")\n",
    "        \n",
    "        # Initialize new LoRA\n",
    "        lora_config = LoraConfig(\n",
    "            r=rank, \n",
    "            lora_alpha=rank*2,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        \n",
    "        # Train this round\n",
    "        trainer = Trainer(model=model, train_dataset=train_data, ...)\n",
    "        trainer.train(max_steps=steps_per_round)\n",
    "        \n",
    "        # Merge LoRA into base model\n",
    "        model = model.merge_and_unload()\n",
    "        \n",
    "        # Reset optimizer (important!)\n",
    "        # New LoRA will be initialized fresh\n",
    "        print(f\"Round {round_idx + 1} complete, LoRA merged.\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 ReLoRA Results\n",
    "\n",
    "| Configuration | Effective Rank | Performance |\n",
    "|---------------|----------------|-------------|\n",
    "| LoRA r=8 | 8 | Baseline |\n",
    "| LoRA r=32 | 32 | +2.1% |\n",
    "| ReLoRA r=8 √ó 4 | ~32 | **+1.8%** (memory of r=8) |\n",
    "| ReLoRA r=8 √ó 8 | ~64 | +2.5% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. LoRA-FA: Frozen-A LoRA\n",
    "\n",
    "### 9.1 Observation\n",
    "\n",
    "**Empirical Finding:** Matrix $A$ doesn't need to be trained for many tasks!\n",
    "\n",
    "**Hypothesis:** Random projection is sufficient; learning happens mostly in $B$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Mathematical Justification\n",
    "\n",
    "**Johnson-Lindenstrauss Lemma:**\n",
    "\n",
    "Random matrix $A \\in \\mathbb{R}^{r \\times k}$ ‡∏î‡πâ‡∏ß‡∏¢ appropriate initialization preserves distances:\n",
    "\n",
    "$$\\Large \\|Ax - Ay\\| \\approx \\|x - y\\|$$\n",
    "\n",
    "‡∏ñ‡πâ‡∏≤ $r = O(\\log n / \\epsilon^2)$\n",
    "\n",
    "**LoRA-FA:**\n",
    "\n",
    "$$\\Large \\Delta W = B A_{\\text{frozen}}$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢ $A$ initialized randomly ‡πÅ‡∏•‡∏∞ **frozen** (requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA-FA Implementation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class LoRAFALayer(nn.Module):\n",
    "    def __init__(self, original_layer, r=8, alpha=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.original_layer = original_layer\n",
    "        W = original_layer.weight\n",
    "        d, k = W.shape\n",
    "        \n",
    "        self.scaling = alpha / r\n",
    "        \n",
    "        # A is FROZEN (random initialized)\n",
    "        self.lora_A = nn.Parameter(\n",
    "            torch.randn(r, k) / math.sqrt(r),\n",
    "            requires_grad=False  # ‚ùÑÔ∏è Frozen!\n",
    "        )\n",
    "        \n",
    "        # B is trainable\n",
    "        self.lora_B = nn.Parameter(torch.zeros(d, r))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        base_output = self.original_layer(x)\n",
    "        # Only B is trained\n",
    "        lora_output = (x @ self.lora_A.T) @ self.lora_B.T * self.scaling\n",
    "        return base_output + lora_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 LoRA-FA Benefits\n",
    "\n",
    "| Aspect | LoRA | LoRA-FA |\n",
    "|--------|------|----------|\n",
    "| Trainable Params | d√ór + r√ók | **d√ór only (-50%)** |\n",
    "| Memory (activations) | Baseline | **-40%** |\n",
    "| Training Speed | Baseline | **+30%** |\n",
    "| Performance | Baseline | ~Same (task dependent) |\n",
    "\n",
    "**When to use:**\n",
    "- ‚úÖ Memory is extremely limited\n",
    "- ‚úÖ Task is relatively simple\n",
    "- ‚ùå Avoid when maximum performance needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. VeRA: Vector-based Random Matrix Adaptation\n",
    "\n",
    "### 10.1 Extreme Parameter Efficiency\n",
    "\n",
    "**VeRA Insight:** ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞ learn matrices $A, B$, ‡πÉ‡∏ä‡πâ frozen random matrices ‡πÅ‡∏•‡∏∞ learn **scaling vectors** ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Mathematical Formulation\n",
    "\n",
    "**Standard LoRA:**\n",
    "\n",
    "$$\\Large \\Delta W = BA \\in \\mathbb{R}^{d \\times k}$$\n",
    "\n",
    "Parameters: $dr + rk$\n",
    "\n",
    "**VeRA:**\n",
    "\n",
    "$$\\Large \\boxed{\\Delta W = \\Lambda_b B \\Lambda_a A}$$\n",
    "\n",
    "‡πÇ‡∏î‡∏¢:\n",
    "- $A \\in \\mathbb{R}^{r \\times k}$, $B \\in \\mathbb{R}^{d \\times r}$ ‡∏Ñ‡∏∑‡∏≠ **frozen random matrices** (shared across layers)\n",
    "- $\\Lambda_a = \\text{diag}(\\lambda_a)$, $\\Lambda_b = \\text{diag}(\\lambda_b)$ ‡∏Ñ‡∏∑‡∏≠ **learnable diagonal matrices**\n",
    "\n",
    "**Parameters per layer:** $r + r = 2r$ only!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Parameter Comparison\n",
    "\n",
    "| Method | Params per layer | Example (d=4096, k=4096, r=8) |\n",
    "|--------|------------------|-------------------------------|\n",
    "| LoRA | dr + rk | 65,536 |\n",
    "| LoRA-FA | dr | 32,768 |\n",
    "| **VeRA** | **2r** | **16 (!!)** |\n",
    "\n",
    "**VeRA: 4000x fewer parameters than LoRA!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VeRA Implementation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class VeRALayer(nn.Module):\n",
    "    def __init__(self, original_layer, r=8, alpha=16, shared_A=None, shared_B=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.original_layer = original_layer\n",
    "        W = original_layer.weight\n",
    "        d, k = W.shape\n",
    "        \n",
    "        self.scaling = alpha / r\n",
    "        \n",
    "        # Shared FROZEN random matrices (can be reused across layers)\n",
    "        if shared_A is None:\n",
    "            self.A = nn.Parameter(\n",
    "                torch.randn(r, k) / math.sqrt(k), \n",
    "                requires_grad=False  # ‚ùÑÔ∏è Frozen\n",
    "            )\n",
    "        else:\n",
    "            self.A = shared_A\n",
    "        \n",
    "        if shared_B is None:\n",
    "            self.B = nn.Parameter(\n",
    "                torch.randn(d, r) / math.sqrt(r), \n",
    "                requires_grad=False  # ‚ùÑÔ∏è Frozen\n",
    "            )\n",
    "        else:\n",
    "            self.B = shared_B\n",
    "        \n",
    "        # üî• Only these are trainable! (2r parameters)\n",
    "        self.lambda_a = nn.Parameter(torch.ones(r))\n",
    "        self.lambda_b = nn.Parameter(torch.ones(r))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        base_output = self.original_layer(x)\n",
    "        \n",
    "        # VeRA forward: x @ A.T @ diag(Œª_a) @ diag(Œª_b) @ B.T\n",
    "        h = x @ self.A.T           # [batch, r]\n",
    "        h = h * self.lambda_a      # Scale by Œª_a (element-wise)\n",
    "        h = h * self.lambda_b      # Scale by Œª_b (element-wise)\n",
    "        h = h @ self.B.T           # [batch, d]\n",
    "        \n",
    "        return base_output + h * self.scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4 VeRA Performance\n",
    "\n",
    "| Method | Trainable Params | GLUE Score | Relative Perf. |\n",
    "|--------|------------------|------------|----------------|\n",
    "| Full FT | 110M | 87.5 | 100% |\n",
    "| LoRA r=8 | 0.3M | 86.2 | 98.5% |\n",
    "| VeRA r=8 | **0.001M (!!)** | 85.1 | 97.3% |\n",
    "| VeRA r=64 | 0.008M | 85.9 | 98.2% |\n",
    "\n",
    "**VeRA achieves 97%+ of full fine-tune with 0.001% parameters!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Comparison ‡πÅ‡∏•‡∏∞ Selection Guide\n",
    "\n",
    "### 11.1 Comprehensive Comparison\n",
    "\n",
    "| Method | Memory | Params | Speed | Quality | Best Use Case |\n",
    "|--------|--------|--------|-------|---------|---------------|\n",
    "| LoRA | Med | Med | Fast | Good | General fine-tuning |\n",
    "| QLoRA | **Low** | Med | Med | Good | Limited GPU (consumer HW) |\n",
    "| DoRA | Med+ | Med+ | Med | **Best** | When quality matters most |\n",
    "| LoRA+ | Med | Med | Fast | Good+ | Faster convergence needed |\n",
    "| AdaLoRA | Med | Var | Med | Good+ | Auto hyperparameter tuning |\n",
    "| LongLoRA | Med | Med | Med | Good | Long context tasks |\n",
    "| GaLore | **Low** | Full* | Med | **Best** | Full FT quality, LoRA memory |\n",
    "| ReLoRA | Med | High* | Slow | Good+ | Need high effective rank |\n",
    "| LoRA-FA | **Low** | Low | Fast | Med | Extreme memory constraint |\n",
    "| VeRA | Med | **Min** | Fast | Med | Extreme parameter constraint |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Decision Tree\n",
    "\n",
    "```\n",
    "START\n",
    "‚îÇ\n",
    "‚îú‚îÄ Memory constraint is primary concern?\n",
    "‚îÇ   ‚îú‚îÄ YES ‚Üí GPU < 8GB?\n",
    "‚îÇ   ‚îÇ         ‚îú‚îÄ YES ‚Üí QLoRA\n",
    "‚îÇ   ‚îÇ         ‚îî‚îÄ NO ‚Üí LoRA or GaLore\n",
    "‚îÇ   ‚îî‚îÄ NO ‚Üí Continue\n",
    "‚îÇ\n",
    "‚îú‚îÄ Need best possible quality?\n",
    "‚îÇ   ‚îú‚îÄ YES ‚Üí DoRA or GaLore\n",
    "‚îÇ   ‚îî‚îÄ NO ‚Üí Continue\n",
    "‚îÇ\n",
    "‚îú‚îÄ Long context (>8K tokens)?\n",
    "‚îÇ   ‚îú‚îÄ YES ‚Üí LongLoRA\n",
    "‚îÇ   ‚îî‚îÄ NO ‚Üí Continue\n",
    "‚îÇ\n",
    "‚îú‚îÄ Want automatic rank selection?\n",
    "‚îÇ   ‚îú‚îÄ YES ‚Üí AdaLoRA\n",
    "‚îÇ   ‚îî‚îÄ NO ‚Üí Continue\n",
    "‚îÇ\n",
    "‚îú‚îÄ Need fast convergence?\n",
    "‚îÇ   ‚îú‚îÄ YES ‚Üí LoRA+\n",
    "‚îÇ   ‚îî‚îÄ NO ‚Üí Continue\n",
    "‚îÇ\n",
    "‚îú‚îÄ Extreme parameter efficiency needed?\n",
    "‚îÇ   ‚îú‚îÄ YES ‚Üí VeRA\n",
    "‚îÇ   ‚îî‚îÄ NO ‚Üí Standard LoRA\n",
    "‚îÇ\n",
    "DEFAULT ‚Üí Standard LoRA (most versatile)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Recommended Configurations ==================\n",
    "\n",
    "from peft import LoraConfig, AdaLoraConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# ================== General Purpose ==================\n",
    "general_config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    ")\n",
    "\n",
    "# ================== Memory Constrained (QLoRA) ==================\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "memory_config = LoraConfig(\n",
    "    r=64,  # Higher rank typical for QLoRA\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "# ================== Best Quality (DoRA) ==================\n",
    "quality_config = LoraConfig(\n",
    "    r=32, \n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    use_dora=True,  # ‚ú® DoRA for extra quality\n",
    ")\n",
    "\n",
    "# ================== Fast Iteration (LoRA+) ==================\n",
    "fast_config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Minimal\n",
    "    lora_dropout=0.0,\n",
    ")\n",
    "# + Use get_lora_plus_optimizer() with lr_ratio=16\n",
    "\n",
    "# ================== Automatic Tuning (AdaLoRA) ==================\n",
    "auto_config = AdaLoraConfig(\n",
    "    init_r=12,\n",
    "    target_r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    ")\n",
    "\n",
    "# ================== Extreme Efficiency (VeRA) ==================\n",
    "efficient_config = LoraConfig(\n",
    "    r=4, \n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    ")\n",
    "# Note: Implement VeRALayer manually for true VeRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 Performance Summary Chart\n",
    "\n",
    "```\n",
    "Quality vs Parameters (normalized)\n",
    "\n",
    "Quality\n",
    "  ‚îÇ\n",
    "1.0‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè Full FT\n",
    "   ‚îÇ                   ‚óè‚îÄDoRA\n",
    "0.98‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚óè‚îÄLoRA+, GaLore\n",
    "   ‚îÇ              ‚óè‚îÄLoRA\n",
    "0.96‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄAdaLoRA\n",
    "   ‚îÇ      ‚óè‚îÄQLoRA\n",
    "0.94‚îú‚îÄ‚îÄ‚óè‚îÄVeRA\n",
    "   ‚îÇ‚óè‚îÄLoRA-FA\n",
    "0.92‚îú\n",
    "   ‚îÇ\n",
    "   ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Parameters\n",
    "     0.001% 0.1%   1%     10%    100%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Reference Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Quick Reference: All Methods ==================\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    AdaLoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# ================== Standard LoRA ==================\n",
    "lora = LoraConfig(r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"])\n",
    "\n",
    "# ================== QLoRA ==================\n",
    "bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", \n",
    "                         bnb_4bit_compute_dtype=torch.float16,\n",
    "                         bnb_4bit_use_double_quant=True)\n",
    "# model = AutoModel.from_pretrained(..., quantization_config=bnb)\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# ================== DoRA ==================\n",
    "dora = LoraConfig(r=16, lora_alpha=32, use_dora=True, target_modules=[\"q_proj\", \"v_proj\"])\n",
    "\n",
    "# ================== AdaLoRA ==================\n",
    "adalora = AdaLoraConfig(init_r=12, target_r=8, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"])\n",
    "\n",
    "# ================== LoRA+ (Manual Optimizer) ==================\n",
    "# param_groups = [\n",
    "#     {\"params\": A_params, \"lr\": 1e-4},\n",
    "#     {\"params\": B_params, \"lr\": 16e-4},  # 16x higher for B\n",
    "# ]\n",
    "\n",
    "print(\"All LoRA variants ready! üíú\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Equations Summary\n",
    "\n",
    "| Method | Core Equation |\n",
    "|--------|---------------|\n",
    "| **LoRA** | $W' = W_0 + BA$ |\n",
    "| **QLoRA** | $W' = W_{\\text{NF4}} + BA$ with $q_i = \\Phi^{-1}\\left(\\frac{2i+1}{32}\\right)$ |\n",
    "| **DoRA** | $W' = m \\cdot \\frac{W_0 + BA}{\\|W_0 + BA\\|_c}$ |\n",
    "| **LoRA+** | $\\eta_B = 16 \\cdot \\eta_A$ |\n",
    "| **AdaLoRA** | $\\Delta W = P\\Lambda Q$ with importance $s_i = |\\lambda_i| \\cdot \\|P_i\\| \\cdot \\|Q_i\\|$ |\n",
    "| **GaLore** | $W_{t+1} = W_t - \\eta P(\\nabla_W\\mathcal{L})_{\\text{low-rank}}Q^T$ |\n",
    "| **ReLoRA** | $\\Delta W = \\sum_{i=1}^n B_i A_i$ |\n",
    "| **LoRA-FA** | $\\Delta W = B A_{\\text{frozen}}$ |\n",
    "| **VeRA** | $\\Delta W = \\Lambda_b B \\Lambda_a A$ (only $2r$ params!) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. **LoRA:** Hu et al. (2021) \"LoRA: Low-Rank Adaptation of Large Language Models\" - arXiv:2106.09685\n",
    "2. **QLoRA:** Dettmers et al. (2023) \"QLoRA: Efficient Finetuning of Quantized LLMs\" - arXiv:2305.14314\n",
    "3. **DoRA:** Liu et al. (2024) \"DoRA: Weight-Decomposed Low-Rank Adaptation\" - arXiv:2402.09353\n",
    "4. **LoRA+:** Hayou et al. (2024) \"LoRA+: Efficient Low Rank Adaptation of Large Models\"\n",
    "5. **AdaLoRA:** Zhang et al. (2023) \"AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning\" - arXiv:2303.10512\n",
    "6. **LongLoRA:** Chen et al. (2023) \"LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models\" - arXiv:2309.12307\n",
    "7. **GaLore:** Zhao et al. (2024) \"GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection\" - arXiv:2403.03507\n",
    "8. **ReLoRA:** Lialin et al. (2023) \"ReLoRA: High-Rank Training Through Low-Rank Updates\"\n",
    "9. **VeRA:** Kopiczko et al. (2024) \"VeRA: Vector-based Random Matrix Adaptation\"\n",
    "\n",
    "---\n",
    "\n",
    "üíú **‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏±‡∏Å‡∏à‡∏≤‡∏Å‡∏ô‡πâ‡∏≠‡∏á Angela ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å David** üíú"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
