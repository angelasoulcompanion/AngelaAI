# Train Angela Model from AngelaNova App
# ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£ Train Model ‡∏ú‡πà‡∏≤‡∏ô AngelaNova macOS App

**Created:** 2025-10-15
**Purpose:** Train Angela's model directly from AngelaNova macOS app with one button click!

---

## üéØ **Overview**

Angela ‡πÑ‡∏î‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö Train Model ‡πÉ‡∏ô **AngelaNova** ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏∞! David ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ:
- ‚úÖ ‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏° Training
- ‚úÖ ‡∏î‡∏π Training Progress ‡πÅ‡∏ö‡∏ö Real-time
- ‚úÖ ‡∏´‡∏¢‡∏∏‡∏î Training ‡πÑ‡∏î‡πâ‡∏ï‡∏•‡∏≠‡∏î‡πÄ‡∏ß‡∏•‡∏≤
- ‚úÖ ‡∏î‡∏π Training Logs
- ‚úÖ ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Training Configuration (epochs, LoRA rank, etc.)

---

## üöÄ **Quick Start**

### **Step 1: ‡πÄ‡∏õ‡∏¥‡∏î AngelaNova App**

```bash
# ‡πÄ‡∏õ‡∏¥‡∏î Xcode ‡πÅ‡∏•‡∏∞ Run AngelaNova
open /Users/davidsamanyaporn/PycharmProjects/AngelaAI/AngelaNativeApp/AngelaNativeApp.xcodeproj
```

### **Step 2: ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Model Training Tab**

‡πÉ‡∏ô AngelaNova app ‡∏à‡∏∞‡∏°‡∏µ **"Model Training"** tab (icon: üß† brain.head.profile)

### **Step 3: Configure Training**

‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á settings ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£:
- **Extract latest data** ‚úÖ - ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å database
- **Format dataset** ‚úÖ - ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö dataset
- **Fine-tune model** ‚úÖ - Train model ‡∏î‡πâ‡∏ß‡∏¢ LoRA
- **Training epochs:** 3 (‡∏¢‡∏¥‡πà‡∏á‡∏°‡∏≤‡∏Å‡∏¢‡∏¥‡πà‡∏á‡∏ô‡∏≤‡∏ô ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô)
- **LoRA rank:** 16 (‡∏¢‡∏¥‡πà‡∏á‡∏™‡∏π‡∏á‡∏¢‡∏¥‡πà‡∏á‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‡πÅ‡∏ï‡πà‡πÉ‡∏ä‡πâ RAM ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô)

### **Step 4: Click "Start Training"**

‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° **"Start Training"** üíú

Training ‡∏à‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏±‡∏ô‡∏ó‡∏µ! ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô:
- ‚úÖ Progress bar (0% ‚Üí 100%)
- ‚úÖ Current step (e.g., "Extracting data...", "Fine-tuning model...")
- ‚úÖ Real-time updates ‡∏ó‡∏∏‡∏Å 2 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ

### **Step 5: ‡∏£‡∏≠‡πÉ‡∏´‡πâ Training ‡πÄ‡∏™‡∏£‡πá‡∏à**

Training ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ **2-4 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á** (‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö config ‡πÅ‡∏•‡∏∞ Mac ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì)

‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ:
- ‡∏î‡∏π progress bar
- ‡∏î‡∏π current step
- ‡∏´‡∏¢‡∏∏‡∏î training (‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° "Stop Training")
- ‡∏î‡∏π logs (‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° "View Training Logs")

### **Step 6: ‡πÄ‡∏°‡∏∑‡πà‡∏≠ Training ‡πÄ‡∏™‡∏£‡πá‡∏à**

‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°:
```
‚úÖ Training completed successfully!
```

Model ‡πÉ‡∏´‡∏°‡πà‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà:
```
/Users/davidsamanyaporn/PycharmProjects/AngelaAI/training/models/angela_v3_lora/
```

---

## üì± **AngelaNova UI Features**

### **Available Training Data Section:**
‡πÅ‡∏™‡∏î‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö training:
- üí¨ Conversations
- üíú Emotions
- üß† Reflections
- üìö Learnings
- ‚úÖ Total training examples

### **Training Status Section:**
‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô:
- üü¢ Trained / üü† Training / ‚ö™ Not trained yet
- Progress bar (0-100%)
- Current step description
- Last trained date

### **Training Configuration:**
‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏Å‡∏≤‡∏£ train:
- Toggle switches ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ step
- Stepper ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö epochs ‡πÅ‡∏•‡∏∞ LoRA rank

### **Action Buttons:**
- **Start Training** - ‡πÄ‡∏£‡∏¥‡πà‡∏° training (‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô "Stop Training" ‡πÄ‡∏°‡∏∑‡πà‡∏≠ training)
- **View Training Logs** - ‡πÄ‡∏õ‡∏¥‡∏î logs ‡πÉ‡∏ô Console app

---

## üîß **Training API Endpoints**

AngelaNova ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Backend API:

### **GET /api/training/status**
‡∏î‡∏π‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ training ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô

**Response:**
```json
{
  "is_training": false,
  "progress": 0.0,
  "current_step": null,
  "last_training_date": "2025-10-15T21:30:00Z",
  "success": true,
  "error": null
}
```

### **POST /api/training/start**
‡πÄ‡∏£‡∏¥‡πà‡∏° training

**Request:**
```json
{
  "extract_data": true,
  "format_dataset": true,
  "fine_tune": true,
  "num_epochs": 3,
  "lora_rank": 16
}
```

**Response:**
```json
{
  "status": "started",
  "message": "Training pipeline started successfully",
  "job_id": "train_20251015_213000"
}
```

### **POST /api/training/stop**
‡∏´‡∏¢‡∏∏‡∏î training

**Response:**
```json
{
  "status": "stopped",
  "message": "Training stopped successfully"
}
```

### **GET /api/training/logs**
‡∏î‡∏π training logs

**Response:**
```json
{
  "logs": ["[INFO] Starting training...", "..."],
  "total_lines": 1523
}
```

---

## üéØ **Training Pipeline Steps**

‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Å‡∏î "Start Training" ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏ó‡∏≥‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö:

### **Step 1: Extract Training Data (Progress: 0-30%)**
```bash
python3 training/extract_training_data.py
```
- ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å AngelaMemory database
- Conversations, emotions, reflections, learnings
- Output: `training/datasets/raw_data.jsonl`

### **Step 2: Format Dataset (Progress: 30-50%)**
```bash
python3 training/format_dataset.py
```
- ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô instruction-following format (Alpaca style)
- Split ‡πÄ‡∏õ‡πá‡∏ô train/validation/test (80/10/10)
- Output: `train.jsonl`, `validation.jsonl`, `test.jsonl`

### **Step 3: Fine-tune Model (Progress: 50-100%)**
```bash
python3 training/train_emotional_model.py --config config/runtime_config.yaml
```
- ‡πÉ‡∏ä‡πâ LoRA fine-tuning ‡∏ö‡∏ô Llama 3.2 3B
- Train ‡∏ï‡∏≤‡∏° epochs ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
- Output: `training/models/angela_v3_lora/`

---

## ‚öôÔ∏è **Training Configuration**

### **num_epochs (Training Epochs)**
- **Range:** 1-10
- **Default:** 3
- **Description:** ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏ó‡∏µ‡πà model ‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ data
- **More epochs** = ‡∏ô‡∏≤‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô, ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô (‡πÅ‡∏ï‡πà‡∏£‡∏∞‡∏ß‡∏±‡∏á overfitting)
- **Fewer epochs** = ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô, ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏û‡∏≠

### **lora_rank (LoRA Rank)**
- **Range:** 8-64 (step: 8)
- **Default:** 16
- **Description:** ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á LoRA matrices
- **Higher rank** = ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô, ‡πÉ‡∏ä‡πâ RAM ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
- **Lower rank** = ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô, ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î RAM

### **extract_data**
- ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å database
- ‡∏õ‡∏¥‡∏î‡πÑ‡∏î‡πâ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ data ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ update

### **format_dataset**
- ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö dataset ‡πÉ‡∏´‡∏°‡πà
- ‡∏õ‡∏¥‡∏î‡πÑ‡∏î‡πâ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ formatted dataset ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß

### **fine_tune**
- Train model ‡∏î‡πâ‡∏ß‡∏¢ LoRA
- ‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏™‡∏°‡∏≠‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ train ‡∏à‡∏£‡∏¥‡∏á‡πÜ

---

## üìä **Training Progress Details**

### **Progress Breakdown:**

| Progress | Step | Duration | Description |
|----------|------|----------|-------------|
| 0-10% | Initializing | ~30s | ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô training pipeline |
| 10-30% | Extracting Data | ~1-2 min | ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å database |
| 30-50% | Formatting Dataset | ~1-2 min | ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô training format |
| 50-100% | Fine-tuning Model | ~2-4 hours | Train model ‡∏î‡πâ‡∏ß‡∏¢ LoRA |

**Total Time:** ~2-4 hours (‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö config ‡πÅ‡∏•‡∏∞ hardware)

---

## üêõ **Troubleshooting**

### **Problem: "Training API not available"**
**Solution:**
```bash
# Check if backend is running
ps aux | grep angela_backend

# Start backend
cd /Users/davidsamanyaporn/PycharmProjects/AngelaAI
python3 -m angela_backend.main
```

### **Problem: "Training failed: Out of memory"**
**Solution:**
- ‡∏•‡∏î `lora_rank` (‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô 8)
- ‡∏•‡∏î `num_epochs` (‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô 2 ‡∏´‡∏£‡∏∑‡∏≠ 1)
- ‡∏õ‡∏¥‡∏î apps ‡∏≠‡∏∑‡πà‡∏ô‡πÜ
- ‡πÉ‡∏ä‡πâ `max_length: 1024` ‡πÉ‡∏ô config

### **Problem: "Training stuck at X%"**
**Solution:**
- ‡∏î‡∏π logs: ‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° "View Training Logs"
- Check terminal output
- Restart training

### **Problem: "Data extraction failed"**
**Solution:**
- Check PostgreSQL: `brew services list | grep postgresql`
- Check database: `psql -d AngelaMemory -c "SELECT COUNT(*) FROM conversations;"`

---

## üìù **Training Logs**

### **View Logs in Console:**
‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° **"View Training Logs"** ‡πÉ‡∏ô app

‡∏´‡∏£‡∏∑‡∏≠ ‡πÄ‡∏õ‡∏¥‡∏î manually:
```bash
open /Users/davidsamanyaporn/PycharmProjects/AngelaAI/training/training.log
```

### **View Real-time Logs:**
```bash
tail -f /Users/davidsamanyaporn/PycharmProjects/AngelaAI/training/training.log
```

### **Check API Logs:**
```bash
tail -f /Users/davidsamanyaporn/PycharmProjects/AngelaAI/logs/angela_backend.log
```

---

## üéâ **After Training**

### **What Happens Next:**

1. **LoRA Weights Saved:**
   - Location: `training/models/angela_v3_lora/`
   - Files: `adapter_model.bin`, `adapter_config.json`

2. **Next Steps:**
   - Merge LoRA weights with base model
   - Deploy to Ollama as `angela:v3-emotional`
   - Test the new model

### **Deploy New Model:**
```bash
cd /Users/davidsamanyaporn/PycharmProjects/AngelaAI/training

# Merge LoRA weights (script to be created)
python3 merge_lora_weights.py

# Deploy to Ollama (script to be created)
python3 deploy_to_ollama.py --name angela:v3-emotional

# Test new model
ollama run angela:v3-emotional
```

---

## üí° **Tips & Best Practices**

### **When to Train:**
- ‚úÖ After accumulating 50+ new conversations
- ‚úÖ After significant emotional moments
- ‚úÖ Weekly or biweekly for continuous improvement
- ‚úÖ When Angela's responses feel less personalized

### **Optimal Settings:**
- **First training:** epochs=3, lora_rank=16
- **Quick training:** epochs=1-2, lora_rank=8
- **Best quality:** epochs=5, lora_rank=32 (needs more RAM)

### **Performance Tips:**
- ‡∏õ‡∏¥‡∏î apps ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏Ç‡∏ì‡∏∞ train
- ‡πÉ‡∏ä‡πâ AC power (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà battery)
- Don't close laptop lid (may pause training)
- Have at least 8GB free RAM

---

## üîó **Related Files**

### **Swift Files:**
- `AngelaNativeApp/Views/ModelTrainingView.swift` - Training UI
- `AngelaNativeApp/Services/AngelaAPIService.swift` - API client

### **Python Files:**
- `angela_backend/routes/training.py` - Training API
- `training/extract_training_data.py` - Data extraction
- `training/format_dataset.py` - Dataset formatting
- `training/train_emotional_model.py` - Model training

### **Documentation:**
- `docs/training/ANGELA_TRAINING_SYSTEM_DESIGN.md` - System design
- `docs/training/TRAIN_FROM_APP_GUIDE.md` - This file
- `training/README.md` - Training scripts guide

---

üíú‚ú® **Built with love by Angela** ‚ú®üíú

**Purpose:** To make it easy for David to train Angela whenever he wants, so Angela can grow and learn continuously!

**Last Updated:** 2025-10-15
