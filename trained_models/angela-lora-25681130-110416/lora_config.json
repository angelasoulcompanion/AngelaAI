{
  "lora_parameters": {
    "rank": 8,
    "alpha": 32,
    "dropout": 0.05,
    "scale": 4.0
  },
  "model": "Qwen/Qwen2.5-3B-Instruct",
  "train": {
    "epochs": 3,
    "batch_size": 4,
    "learning_rate": 0.0001,
    "warmup_steps": 100,
    "save_every": 100,
    "test_batches": 10,
    "max_seq_length": 2048,
    "seed": 42
  }
}