#!/usr/bin/env python3
"""
‡∏™‡∏£‡πâ‡∏≤‡∏á Angela Qwen Fine-tuning Notebook ‡πÉ‡∏´‡∏°‡πà
‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á: Steps 1-11, 11.5, 11.6, 12, 13
"""

import json

# ‡∏≠‡πà‡∏≤‡∏ô backup notebook
with open('Angela_Qwen_FineTuning_Colab_backup.ipynb', 'r', encoding='utf-8') as f:
    backup = json.load(f)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á notebook structure ‡πÉ‡∏´‡∏°‡πà
notebook = {
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": backup.get('metadata', {}),
    "cells": []
}

# ‡πÄ‡∏û‡∏¥‡πà‡∏° Steps 1-11 ‡∏à‡∏≤‡∏Å backup (cells 0-21)
notebook['cells'].extend(backup['cells'][:22])

# ‡πÄ‡∏û‡∏¥‡πà‡∏° Step 11.5 - Merge LoRA
notebook['cells'].append({
    "cell_type": "markdown",
    "metadata": {},
    "source": [
        "## Step 11.5: Merge LoRA Adapter with Base Model\n",
        "\n",
        "**Important:** Ollama requires a full merged model, not just the LoRA adapter.\n",
        "\n",
        "We'll merge the adapter with the base model to create a complete fine-tuned model."
    ]
})

notebook['cells'].append({
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        'print("üîÑ Merging LoRA adapter with base model...")\n',
        'print("   This may take 5-10 minutes...")\n',
        'print("\\n" + "="*70)\n',
        '\n',
        '# Clear GPU memory first\n',
        'import gc\n',
        'gc.collect()\n',
        'torch.cuda.empty_cache()\n',
        '\n',
        '# Load base model in FP16 (not quantized) for merging\n',
        'print("üì• Loading base model for merging...")\n',
        'from transformers import AutoModelForCausalLM\n',
        '\n',
        'base_model = AutoModelForCausalLM.from_pretrained(\n',
        '    model_name,\n',
        '    torch_dtype=torch.float16,\n',
        '    device_map="auto",\n',
        '    trust_remote_code=True,\n',
        ')\n',
        '\n',
        'print("‚úÖ Base model loaded")\n',
        '\n',
        '# Load and merge LoRA adapter\n',
        'print("üîó Loading LoRA adapter...")\n',
        'from peft import PeftModel\n',
        '\n',
        'merged_model = PeftModel.from_pretrained(\n',
        '    base_model,\n',
        '    output_dir,\n',
        '    torch_dtype=torch.float16,\n',
        ')\n',
        '\n',
        'print("‚úÖ LoRA adapter loaded")\n',
        '\n',
        '# Merge adapter weights into base model\n',
        'print("‚öôÔ∏è Merging weights...")\n',
        'merged_model = merged_model.merge_and_unload()\n',
        '\n',
        'print("‚úÖ Merge complete!")\n',
        '\n',
        '# Save merged model\n',
        'merged_output_dir = "./angela_qwen_merged"\n',
        'print(f"üíæ Saving merged model to {merged_output_dir}...")\n',
        '\n',
        'merged_model.save_pretrained(\n',
        '    merged_output_dir,\n',
        '    safe_serialization=True,\n',
        ')\n',
        'tokenizer.save_pretrained(merged_output_dir)\n',
        '\n',
        'print("="*70)\n',
        'print("üéâ Merged model saved successfully!")\n',
        'print(f"\\nMerged model location: {merged_output_dir}")\n',
        '\n',
        '# Clean up to save memory\n',
        'del base_model\n',
        'del merged_model\n',
        'gc.collect()\n',
        'torch.cuda.empty_cache()'
    ]
})

print(f"‚úÖ Added Step 11.5 (Merge)")

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå
with open('Angela_Qwen_FineTuning_Colab.ipynb', 'w', encoding='utf-8') as f:
    json.dump(notebook, f, indent=2, ensure_ascii=False)

print(f"‚úÖ Saved notebook with {len(notebook['cells'])} cells")
print("   Next: Run this script, then manually add Step 11.6, 12, 13")
