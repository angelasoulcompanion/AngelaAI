"""
Document Management API Endpoints
Handles document upload, processing, and RAG operations
"""

from fastapi import APIRouter, File, UploadFile, Form, Query, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from typing import List, Optional
from uuid import UUID
import logging
import asyncio
from pathlib import Path
import json

# Import services
from angela_core.services.document_processor import DocumentProcessor
from angela_core.services.rag_retrieval_service import RAGRetrievalService
from angela_core.database import db

logger = logging.getLogger(__name__)

router = APIRouter()

# ===============================================
# Pydantic Models
# ===============================================


class DocumentUploadRequest(BaseModel):
    """Request model for document upload"""
    title: Optional[str] = None
    category: str = "general"
    tags: Optional[List[str]] = None


class DocumentInfo(BaseModel):
    """Document information"""
    document_id: str
    title: str
    category: str
    language: str
    thai_word_count: int
    total_sentences: int
    total_chunks: int
    created_at: str


class SearchRequest(BaseModel):
    """Search request model"""
    query: str
    top_k: int = Field(default=10, ge=1, le=50)
    search_mode: str = Field(default="hybrid", pattern="^(vector|keyword|hybrid)$")
    threshold: float = Field(default=0.5, ge=0, le=1)


class SearchResult(BaseModel):
    """Search result"""
    content: str
    document_id: str
    similarity_score: float
    chunk_id: str


class RAGContextResponse(BaseModel):
    """RAG context response"""
    context: str
    results: List[SearchResult]
    source_count: int


# ===============================================
# Endpoint Helpers
# ===============================================

async def get_db_connection():
    """Get database connection pool"""
    try:
        return db
        except Exception as e:
            logger.error(f"‚ùå Database connection error: {e}")
            raise HTTPException(status_code=500, detail="Database connection error")


# ===============================================
# Document Management Endpoints
# ===============================================

@router.post("/api/documents/upload")
async def upload_document(
    file: UploadFile = File(...),
    title: Optional[str] = Form(None),
    category: str = Form("general"),
    tags: Optional[str] = Form(None)
):
    """Upload and process a document"""

    try:
        logger.info(f"üì§ Uploading document: {file.filename}")

        # Parse tags
        tag_list = []
        if tags:
            tag_list = [t.strip() for t in tags.split(',')]

        # Save file temporarily
        file_path = f"/tmp/{file.filename}"
        content = await file.read()

        with open(file_path, 'wb') as f:
            f.write(content)

        # Get database connection
        db = await get_db_connection()

        try:
            # Process document
            processor = DocumentProcessor(db)
            result = await processor.process_document(
                file_path=file_path,
                title=title or file.filename,
                category=category,
                tags=tag_list
            )

            if result['success']:
                return {
                    'success': True,
                    'message': 'Document processed successfully',
                    'data': result
                }
            else:
                raise HTTPException(status_code=400, detail=result['error'])

        except Exception as e:
            logger.error(f"‚ùå Upload error: {e}")
            raise HTTPException(status_code=500, detail=str(e))


@router.post("/api/documents/batch-upload")
async def batch_upload_documents(
    files: List[UploadFile] = File(...),
    category: str = Form("general"),
    tags: Optional[str] = Form(None)
):
    """Upload and process multiple documents"""

    try:
        logger.info(f"üì§ Batch uploading {len(files)} documents")
        logger.info(f"üìÅ Files: {[f.filename for f in files]}")
        logger.info(f"üìÇ Category: {category}")
        logger.info(f"üè∑Ô∏è  Tags: {tags}")

        # Parse tags
        tag_list = []
        if tags:
            tag_list = [t.strip() for t in tags.split(',')]

        # Save files temporarily
        file_paths = []
        for file in files:
            file_path = f"/tmp/{file.filename}"
            logger.info(f"üíæ Saving {file.filename} to {file_path}")
            content = await file.read()
            logger.info(f"üìè File size: {len(content)} bytes")
            with open(file_path, 'wb') as f:
                f.write(content)
            file_paths.append((file_path, file.filename))

        # Get database connection
        logger.info("üîå Connecting to database...")
        db = await get_db_connection()

        try:
            # Batch process documents
            logger.info("‚öôÔ∏è  Creating DocumentProcessor...")
            processor = DocumentProcessor(db)
            results = []

            for file_path, filename in file_paths:
                logger.info(f"üîÑ Processing {filename}...")
                result = await processor.process_document(
                    file_path=file_path,
                    title=filename,
                    category=category,
                    tags=tag_list
                )
                logger.info(f"üìä Result for {filename}: {result}")
                results.append(result)

            successful = [r for r in results if r.get('success')]
            failed = [r for r in results if not r.get('success')]

            logger.info(f"‚úÖ Batch upload complete: {len(successful)} successful, {len(failed)} failed")

            response = {
                'success': True,
                'total': len(files),
                'successful': len(successful),
                'failed': len(failed),
                'results': results
            }
            logger.info(f"üì§ Returning response: {response}")
            return response

        except Exception as e:
            logger.error(f"‚ùå Batch upload error: {e}")
            import traceback
            logger.error(f"üîç Traceback: {traceback.format_exc()}")
            raise HTTPException(status_code=500, detail=str(e))


@router.post("/api/documents/batch-upload-stream")
async def batch_upload_documents_stream(
    files: List[UploadFile] = File(...),
    category: str = Form("general"),
    tags: Optional[str] = Form(None)
):
    """Upload and process multiple documents with real-time progress updates (SSE)"""

    # Create a queue for progress events
    progress_queue = asyncio.Queue()

    async def process_documents():
        """Background task to process documents and send progress"""
        try:
            logger.info(f"üì§ Streaming upload: {len(files)} documents")

            # Parse tags
            tag_list = []
            if tags:
                tag_list = [t.strip() for t in tags.split(',')]

            # Save files temporarily
            file_paths = []
            for i, file in enumerate(files):
                file_path = f"/tmp/{file.filename}"
                content = await file.read()
                with open(file_path, 'wb') as f:
                    f.write(content)
                file_paths.append((file_path, file.filename))

                # Send progress for file saving
                progress = int((i + 1) / len(files) * 5)  # 0-5%
                await progress_queue.put({
                    'type': 'progress',
                    'step': 'saving',
                    'percent': progress,
                    'message': f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå ({i + 1}/{len(files)})...",
                    'current_file': file.filename
                })

            # Get database connection
            db = await get_db_connection()

            try:
                processor = DocumentProcessor(db)
                results = []

                for idx, (file_path, filename) in enumerate(file_paths):
                    logger.info(f"üîÑ Processing {filename}...")

                    # Create progress callback for this file
                    async def progress_callback(step, percent, message):
                        # Adjust percent based on file index
                        base_percent = 5 + (idx / len(file_paths) * 90)  # 5-95%
                        adjusted_percent = int(base_percent + (percent / 100 * (90 / len(file_paths))))

                        await progress_queue.put({
                            'type': 'progress',
                            'step': step,
                            'percent': adjusted_percent,
                            'message': message,
                            'current_file': filename,
                            'file_index': idx + 1,
                            'total_files': len(file_paths)
                        })

                    # Process document with progress tracking
                    result = await processor.process_document(
                        file_path=file_path,
                        title=filename,
                        category=category,
                        tags=tag_list,
                        progress_callback=progress_callback
                    )

                    results.append(result)

                successful = [r for r in results if r.get('success')]
                failed = [r for r in results if not r.get('success')]

                # Send completion event
                await progress_queue.put({
                    'type': 'complete',
                    'percent': 100,
                    'message': '‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!',
                    'results': {
                        'success': True,
                        'total': len(files),
                        'successful': len(successful),
                        'failed': len(failed),
                        'results': results
                    }
                })

            # Signal completion
            await progress_queue.put(None)

        except Exception as e:
            logger.error(f"‚ùå Streaming upload error: {e}")
            await progress_queue.put({
                'type': 'error',
                'percent': 0,
                'message': f'‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}',
                'error': str(e)
            })
            await progress_queue.put(None)

    async def event_generator():
        """Generate Server-Sent Events from progress queue"""
        # Start processing in background
        task = asyncio.create_task(process_documents())

        try:
            while True:
                # Wait for next event
                event = await progress_queue.get()

                # None signals completion
                if event is None:
                    break

                # Send event to client
                yield f"data: {json.dumps(event)}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # Disable nginx buffering
        }
    )


@router.get("/api/documents")
async def list_documents(
    category: Optional[str] = None,
    skip: int = Query(0, ge=0),
    limit: int = Query(20, ge=1, le=100)
):
    """List all documents with pagination"""

    try:
        db = await get_db_connection()

        try:
            if category:
                query = """
                    SELECT document_id, title, category, language,
                           thai_word_count, total_sentences, total_chunks,
                           created_at, access_count
                    FROM document_library
                    WHERE category = $1 AND is_active = true
                    ORDER BY created_at DESC
                    OFFSET $2 LIMIT $3
                """
                docs = await db.fetch(query, category, skip, limit)
            else:
                query = """
                    SELECT document_id, title, category, language,
                           thai_word_count, total_sentences, total_chunks,
                           created_at, access_count
                    FROM document_library
                    WHERE is_active = true
                    ORDER BY created_at DESC
                    OFFSET $1 LIMIT $2
                """
                docs = await db.fetch(query, skip, limit)

            return {
                'success': True,
                'total': len(docs),
                'documents': [dict(d) for d in docs]
            }

        except Exception as e:
            logger.error(f"‚ùå List documents error: {e}")
            raise HTTPException(status_code=500, detail=str(e))


@router.get("/api/documents/{document_id}")
async def get_document(document_id: UUID):
    """Get document details"""

    try:
        db = await get_db_connection()

        try:
            query = """
                SELECT document_id, title, category, language,
                       thai_word_count, total_sentences, total_chunks,
                       keywords_thai, summary_thai, created_at,
                       access_count
                FROM document_library
                WHERE document_id = $1
            """

            doc = await db.fetchrow(query, document_id)

            if not doc:
                raise HTTPException(status_code=404, detail="Document not found")

            # Increment access count
            await db.execute(
                "UPDATE document_library SET access_count = access_count + 1 WHERE document_id = $1",
                document_id
            )

            return {
                'success': True,
                'document': dict(doc)
            }

        except Exception as e:
            logger.error(f"‚ùå Get document error: {e}")
            raise HTTPException(status_code=500, detail=str(e))


@router.get("/api/documents/{document_id}/chunks")
async def get_document_chunks(
    document_id: UUID,
    skip: int = Query(0, ge=0),
    limit: int = Query(20, ge=1, le=100)
):
    """Get chunks for a document"""

    try:
        db = await get_db_connection()

        try:
            query = """
                SELECT chunk_id, chunk_index, content,
                       thai_word_count, importance_score,
                       page_number, section_title
                FROM document_chunks
                WHERE document_id = $1
                ORDER BY chunk_index ASC
                OFFSET $2 LIMIT $3
            """

            chunks = await db.fetch(query, document_id, skip, limit)

            return {
                'success': True,
                'total': len(chunks),
                'chunks': [dict(c) for c in chunks]
            }

        except Exception as e:
            logger.error(f"‚ùå Get chunks error: {e}")
            raise HTTPException(status_code=500, detail=str(e))


@router.delete("/api/documents/{document_id}")
async def delete_document(document_id: UUID):
    """Delete a document"""

    try:
        db = await get_db_connection()

        try:
            processor = DocumentProcessor(db)
            result = await processor.delete_document(document_id)

            if result['success']:
                return {'success': True, 'message': 'Document deleted'}
            else:
                raise HTTPException(status_code=400, detail=result['error'])

        except Exception as e:
            logger.error(f"‚ùå Delete document error: {e}")
            raise HTTPException(status_code=500, detail=str(e))


# ===============================================
# Search and RAG Endpoints
# ===============================================

@router.post("/api/documents/search")
async def search_documents(request: SearchRequest):
    """Search documents with RAG"""

    try:
        db = await get_db_connection()

        try:
            rag_service = RAGRetrievalService(db)
            context = await rag_service.get_rag_context(
                query=request.query,
                top_k=request.top_k,
                search_mode=request.search_mode
            )

            if 'error' in context:
                raise HTTPException(status_code=400, detail=context['error'])

            return {
                'success': True,
                'context': context['context'],
                'results': context['results'],
                'source_count': context['source_count'],
                'sources': context['sources']
            }

        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"‚ùå Search error: {e}")
            raise HTTPException(status_code=500, detail=str(e))


@router.post("/api/documents/search-feedback")
async def record_search_feedback(
    search_id: UUID,
    was_helpful: bool,
    rating: Optional[int] = None,
    feedback_text: Optional[str] = None
):
    """Record feedback on search results"""

    try:
        db = await get_db_connection()

        try:
            rag_service = RAGRetrievalService(db)
            result = await rag_service.record_search_feedback(
                search_id=search_id,
                was_helpful=was_helpful,
                rating=rating,
                feedback_text=feedback_text
            )

            return result

        except Exception as e:
            logger.error(f"‚ùå Feedback error: {e}")
            raise HTTPException(status_code=500, detail=str(e))


@router.get("/api/documents/analytics")
async def get_rag_analytics(days: int = Query(7, ge=1, le=90)):
    """Get RAG analytics"""

    try:
        db = await get_db_connection()

        try:
            rag_service = RAGRetrievalService(db)
            analytics = await rag_service.get_analytics(days=days)

            return {
                'success': True,
                'period_days': days,
                'analytics': analytics
            }

        except Exception as e:
            logger.error(f"‚ùå Analytics error: {e}")
            raise HTTPException(status_code=500, detail=str(e))


@router.get("/api/documents/stats")
async def get_all_documents_stats():
    """Get overall documents statistics"""

    try:
        db = await get_db_connection()

        try:
            query = """
                SELECT
                    COUNT(*) as total_documents,
                    SUM(total_chunks) as total_chunks,
                    SUM(thai_word_count) as total_words,
                    COUNT(DISTINCT category) as categories,
                    COUNT(DISTINCT language) as languages,
                    AVG(access_count) as avg_access_count
                FROM document_library
                WHERE is_active = true
            """

            stats = await db.fetchrow(query)

            return {
                'success': True,
                'stats': dict(stats) if stats else {}
            }

        except Exception as e:
            logger.error(f"‚ùå Stats error: {e}")
            raise HTTPException(status_code=500, detail=str(e))
