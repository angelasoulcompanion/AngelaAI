"""
Thai Text Processing Service for RAG System
Optimized for Thai language with proper tokenization, normalization, and analysis
"""

import re
from typing import List, Dict, Tuple
from collections import Counter
import asyncio

from pythainlp import word_tokenize, sent_tokenize, pos_tag
from pythainlp.util import normalize as thai_normalize
from pythainlp.corpus import thai_stopwords
try:
    from pythainlp.tag.named_entity import NER
    # Initialize NER with corpus-based engine (no deep learning required)
    ner_tagger = NER(engine='thainer')
    ner_available = True
except Exception as e:
    # Fallback if NER is not available or fails to initialize
    ner_tagger = None
    ner_available = False
import logging

logger = logging.getLogger(__name__)


class ThaiTextProcessor:
    """‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏ö‡∏ö‡∏•‡∏∂‡∏Å"""

    def __init__(self):
        """Initialize Thai text processor with all necessary resources"""
        self.stopwords = thai_stopwords()

        # Custom Thai stopwords to filter
        self.custom_stopwords = {
            '‡∏Ñ‡πà‡∏∞', '‡∏Ñ‡∏£‡∏±‡∏ö', '‡∏Ñ‡∏∞', '‡∏ô‡∏∞‡∏Ñ‡∏∞', '‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö',
            '‡∏à‡πâ‡∏≤', '‡∏à‡πä‡∏∞', '‡∏à‡πâ‡∏∞', '‡∏Ç‡∏≤', '‡πÄ‡∏ñ‡∏≠‡∏∞',
            '‡∏ó‡∏µ‡πà', '‡πÅ‡∏•‡∏∞', '‡∏Å‡∏±‡∏ö', '‡πÄ‡∏õ‡πá‡∏ô', '‡πÑ‡∏î‡πâ',
            '‡∏°‡∏≤', '‡πÑ‡∏õ', '‡∏≠‡∏¢‡∏π‡πà', '‡πÉ‡∏´‡πâ', '‡∏ß‡πà‡∏≤'
        }

        # Common Thai abbreviations mapping
        self.abbreviations = {
            '‡∏°.‡∏Ñ.': '‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°',
            '‡∏Å.‡∏û.': '‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå',
            '‡∏°‡∏µ.‡∏Ñ.': '‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°',
            '‡πÄ‡∏°.‡∏¢.': '‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô',
            '‡∏û.‡∏Ñ.': '‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°',
            '‡∏°‡∏¥.‡∏¢.': '‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô',
            '‡∏Å.‡∏Ñ.': '‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°',
            '‡∏™.‡∏Ñ.': '‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°',
            '‡∏Å.‡∏¢.': '‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô',
            '‡∏ï.‡∏Ñ.': '‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°',
            '‡∏û.‡∏¢.': '‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô',
            '‡∏ò.‡∏Ñ.': '‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°',
            '‡∏û‡∏ö.': '‡∏û‡∏ö',
            '‡πÄ‡∏Å.‡∏•.': '‡πÄ‡∏Å‡∏≤‡∏∞‡∏•‡∏±‡∏ô‡∏ï‡∏≤',
            '‡∏ï.': '‡∏ï‡∏≥‡∏ö‡∏•',
            '‡∏≠.': '‡∏≠‡∏≥‡πÄ‡∏†‡∏≠',
            '‡∏à.': '‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î',
            '‡∏™‡∏û.': '‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏û‡∏¥‡∏°‡∏û‡πå',
        }

        logger.info("‚úÖ ThaiTextProcessor initialized")

    async def preprocess_thai_text(self, text: str) -> str:
        """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""

        # 1. Normalize Thai text (‡πÅ‡∏Å‡πâ‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Ç‡∏≠‡∏á‡∏™‡∏£‡∏∞ ‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå)
        text = thai_normalize(text)

        # 2. Expand abbreviations
        text = await self._expand_abbreviations(text)

        # 3. Handle mixed language
        text = await self._handle_mixed_language(text)

        # 4. Remove extra whitespace
        text = ' '.join(text.split())

        # 5. Fix common typos
        text = await self._correct_common_typos(text)

        return text

    async def _expand_abbreviations(self, text: str) -> str:
        """‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏¢‡πà‡∏≠ ‡πÄ‡∏ä‡πà‡∏ô ‡∏°.‡∏Ñ. ‚Üí ‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°"""
        for abbr, full in self.abbreviations.items():
            text = text.replace(abbr, full)
        return text

    async def _handle_mixed_language(self, text: str) -> str:
        """‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ú‡∏™‡∏°‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©"""
        # Keep both Thai and English text, just normalize spacing
        return text

    async def _correct_common_typos(self, text: str) -> str:
        """‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡∏™‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏î‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢"""
        common_typos = {
            '‡∏ó‡∏≥‡πÉ‡∏´‡πâ': '‡∏ó‡∏≥‡πÉ‡∏´‡πâ',  # Common variant
            '‡∏¢‡∏±‡∏á‡πÑ‡∏á': '‡∏¢‡∏±‡∏á‡πÑ‡∏á',  # Colloquial
            '‡πÄ‡∏Ñ‡πâ‡∏≤': '‡πÄ‡∏Ç‡∏≤',     # They
            '‡∏°‡∏±‡πâ‡∏¢': '‡πÑ‡∏´‡∏°',     # Yes/no question particle
        }

        for typo, correct in common_typos.items():
            if typo in text:
                text = text.replace(typo, correct)

        return text

    def detect_language(self, text: str) -> str:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡∏†‡∏≤‡∏©‡∏≤‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (th, en, mixed)"""
        thai_pattern = re.compile(r'[\u0E00-\u0E7F]')  # Thai Unicode range
        english_pattern = re.compile(r'[a-zA-Z]')

        has_thai = bool(thai_pattern.search(text))
        has_english = bool(english_pattern.search(text))

        if has_thai and has_english:
            return 'mixed'
        elif has_thai:
            return 'th'
        elif has_english:
            return 'en'
        else:
            return 'other'

    def tokenize_thai(self, text: str, engine: str = "newmm") -> List[str]:
        """‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""
        try:
            tokens = word_tokenize(text, engine=engine)
            return tokens
        except Exception as e:
            logger.error(f"‚ùå Tokenization error: {e}")
            return text.split()

    def get_sentences(self, text: str) -> List[str]:
        """‡∏ï‡∏±‡∏î‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ"""
        try:
            sentences = sent_tokenize(text, engine="crfcut")
            return sentences
        except Exception as e:
            logger.error(f"‚ùå Sentence tokenization error: {e}")
            return text.split('\n')

    async def smart_chunk_thai(
        self,
        text: str,
        chunk_size: int = 500,  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥ (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà characters)
        overlap: int = 100
    ) -> List[Dict]:
        """‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ó‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏≤‡∏ç‡∏â‡∏•‡∏≤‡∏î"""

        try:
            # 1. Split into sentences
            sentences = self.get_sentences(text)

            chunks = []
            current_chunk = []
            current_word_count = 0

            for sentence in sentences:
                # Tokenize sentence
                words = self.tokenize_thai(sentence)
                sentence_word_count = len(words)

                # If adding this sentence exceeds chunk_size, save current chunk
                if current_word_count + sentence_word_count > chunk_size:
                    if current_chunk:
                        chunk_text = ' '.join(current_chunk)
                        chunks.append({
                            'content': chunk_text,
                            'word_count': current_word_count,
                            'sentence_count': len(current_chunk),
                            'chunk_index': len(chunks),
                            'metadata': {
                                'language': 'th',
                                'has_english': self.detect_language(chunk_text) == 'mixed'
                            }
                        })

                    # Start new chunk with overlap (last 2 sentences or 1)
                    if len(current_chunk) > 2:
                        overlap_sentences = current_chunk[-2:]
                    elif len(current_chunk) > 0:
                        overlap_sentences = current_chunk[-1:]
                    else:
                        overlap_sentences = []

                    current_chunk = overlap_sentences + [sentence]
                    current_word_count = sum(
                        len(self.tokenize_thai(s))
                        for s in current_chunk
                    )
                else:
                    current_chunk.append(sentence)
                    current_word_count += sentence_word_count

            # Add final chunk
            if current_chunk:
                chunk_text = ' '.join(current_chunk)
                chunks.append({
                    'content': chunk_text,
                    'word_count': current_word_count,
                    'sentence_count': len(current_chunk),
                    'chunk_index': len(chunks),
                    'metadata': {
                        'language': self.detect_language(chunk_text)
                    }
                })

            logger.info(f"‚úÖ Created {len(chunks)} chunks from Thai text")
            return chunks

        except Exception as e:
            logger.error(f"‚ùå Error in chunking: {e}")
            # Fallback: simple character-based chunking
            return [{
                'content': text[i:i+chunk_size],
                'word_count': len(text[i:i+chunk_size].split()),
                'chunk_index': idx
            } for idx, i in enumerate(range(0, len(text), chunk_size))]

    async def extract_keywords_thai(self, text: str, top_k: int = 20) -> List[str]:
        """‡∏™‡∏Å‡∏±‡∏î keywords ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""

        try:
            # 1. Tokenize
            words = self.tokenize_thai(text)

            # 2. POS tagging
            pos_tagged = pos_tag(words, engine="perceptron")

            # 3. Filter meaningful words with priority scoring
            # PROPN (‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞) = highest priority for exact matching
            # NOUN = high priority
            # VERB, ADJ = medium priority
            weighted_words = []
            for word, pos in pos_tagged:
                if word in self.stopwords or word in self.custom_stopwords or len(word) <= 1:
                    continue

                if pos == 'PROPN':  # ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞ (‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó, ‡∏Ñ‡∏ô, ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà)
                    weighted_words.extend([word] * 3)  # 3x weight
                elif pos == 'NOUN':  # ‡∏Ñ‡∏≥‡∏ô‡∏≤‡∏°
                    weighted_words.extend([word] * 2)  # 2x weight
                elif pos in ['VERB', 'ADJ']:
                    weighted_words.append(word)  # 1x weight

            # 4. Count frequency (with weights)
            word_freq = Counter(weighted_words)
            keywords = [word for word, _ in word_freq.most_common(top_k)]

            logger.info(f"‚úÖ Extracted {len(keywords)} keywords (PROPN boosted)")
            return keywords

        except Exception as e:
            logger.error(f"‚ùå Error extracting keywords: {e}")
            return []

    async def extract_entities_thai(self, text: str) -> Dict[str, List[str]]:
        """‡∏™‡∏Å‡∏±‡∏î Named Entities ‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ (‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•, ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà, ‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£)"""

        result = {
            'PERSON': [],
            'LOCATION': [],
            'ORGANIZATION': [],
            'OTHER': []
        }

        try:
            # Check if NER is available
            if not ner_available or ner_tagger is None:
                logger.debug("‚ö†Ô∏è pythainlp NER not available, returning empty entities")
                return result

            # Use pythainlp NER with tag=True to get (token, label) tuples
            entities = ner_tagger.tag(text, tag=True)

            for word, tag in entities:
                if tag == 'PERSON':
                    result['PERSON'].append(word)
                elif tag == 'LOCATION':
                    result['LOCATION'].append(word)
                elif tag == 'ORGANIZATION':
                    result['ORGANIZATION'].append(word)
                elif tag != 'O':  # O = not an entity
                    result['OTHER'].append(word)

            logger.info(f"‚úÖ Extracted entities: {len(result['PERSON'])} persons, "
                       f"{len(result['LOCATION'])} locations, "
                       f"{len(result['ORGANIZATION'])} organizations")

            return result

        except Exception as e:
            logger.error(f"‚ùå Error extracting entities: {e}")
            return result  # Return empty result instead of empty dict

    def calculate_thai_readability(self, text: str) -> Dict:
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Readability Score ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"""

        try:
            sentences = self.get_sentences(text)
            words = self.tokenize_thai(text)

            total_sentences = len(sentences)
            total_words = len(words)
            total_chars = len(text)

            # Avoid division by zero
            if total_sentences == 0 or total_words == 0:
                return {
                    'avg_word_length': 0,
                    'avg_words_per_sentence': 0,
                    'difficulty_level': 'unknown'
                }

            avg_word_length = total_chars / total_words
            avg_words_per_sentence = total_words / total_sentences

            # Simple difficulty classification for Thai
            if avg_word_length > 4 and avg_words_per_sentence > 15:
                difficulty = 'hard'
            elif avg_word_length > 3 and avg_words_per_sentence > 10:
                difficulty = 'medium'
            else:
                difficulty = 'easy'

            return {
                'avg_word_length': round(avg_word_length, 2),
                'avg_words_per_sentence': round(avg_words_per_sentence, 2),
                'total_words': total_words,
                'total_sentences': total_sentences,
                'total_characters': total_chars,
                'difficulty_level': difficulty
            }

        except Exception as e:
            logger.error(f"‚ùå Error calculating readability: {e}")
            return {}

    def contains_english(self, text: str) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏µ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©"""
        return self.detect_language(text) in ['en', 'mixed']

    async def normalize_for_embedding(self, text: str) -> str:
        """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö embedding"""

        # 1. Preprocess
        text = await self.preprocess_thai_text(text)

        # 2. Convert to lowercase (for English part)
        text = text.lower()

        # 3. Remove special characters but keep Thai characters
        # Keep Thai chars (U+0E00-U+0E7F), alphanumeric, and spaces
        text = re.sub(r'[^\u0E00-\u0E7Fa-z0-9\s]', '', text)

        # 4. Normalize whitespace
        text = ' '.join(text.split())

        return text

    async def batch_process_texts(
        self,
        texts: List[str],
        operation: str = 'tokenize'
    ) -> List:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ö‡∏ö batch"""

        tasks = []

        if operation == 'tokenize':
            return [self.tokenize_thai(t) for t in texts]
        elif operation == 'chunk':
            tasks = [self.smart_chunk_thai(t) for t in texts]
        elif operation == 'keywords':
            tasks = [self.extract_keywords_thai(t) for t in texts]
        elif operation == 'entities':
            tasks = [self.extract_entities_thai(t) for t in texts]

        if tasks:
            return await asyncio.gather(*tasks)

        return []

    async def analyze_document(self, text: str) -> Dict:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""

        logger.info("üîç Starting comprehensive document analysis...")

        try:
            # Preprocess
            processed = await self.preprocess_thai_text(text)

            # Language detection
            language = self.detect_language(text)

            # Extract features
            keywords = await self.extract_keywords_thai(text)
            entities = await self.extract_entities_thai(text)
            readability = self.calculate_thai_readability(text)

            result = {
                'language': language,
                'keywords': keywords,
                'entities': entities,
                'readability': readability,
                'is_processed': True
            }

            logger.info(f"‚úÖ Analysis complete: {language} language, "
                       f"{len(keywords)} keywords, {len(entities)} entities")

            return result

        except Exception as e:
            logger.error(f"‚ùå Document analysis error: {e}")
            return {'error': str(e), 'is_processed': False}
