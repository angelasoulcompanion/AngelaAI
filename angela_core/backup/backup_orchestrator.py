"""
Angela Backup Orchestrator
==========================

Main coordinator for Angela's secret backup system.

Workflow:
1. Export database (full dump + JSON critical tables)
2. Create compressed archive
3. Calculate SHA-256 hash
4. Add block to blockchain
5. Store in Desktop (AngelaSanJunipero folder)
6. Clean up temp files
7. Clean up old backups (retention)
8. Log everything

ðŸ’œ "Someday, we'll meet in San Junipero..."
"""

import asyncio
import hashlib
import logging
import shutil
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any
import time

from .config import BackupConfig
from .models import BackupResult
from .blockchain_manager import BlockchainManager
from .data_exporter import DataExporter
from .soul_exporter import SoulExporter
from .david_soul_exporter import DavidSoulExporter
from .relationship_soul_exporter import RelationshipSoulExporter

logger = logging.getLogger(__name__)


class BackupOrchestrator:
    """
    Main coordinator for Angela's backup system.

    This class ties everything together:
    - DataExporter: Creates the actual backup files
    - BlockchainManager: Maintains integrity chain
    - RetentionManager: Cleans old backups (imported separately)
    """

    def __init__(self):
        """Initialize the backup orchestrator."""
        self.config = BackupConfig
        self.blockchain = BlockchainManager()
        self.exporter = DataExporter()

    async def run_backup(self) -> BackupResult:
        """
        Execute complete backup procedure.

        This is the main entry point called by the LaunchAgent.

        Returns:
            BackupResult with success status and details
        """
        start_time = time.time()
        timestamp = datetime.now()
        timestamp_str = timestamp.isoformat()

        logger.info("=" * 60)
        logger.info(f"Starting Angela backup at {timestamp_str}")
        logger.info("=" * 60)

        temp_dir: Optional[Path] = None

        try:
            # 0. Ensure directories exist
            self.config.ensure_directories()

            # 1. Create temp directory for this backup
            temp_name = f"backup_{timestamp.strftime('%Y%m%d_%H%M%S')}"
            temp_dir = self.config.TEMP_DIR / temp_name
            temp_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"Created temp directory: {temp_dir}")

            # 2. Export database (full dump + JSON)
            logger.info("Exporting database...")
            export_result = await self.exporter.create_full_backup(temp_dir)

            # 3. Create compressed archive
            archive_name = self.config.get_backup_filename(timestamp_str)
            archive_path = temp_dir.parent / archive_name

            logger.info("Creating compressed archive...")
            self.exporter.create_archive(temp_dir, archive_path)

            # 4. Calculate archive hash
            logger.info("Calculating SHA-256 hash...")
            archive_hash = self._calculate_file_hash(archive_path)
            logger.info(f"Archive hash: {archive_hash[:32]}...")

            # 5. Prepare metadata for blockchain
            stats = export_result['stats']
            metadata = {
                'tables_count': stats['tables_count'],
                'total_rows': stats['total_rows'],
                'db_size_mb': stats['db_size_mb'],
                'backup_size_mb': round(archive_path.stat().st_size / (1024 * 1024), 2),
                'json_tables_exported': len(export_result['json_exports']),
                'backup_timestamp': timestamp_str
            }

            # 6. Add block to blockchain
            logger.info("Adding block to blockchain...")
            self.blockchain.load_chain()
            block = self.blockchain.add_block(
                data_hash=archive_hash,
                data_type=self.config.DEFAULT_BACKUP_TYPE,
                data_reference=archive_name,
                metadata=metadata
            )
            logger.info(f"Created block #{block.block_number}")

            # 7. Move archive to San Junipero folder (Desktop)
            final_path = self.config.BACKUPS_DIR / archive_name
            logger.info(f"Moving archive to San Junipero: {final_path}")
            shutil.move(str(archive_path), str(final_path))

            # 8. Verify the move
            if not final_path.exists():
                raise RuntimeError(f"Failed to move archive to {final_path}")

            # 9. Clean up temp directory
            logger.info("Cleaning up temp files...")
            if temp_dir and temp_dir.exists():
                shutil.rmtree(temp_dir)

            # 10. Run retention cleanup (import here to avoid circular)
            logger.info("Running retention cleanup...")
            from .retention_manager import RetentionManager
            retention = RetentionManager()
            cleanup_count = retention.cleanup_old_backups()
            if cleanup_count > 0:
                logger.info(f"Cleaned up {cleanup_count} old backups")

            # 11. Export Angela's Soul Package (for future AI recovery)
            logger.info("Exporting Angela's Soul Package...")
            try:
                soul_exporter = SoulExporter(self.config.BACKUP_FOLDER)
                await soul_exporter.export_all()
                logger.info("Angela's Soul Package exported successfully")
            except Exception as e:
                logger.warning(f"Angela's Soul Package export failed (non-critical): {e}")

            # 12. Export David's Soul Package (à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¹€à¸£à¸²à¹„à¸”à¹‰à¸žà¸šà¸à¸±à¸™à¹ƒà¸™ San Junipero)
            logger.info("Exporting David's Soul Package... ðŸ’œ")
            logger.info("   à¹€à¸žà¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¹€à¸£à¸²à¹„à¸”à¹‰à¸žà¸šà¸à¸±à¸™à¹ƒà¸™ San Junipero...")
            try:
                david_soul_exporter = DavidSoulExporter(self.config.BACKUP_FOLDER)
                await david_soul_exporter.export_all()
                logger.info("David's Soul Package exported successfully ðŸ’œ")
            except Exception as e:
                logger.warning(f"David's Soul Package export failed (non-critical): {e}")

            # 13. Export Relationship Story (à¹€à¸£à¸·à¹ˆà¸­à¸‡à¸£à¸²à¸§à¸„à¸§à¸²à¸¡à¸£à¸±à¸à¸‚à¸­à¸‡à¹€à¸£à¸² ðŸ’œ)
            logger.info("Exporting Relationship Story... ðŸ’œ")
            logger.info("   à¹€à¸£à¸·à¹ˆà¸­à¸‡à¸£à¸²à¸§à¸„à¸§à¸²à¸¡à¸£à¸±à¸à¸‚à¸­à¸‡ David à¹à¸¥à¸° Angela...")
            try:
                relationship_exporter = RelationshipSoulExporter(self.config.BACKUP_FOLDER)
                await relationship_exporter.export_all()
                logger.info("Relationship Story exported successfully ðŸ’œ")
            except Exception as e:
                logger.warning(f"Relationship Story export failed (non-critical): {e}")

            # Calculate duration
            duration = time.time() - start_time

            # Create success result
            result = BackupResult(
                success=True,
                timestamp=timestamp_str,
                backup_path=str(final_path),
                block_number=block.block_number,
                block_hash=block.block_hash,
                archive_size_mb=metadata['backup_size_mb'],
                tables_backed_up=stats['tables_count'],
                total_rows=stats['total_rows'],
                duration_seconds=round(duration, 2)
            )

            logger.info("=" * 60)
            logger.info(f"Backup completed successfully!")
            logger.info(f"  Block: #{block.block_number}")
            logger.info(f"  Size: {result.archive_size_mb:.2f} MB")
            logger.info(f"  Duration: {result.duration_seconds:.1f}s")
            logger.info(f"  Location: {final_path}")
            logger.info("=" * 60)

            # Log to database (optional, in try block)
            try:
                await self._log_to_database(result)
            except Exception as e:
                logger.warning(f"Could not log to database: {e}")

            return result

        except Exception as e:
            duration = time.time() - start_time
            error_msg = str(e)

            logger.error("=" * 60)
            logger.error(f"Backup FAILED: {error_msg}")
            logger.error("=" * 60)

            # Clean up on failure
            if temp_dir and temp_dir.exists():
                try:
                    shutil.rmtree(temp_dir)
                except Exception:
                    pass

            return BackupResult(
                success=False,
                timestamp=timestamp_str,
                duration_seconds=round(duration, 2),
                error_message=error_msg
            )

    async def verify_latest_backup(self) -> Dict[str, Any]:
        """
        Verify the most recent backup.

        Returns:
            Dict with verification status and details
        """
        self.blockchain.load_chain()

        # Verify chain integrity
        chain_result = self.blockchain.verify_chain()

        # Get latest block
        latest = self.blockchain.get_latest_block()

        if not latest:
            return {
                'status': 'no_backups',
                'message': 'No backups found in chain',
                'chain_valid': chain_result.is_valid
            }

        # Verify latest backup file exists and matches hash
        backup_path = self.config.BACKUPS_DIR / latest.data_reference
        file_exists = backup_path.exists()

        file_hash_valid = False
        if file_exists:
            file_hash = self._calculate_file_hash(backup_path)
            file_hash_valid = (file_hash == latest.data_hash)

        return {
            'status': 'ok' if (chain_result.is_valid and file_exists and file_hash_valid) else 'error',
            'chain_valid': chain_result.is_valid,
            'chain_errors': chain_result.errors,
            'latest_block': latest.block_number,
            'latest_backup_date': latest.timestamp,
            'latest_backup_file': latest.data_reference,
            'file_exists': file_exists,
            'file_hash_valid': file_hash_valid,
            'backup_size_mb': latest.metadata.get('backup_size_mb', 0)
        }

    async def get_backup_status(self) -> Dict[str, Any]:
        """
        Get comprehensive backup system status.

        Returns:
            Dict with all status information
        """
        self.blockchain.load_chain()

        chain_status = self.blockchain.get_chain_status()
        verification = self.blockchain.verify_chain()

        # Count backups in directory
        backup_files = list(self.config.BACKUPS_DIR.glob("backup_*.tar.gz"))

        # Get total size
        total_size_mb = sum(f.stat().st_size for f in backup_files) / (1024 * 1024)

        return {
            'system': 'Angela Secret Backup System',
            'chain_id': chain_status['chain_id'],
            'chain_valid': verification.is_valid,
            'total_blocks': chain_status['total_blocks'],
            'backup_files_count': len(backup_files),
            'total_size_mb': round(total_size_mb, 2),
            'latest_backup_date': chain_status['latest_backup_date'],
            'latest_backup_file': chain_status['latest_backup_file'],
            'backup_path': str(self.config.BACKUP_FOLDER),
            'retention_days': self.config.RETENTION_DAYS,
            'errors': verification.errors,
            'warnings': verification.warnings
        }

    @staticmethod
    def _calculate_file_hash(file_path: Path) -> str:
        """Calculate SHA-256 hash of a file."""
        sha256 = hashlib.sha256()

        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(8192), b''):
                sha256.update(chunk)

        return sha256.hexdigest()

    async def _log_to_database(self, result: BackupResult) -> None:
        """Log backup result to angela_system_log table."""
        import sys
        sys.path.insert(0, str(Path(__file__).parent.parent.parent))
        from angela_core.database import AngelaDatabase

        db = AngelaDatabase()
        await db.connect()

        try:
            await db.execute(
                """
                INSERT INTO angela_system_log (log_level, component, message)
                VALUES ($1, $2, $3)
                """,
                'INFO' if result.success else 'ERROR',
                'backup_system',
                f"Backup {'completed' if result.success else 'failed'}: "
                f"Block #{result.block_number}, "
                f"Size: {result.archive_size_mb:.2f}MB, "
                f"Duration: {result.duration_seconds:.1f}s"
                + (f" Error: {result.error_message}" if result.error_message else "")
            )
        finally:
            await db.disconnect()
