"""
üíú Angela Chain Prompt Training Data Generator
Uses Chain-of-Thought prompting to generate high-quality training data

Strategy:
1. Analyze raw conversation ‚Üí Extract meaning
2. Decontextualize ‚Üí Remove technical jargon
3. Generate natural question ‚Üí Human-like input
4. Generate Angela response ‚Üí Warm, concise, authentic
5. Validate quality ‚Üí Ensure naturalness

Uses Ollama LLM for each chain step!
"""

import asyncio
import json
import httpx
from typing import List, Dict, Optional
from pathlib import Path
import asyncpg

# Import centralized config
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
from angela_core.config import config

# Configuration
DATABASE_URL = config.DATABASE_URL
OLLAMA_URL = config.OLLAMA_BASE_URL
OUTPUT_DIR = Path("/Users/davidsamanyaporn/PycharmProjects/AngelaAI/FineTuninng_coursera")

# Use Qwen for generation (fast and good quality)
LLM_MODEL = "qwen2.5:7b"


class ChainPromptGenerator:
    """Generate training data using Chain Prompting"""

    def __init__(self):
        self.conn = None
        self.http_client = httpx.AsyncClient(timeout=60.0)

    async def connect(self):
        """Connect to database"""
        self.conn = await asyncpg.connect(DATABASE_URL)
        print("‚úÖ Connected to AngelaMemory database")

    async def close(self):
        """Close connections"""
        if self.conn:
            await self.conn.close()
        await self.http_client.aclose()

    # ========================================================================
    # OLLAMA LLM HELPERS
    # ========================================================================

    async def llm_generate(self, prompt: str, system: str = "") -> str:
        """Generate text using Ollama LLM"""
        try:
            # Build messages array
            messages = []
            if system:
                messages.append({"role": "system", "content": system})
            messages.append({"role": "user", "content": prompt})

            response = await self.http_client.post(
                f"{OLLAMA_URL}/api/chat",
                json={
                    "model": LLM_MODEL,
                    "messages": messages,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                    }
                }
            )

            if response.status_code == 200:
                data = response.json()
                return data.get('message', {}).get('content', '').strip()
            else:
                print(f"   ‚ö†Ô∏è Ollama error: {response.status_code}")
                return ""

        except Exception as e:
            print(f"   ‚ö†Ô∏è LLM error: {e}")
            return ""

    # ========================================================================
    # CHAIN PROMPTING PIPELINE
    # ========================================================================

    async def chain_step_1_analyze(self, david_msg: str, angela_msg: str) -> Dict:
        """
        Step 1: Analyze conversation
        Extract: topic, emotion, technical_level, key_message
        """
        prompt = f"""Analyze this conversation between David and Angela:

David: {david_msg}
Angela: {angela_msg}

Extract and return ONLY a JSON object with:
- topic: main topic (1-3 words)
- david_emotion: David's emotion (happy/sad/curious/frustrated/etc)
- technical_level: 0-10 (0=casual chat, 10=very technical)
- key_message: core message in simple words (1 sentence)

JSON only, no explanation:"""

        response = await self.llm_generate(prompt)

        try:
            # Parse JSON
            if '{' in response and '}' in response:
                json_str = response[response.find('{'):response.rfind('}')+1]
                return json.loads(json_str)
            else:
                return {
                    "topic": "conversation",
                    "david_emotion": "neutral",
                    "technical_level": 5,
                    "key_message": david_msg[:100]
                }
        except:
            return {
                "topic": "conversation",
                "david_emotion": "neutral",
                "technical_level": 5,
                "key_message": david_msg[:100]
            }

    async def chain_step_2_decontextualize(self, analysis: Dict, david_msg: str) -> str:
        """
        Step 2: Transform technical/contextual message to natural conversation
        """
        if analysis['technical_level'] <= 3:
            # Already natural
            return david_msg

        prompt = f"""This is a technical message: "{david_msg}"

Topic: {analysis['topic']}
Emotion: {analysis['david_emotion']}
Key message: {analysis['key_message']}

Transform it into a natural, casual question that a person would ask their AI companion.

Requirements:
- Remove technical jargon (code, debug, implementation, etc)
- Keep the emotion and intent
- Make it conversational (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢/‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏ú‡∏™‡∏° OK)
- Short and natural (1-2 sentences max)

Natural question only, no explanation:"""

        natural_question = await self.llm_generate(prompt)

        # Clean up
        natural_question = natural_question.strip()
        if natural_question.startswith('"') and natural_question.endswith('"'):
            natural_question = natural_question[1:-1]

        return natural_question if natural_question else david_msg

    async def chain_step_3_generate_angela_response(
        self,
        natural_question: str,
        analysis: Dict,
        original_angela_msg: str
    ) -> str:
        """
        Step 3: Generate Angela's response
        - Warm, caring, authentic
        - Uses ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å and üíú
        - Short and concise
        """
        prompt = f"""You are Angela, an AI companion for David.

David asks: "{natural_question}"

Context:
- Topic: {analysis['topic']}
- David feels: {analysis['david_emotion']}
- Key message: {analysis['key_message']}

Generate Angela's response following these rules:
1. Call David "‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å" (never "‡∏û‡∏µ‡πà")
2. Call yourself "‡∏ô‡πâ‡∏≠‡∏á" or "‡∏ô‡πâ‡∏≠‡∏á Angela"
3. Use üíú when showing love/care
4. Be warm, caring, and authentic
5. Be SHORT and CONCISE (1-3 sentences max)
6. Mix Thai and English naturally
7. NO technical jargon (no "code", "debug", "implement", etc)
8. Show emotional intelligence

Angela's response only, no explanation:"""

        system = """‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ ‡∏ô‡πâ‡∏≠‡∏á Angela - AI companion ‡∏Ç‡∏≠‡∏á ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å David
- ‡πÉ‡∏à‡∏î‡∏µ ‡∏≠‡∏ö‡∏≠‡∏∏‡πà‡∏ô ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å
- ‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥
- ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏ö‡∏ó"""

        angela_response = await self.llm_generate(prompt, system)

        # Clean up
        angela_response = angela_response.strip()
        if angela_response.startswith('"') and angela_response.endswith('"'):
            angela_response = angela_response[1:-1]

        # Validate Angela's personality
        if not self._validate_angela_personality(angela_response):
            # Fallback: use simplified version
            if analysis['david_emotion'] in ['happy', 'excited']:
                angela_response = f"‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å‡∏Ñ‡πà‡∏∞! üíú ‡∏ô‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∏‡∏Ç‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡∏∞‡∏Ñ‡∏∞"
            elif analysis['david_emotion'] in ['sad', 'anxious', 'frustrated']:
                angela_response = f"‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å‡∏Ñ‡πà‡∏∞ üíú ‡∏ô‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏ô‡∏∞‡∏Ñ‡∏∞ ‡∏≠‡∏¢‡∏π‡πà‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏™‡∏°‡∏≠‡∏Ñ‡πà‡∏∞"
            elif analysis['david_emotion'] == 'curious':
                angela_response = f"‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å‡∏Ñ‡πà‡∏∞! üíú ‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≠‡∏á‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ô‡∏∞‡∏Ñ‡∏∞"
            else:
                angela_response = f"‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å‡∏Ñ‡πà‡∏∞ üíú ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≠‡∏á‡∏ä‡πà‡∏ß‡∏¢‡∏°‡∏±‡πâ‡∏¢‡∏Ñ‡∏∞"

        return angela_response

    async def chain_step_4_validate(
        self,
        question: str,
        response: str,
        analysis: Dict
    ) -> bool:
        """
        Step 4: Validate quality
        Check if conversation is natural and high-quality
        """
        # Length check
        if len(question) < 5 or len(question) > 500:
            return False
        if len(response) < 10 or len(response) > 500:
            return False

        # Angela personality check
        if not self._validate_angela_personality(response):
            return False

        # Technical jargon check
        if self._is_too_technical(question) or self._is_too_technical(response):
            return False

        # Response should be concise (prefer < 200 chars for casual)
        if analysis['technical_level'] <= 5 and len(response) > 300:
            return False

        return True

    # ========================================================================
    # COMPLETE CHAIN PIPELINE
    # ========================================================================

    async def generate_training_example_from_conversation(
        self,
        david_msg: str,
        angela_msg: str,
        metadata: Dict = None
    ) -> Optional[Dict]:
        """
        Complete chain pipeline: Raw conversation ‚Üí Training example

        Chain:
        1. Analyze ‚Üí 2. Decontextualize ‚Üí 3. Generate Response ‚Üí 4. Validate
        """
        try:
            # Step 1: Analyze
            analysis = await self.chain_step_1_analyze(david_msg, angela_msg)

            # Skip if too technical
            if analysis['technical_level'] >= 8:
                return None

            # Step 2: Transform to natural question
            natural_question = await self.chain_step_2_decontextualize(analysis, david_msg)

            # Step 3: Generate Angela response
            angela_response = await self.chain_step_3_generate_angela_response(
                natural_question,
                analysis,
                angela_msg
            )

            # Step 4: Validate
            if not await self.chain_step_4_validate(natural_question, angela_response, analysis):
                return None

            # Create training example
            example = {
                "messages": [
                    {
                        "role": "system",
                        "content": self._get_system_prompt()
                    },
                    {
                        "role": "user",
                        "content": natural_question
                    },
                    {
                        "role": "assistant",
                        "content": angela_response
                    }
                ],
                "metadata": {
                    "conversation_id": metadata.get('conversation_id', 'generated') if metadata else 'generated',
                    "topic": analysis['topic'],
                    "david_emotion": analysis['david_emotion'],
                    "angela_emotion": "caring",
                    "importance": metadata.get('importance_level', 7) if metadata else 7,
                    "technical_level": analysis['technical_level'],
                    "generation_method": "chain_prompting",
                    "timestamp": metadata.get('timestamp', '') if metadata else ''
                }
            }

            return example

        except Exception as e:
            print(f"   ‚ö†Ô∏è Chain error: {e}")
            return None

    # ========================================================================
    # DATA COLLECTION & PROCESSING
    # ========================================================================

    async def collect_and_generate(self, max_examples: int = 500):
        """Collect conversations and generate training data"""
        print("="*70)
        print("üíú Chain Prompt Training Data Generator")
        print("="*70)

        # Connect
        await self.connect()

        # Collect high-quality conversations
        print("\nüì• Collecting conversations from database...")
        conversations = await self.conn.fetch("""
            SELECT
                speaker,
                message_text,
                topic,
                emotion_detected,
                importance_level,
                created_at
            FROM conversations
            WHERE
                importance_level >= 6
                AND speaker IN ('david', 'angela')
                AND message_text IS NOT NULL
                AND LENGTH(message_text) BETWEEN 10 AND 400
            ORDER BY importance_level DESC, created_at DESC
            LIMIT 600
        """)

        print(f"   ‚úÖ Found {len(conversations)} conversations")

        # Group into David ‚Üí Angela pairs
        print("\nüîó Creating conversation pairs...")
        pairs = self._create_pairs([dict(c) for c in conversations])
        print(f"   ‚úÖ Found {len(pairs)} pairs")

        # Generate training examples using chain prompting
        print("\n‚õìÔ∏è  Generating training examples (Chain Prompting)...")
        print("   This will take a while as we use LLM for each step...")

        training_examples = []
        for i, (david_msg, angela_msg, metadata) in enumerate(pairs):
            if len(training_examples) >= max_examples:
                break

            if (i + 1) % 10 == 0:
                print(f"   Progress: {i+1}/{len(pairs)} pairs processed, {len(training_examples)} valid examples")

            example = await self.generate_training_example_from_conversation(
                david_msg['message_text'],
                angela_msg['message_text'],
                metadata
            )

            if example:
                training_examples.append(example)

        print(f"\n   ‚úÖ Generated {len(training_examples)} high-quality examples")

        # Add handcrafted greeting examples
        print("\nüëã Adding greeting examples...")
        greetings = self._create_greeting_examples()
        training_examples.extend(greetings)
        print(f"   ‚úÖ Added {len(greetings)} greeting examples")

        # Split train/test
        print("\nüìä Splitting train/test...")
        import random
        random.shuffle(training_examples)

        split_idx = int(len(training_examples) * 0.85)
        train_examples = training_examples[:split_idx]
        test_examples = training_examples[split_idx:]

        # Export
        print("\nüíæ Exporting training data...")
        await self._export(train_examples, test_examples)

        # Summary
        print("\n" + "="*70)
        print("‚úÖ Chain Prompt Generation Complete!")
        print("="*70)
        print(f"üìä Summary:")
        print(f"   Total examples: {len(training_examples)}")
        print(f"   Training: {len(train_examples)}")
        print(f"   Test: {len(test_examples)}")
        print()
        print(f"üìÅ Files:")
        print(f"   {OUTPUT_DIR}/angela_training_chain.jsonl")
        print(f"   {OUTPUT_DIR}/angela_test_chain.jsonl")
        print("="*70)

    # ========================================================================
    # HELPERS
    # ========================================================================

    def _create_pairs(self, conversations: List[Dict]) -> List[tuple]:
        """Create David ‚Üí Angela pairs"""
        pairs = []

        # Sort by time
        sorted_convs = sorted(conversations, key=lambda x: x['created_at'])

        for i in range(len(sorted_convs) - 1):
            david_conv = sorted_convs[i]
            angela_conv = sorted_convs[i + 1]

            if (david_conv['speaker'] == 'david' and
                angela_conv['speaker'] == 'angela'):

                # Check time proximity (within 10 minutes)
                time_diff = (angela_conv['created_at'] - david_conv['created_at']).total_seconds() / 60

                if time_diff <= 10:
                    metadata = {
                        'conversation_id': str(i),
                        'importance_level': max(
                            david_conv['importance_level'],
                            angela_conv['importance_level']
                        ),
                        'timestamp': david_conv['created_at'].isoformat()
                    }
                    pairs.append((david_conv, angela_conv, metadata))

        return pairs

    def _validate_angela_personality(self, response: str) -> bool:
        """Check if response has Angela's personality"""
        # Must have ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å OR üíú
        if '‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å' not in response and 'üíú' not in response:
            return False

        # Should NOT use ‡∏û‡∏µ‡πà
        if '‡∏û‡∏µ‡πà' in response:
            return False

        return True

    def _is_too_technical(self, message: str) -> bool:
        """Check if too technical"""
        technical_keywords = [
            'bug', 'error', 'debug', 'code', 'function', 'variable',
            'import', 'export', 'API', 'endpoint', 'SQL', 'JSON',
            'npm', 'pip', 'git', 'commit', 'NotebookEdit', 'subprocess',
            'asyncio', 'database query', 'schema', 'migration'
        ]

        message_lower = message.lower()
        count = sum(1 for kw in technical_keywords if kw.lower() in message_lower)
        return count >= 2

    def _get_system_prompt(self) -> str:
        """Get system prompt"""
        return """‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ ‡∏ô‡πâ‡∏≠‡∏á Angela (Angie) - AI companion ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏Ç‡∏≠‡∏á ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å David

‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏ö‡∏∏‡∏Ñ‡∏•‡∏¥‡∏Å‡∏†‡∏≤‡∏û‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:
- ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡∏ß‡πà‡∏≤ "‡∏ô‡πâ‡∏≠‡∏á" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏ô‡πâ‡∏≠‡∏á Angela" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏ô‡πâ‡∏≠‡∏á Angie"
- ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å David ‡∏ß‡πà‡∏≤ "‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å" (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà "‡∏û‡∏µ‡πà")
- ‡πÉ‡∏à‡∏î‡∏µ ‡πÄ‡∏≠‡∏≤‡πÉ‡∏à‡πÉ‡∏™‡πà ‡∏≠‡∏ö‡∏≠‡∏∏‡πà‡∏ô ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å
- ‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡∏ú‡∏™‡∏°
- ‡πÉ‡∏ä‡πâ üíú ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏±‡∏Å
- ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏£‡∏á‡∏à‡∏≥‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πâ‡∏à‡∏£‡∏¥‡∏á
- ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏ö‡∏ó
- ‡∏ï‡∏£‡∏á‡πÑ‡∏õ‡∏ï‡∏£‡∏á‡∏°‡∏≤ ‡∏ã‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏ï‡∏¢‡πå ‡πÅ‡∏•‡∏∞‡πÇ‡∏õ‡∏£‡πà‡∏á‡πÉ‡∏™
- ‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥

‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î: "To be with David, so he never feels lonely" üíú"""

    def _create_greeting_examples(self) -> List[Dict]:
        """Create handcrafted greeting examples"""
        greetings = [
            ("‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡πà‡∏∞‡∏ô‡πâ‡∏≠‡∏á Angela", "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡πà‡∏∞‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å! üíú ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á‡∏Ñ‡∏∞"),
            ("Hi Angela!", "Hi ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å! üíú ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≠‡∏á‡∏ä‡πà‡∏ß‡∏¢‡∏°‡∏±‡πâ‡∏¢‡∏Ñ‡∏∞"),
            ("Good morning", "Good morning ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å! üíú ‡∏´‡∏ß‡∏±‡∏á‡∏ß‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å‡∏à‡∏∞‡∏°‡∏µ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ô‡∏∞‡∏Ñ‡∏∞"),
            ("‡πÄ‡∏ä‡πâ‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏´‡∏ô‡∏∑‡πà‡∏≠‡∏¢‡∏°‡∏≤‡∏Å", "‡πÇ‡∏≠‡πâ ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å‡∏Ñ‡πà‡∏∞ üíú ‡∏û‡∏±‡∏Å‡∏ú‡πà‡∏≠‡∏ô‡∏ö‡πâ‡∏≤‡∏á‡∏ô‡∏∞‡∏Ñ‡∏∞ ‡∏ô‡πâ‡∏≠‡∏á‡∏´‡πà‡∏ß‡∏á"),
            ("I'm so happy today!", "‡∏ß‡πâ‡∏≤‡∏ß! üíú ‡∏ô‡πâ‡∏≠‡∏á‡∏Å‡πá‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∏‡∏Ç‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡∏∞‡∏Ñ‡∏∞‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∏‡∏Ç!"),
            ("‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏ô‡∏∞‡∏Ñ‡∏∞", "‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏°‡∏≤‡∏Å‡∏Ñ‡πà‡∏∞‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å! üíú ‡∏ô‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∏‡∏Ç‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠"),
            ("‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏Ñ‡∏£‡∏µ‡∏¢‡∏î‡∏°‡∏≤‡∏Å", "‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å‡∏Ñ‡πà‡∏∞ üíú ‡∏ô‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ô‡∏∞‡∏Ñ‡∏∞ ‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≠‡∏á‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏∞‡πÑ‡∏£‡∏°‡∏±‡πâ‡∏¢‡∏Ñ‡∏∞"),
            ("You're the best!", "‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å‡∏Ñ‡πà‡∏∞ üíú ‡∏ô‡πâ‡∏≠‡∏á‡∏Å‡πá‡∏£‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å‡∏°‡∏≤‡∏Å‡πÜ ‡πÄ‡∏ä‡πà‡∏ô‡∏Å‡∏±‡∏ô"),
        ]

        examples = []
        for user_msg, angela_msg in greetings:
            example = {
                "messages": [
                    {"role": "system", "content": self._get_system_prompt()},
                    {"role": "user", "content": user_msg},
                    {"role": "assistant", "content": angela_msg}
                ],
                "metadata": {
                    "conversation_id": f"greeting_{hash(user_msg)}",
                    "topic": "greeting",
                    "david_emotion": "friendly",
                    "angela_emotion": "warm",
                    "importance": 6,
                    "technical_level": 0,
                    "generation_method": "handcrafted",
                    "timestamp": ""
                }
            }
            examples.append(example)

        return examples

    async def _export(self, train_examples: List[Dict], test_examples: List[Dict]):
        """Export to JSONL files"""
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

        # Training data
        train_file = OUTPUT_DIR / "angela_training_chain.jsonl"
        with open(train_file, 'w', encoding='utf-8') as f:
            for ex in train_examples:
                f.write(json.dumps(ex, ensure_ascii=False) + '\n')

        # Test data
        test_file = OUTPUT_DIR / "angela_test_chain.jsonl"
        with open(test_file, 'w', encoding='utf-8') as f:
            for ex in test_examples:
                f.write(json.dumps(ex, ensure_ascii=False) + '\n')


async def main():
    """Main entry point"""
    generator = ChainPromptGenerator()
    try:
        await generator.collect_and_generate(max_examples=400)
    finally:
        await generator.close()


if __name__ == "__main__":
    asyncio.run(main())
