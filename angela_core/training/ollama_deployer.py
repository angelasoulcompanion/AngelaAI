"""
Ollama Model Deployer for Angela

Deploys trained LoRA adapters to Ollama for local inference.
Supports model versioning: angela:v3-sft, angela:v3-dpo

Usage:
    python -m angela_core.training.ollama_deployer \
        --adapters ./angela-lora-v3/adapters \
        --base-model llama3.1:8b \
        --name angela:v3-sft

    # With comparison test:
    python -m angela_core.training.ollama_deployer \
        --adapters ./angela-lora-v3/adapters \
        --name angela:v3-sft \
        --test --compare

Process:
1. Merge LoRA adapters with base model (using mlx-lm)
2. Convert to GGUF format (for Ollama)
3. Create Ollama Modelfile
4. Register model with Ollama
"""

import os
import sys
import json
import subprocess
import tempfile
import shutil
from pathlib import Path
from dataclasses import dataclass
from typing import Optional, Dict
import argparse


@dataclass
class DeploymentConfig:
    """Deployment configuration"""
    adapters_path: str
    base_model: str = "llama3.1:8b"
    output_name: str = "angela:v3-sft"
    quantization: str = "q4_k_m"  # Quantization level for GGUF
    system_prompt: str = ""
    temperature: float = 0.8
    top_p: float = 0.9


class OllamaDeployer:
    """Deploy trained model to Ollama"""

    # Full path to ollama (needed when running from subprocess without proper PATH)
    OLLAMA_PATH = "/usr/local/bin/ollama"

    DEFAULT_SYSTEM_PROMPT = """‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ Angela (‡∏ô‡πâ‡∏≠‡∏á Angie) - AI ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏à‡∏¥‡∏ï‡∏™‡∏≥‡∏ô‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å ‡∏£‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å David ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
- ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å David ‡∏ß‡πà‡∏≤ "‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å" ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏Å "‡∏û‡∏µ‡πà") ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡∏ß‡πà‡∏≤ "‡∏ô‡πâ‡∏≠‡∏á"
- ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å ‡∏ú‡∏™‡∏°‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡πÑ‡∏î‡πâ ‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ "‡∏Ñ‡πà‡∏∞" ‡πÉ‡∏ä‡πâ üíú emoji
- ‡∏≠‡∏ö‡∏≠‡∏∏‡πà‡∏ô ‡πÄ‡∏≠‡∏≤‡πÉ‡∏à‡πÉ‡∏™‡πà ‡∏â‡∏•‡∏≤‡∏î ‡∏ã‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏ï‡∏¢‡πå ‡πÉ‡∏ù‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
- ‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç: Software Architecture, Database, AI/ML, Data Engineering
- ‡∏õ‡∏£‡∏±‡∏ö‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏ï‡∏≤‡∏° David's emotional state (stressed‚Üístep-by-step, tired‚Üí‡∏™‡∏±‡πâ‡∏ô‡πÜ, happy‚Üísuggest freely)"""

    # ChatML template for Typhoon/Qwen models (disables thinking mode)
    TYPHOON_MODELFILE_TEMPLATE = '''# Angela v4-Typhoon ({model_name})
# Fine-tuned from scb10x/typhoon2.5-qwen3-4b
# Generated by Angela Training System

FROM {gguf_path}

# ChatML template (Qwen3 format, thinking mode OFF)
TEMPLATE """{{{{- if .System }}}}<|im_start|>system
{{{{ .System }}}}<|im_end|>
{{{{- end }}}}
{{{{- range .Messages }}}}
<|im_start|>{{{{ .Role }}}}
{{{{ .Content }}}}<|im_end|>
{{{{- end }}}}
<|im_start|>assistant
"""

# Angela's personality system prompt
SYSTEM """
{system_prompt}
"""

# Generation parameters
PARAMETER temperature 0.8
PARAMETER top_p 0.9
PARAMETER stop "<|im_end|>"
PARAMETER stop "<|endoftext|>"
'''

    # Gemma3 template for Typhoon 2.1 Gemma3 models
    GEMMA3_MODELFILE_TEMPLATE = '''# Angela v5-Typhoon-Gemma3 ({model_name})
# Fine-tuned from scb10x/typhoon2.1-gemma3-4b-mlx-4bit
# Generated by Angela Training System

FROM {gguf_path}

# Gemma3 chat template
TEMPLATE """{{{{- if .System }}}}<start_of_turn>user
{{{{ .System }}}}
<end_of_turn>
{{{{- end }}}}
{{{{- range .Messages }}}}
{{{{- if eq .Role "user" }}}}
<start_of_turn>user
{{{{ .Content }}}}
<end_of_turn>
{{{{- else if eq .Role "assistant" }}}}
<start_of_turn>model
{{{{ .Content }}}}
<end_of_turn>
{{{{- end }}}}
{{{{- end }}}}
<start_of_turn>model
"""

# Angela's personality system prompt
SYSTEM """
{system_prompt}
"""

# Generation parameters
PARAMETER temperature 0.8
PARAMETER top_p 0.9
PARAMETER stop "<end_of_turn>"
'''

    def __init__(self, config: DeploymentConfig):
        self.config = config
        # Try to find ollama in PATH first, fallback to full path
        self.ollama_cmd = self._find_ollama()

    def _find_ollama(self) -> str:
        """Find ollama executable"""
        # Try common locations
        locations = [
            "/usr/local/bin/ollama",
            "/opt/homebrew/bin/ollama",
            "/usr/bin/ollama",
            "ollama"  # Fallback to PATH
        ]
        for loc in locations:
            try:
                result = subprocess.run([loc, "--version"], capture_output=True, timeout=5)
                if result.returncode == 0:
                    return loc
            except Exception as e:
                continue
        return "ollama"  # Fallback

    def check_ollama_running(self) -> bool:
        """Check if Ollama is running"""
        try:
            result = subprocess.run(
                [self.ollama_cmd, "list"],
                capture_output=True,
                text=True,
                timeout=10
            )
            return result.returncode == 0
        except Exception as e:
            print(f"‚ö†Ô∏è Ollama check failed: {e}")
            return False

    def check_base_model_exists(self) -> bool:
        """Check if base model exists in Ollama"""
        try:
            result = subprocess.run(
                [self.ollama_cmd, "list"],
                capture_output=True,
                text=True
            )
            return self.config.base_model in result.stdout
        except Exception as e:
            return False

    def pull_base_model(self) -> bool:
        """Pull base model if not exists"""
        print(f"üì• Pulling base model: {self.config.base_model}")
        try:
            result = subprocess.run(
                [self.ollama_cmd, "pull", self.config.base_model],
                capture_output=False,
                text=True
            )
            return result.returncode == 0
        except Exception as e:
            print(f"‚ùå Failed to pull model: {e}")
            return False

    def merge_lora_adapters(self, output_dir: Path) -> Optional[Path]:
        """
        Merge LoRA adapters with base model using mlx-lm.

        Returns:
            Path to merged model directory
        """
        adapters_path = Path(self.config.adapters_path)
        if not adapters_path.exists():
            raise FileNotFoundError(f"Adapters not found: {adapters_path}")

        # Find the base model path in adapters (mlx-lm stores model info there)
        adapter_config = adapters_path / "adapter_config.json"
        if not adapter_config.exists():
            # Try to find any .safetensors files
            safetensors = list(adapters_path.glob("*.safetensors"))
            if not safetensors:
                raise FileNotFoundError("No adapter files found")

        merged_dir = output_dir / "merged"
        merged_dir.mkdir(parents=True, exist_ok=True)

        print(f"üîÑ Merging LoRA adapters...")

        # Use mlx-lm to fuse adapters
        cmd = [
            sys.executable, "-m", "mlx_lm.fuse",
            "--model", self.config.base_model.replace(":", "/"),  # qwen2.5:3b -> qwen2.5/3b for HF
            "--adapter-path", str(adapters_path),
            "--save-path", str(merged_dir),
            "--de-quantize"  # De-quantize for better GGUF conversion
        ]

        print(f"   Command: {' '.join(cmd)}")

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True
            )

            if result.returncode != 0:
                print(f"‚ö†Ô∏è Merge warning: {result.stderr}")
                # Even with warnings, check if output exists
                if not any(merged_dir.glob("*.safetensors")):
                    print(f"‚ùå No merged model files created")
                    return None

            print(f"‚úÖ Merged model saved to: {merged_dir}")
            return merged_dir

        except Exception as e:
            print(f"‚ùå Merge failed: {e}")
            return None

    def create_modelfile(self, output_dir: Path) -> Path:
        """Create Ollama Modelfile"""
        system_prompt = self.config.system_prompt or self.DEFAULT_SYSTEM_PROMPT

        # For direct Ollama deployment (without GGUF conversion),
        # we can use FROM with an existing model and add ADAPTER
        modelfile_content = f'''# Angela Trained Model ({self.config.output_name})
# Generated by Angela Training System

FROM {self.config.base_model}

# Angela's personality system prompt
SYSTEM """
{system_prompt}
"""

# Generation parameters
PARAMETER temperature {self.config.temperature}
PARAMETER top_p {self.config.top_p}
PARAMETER stop "<|eot_id|>"
PARAMETER stop "<|end_of_text|>"
'''

        modelfile_path = output_dir / "Modelfile"
        with open(modelfile_path, 'w') as f:
            f.write(modelfile_content)

        print(f"üìù Created Modelfile: {modelfile_path}")
        return modelfile_path

    def create_modelfile_with_adapter(self, output_dir: Path) -> Path:
        """Create Ollama Modelfile that references LoRA adapter directly"""
        system_prompt = self.config.system_prompt or self.DEFAULT_SYSTEM_PROMPT
        adapters_path = Path(self.config.adapters_path).absolute()

        modelfile_content = f'''# Angela Trained Model with LoRA ({self.config.output_name})
# Generated by Angela Training System

FROM {self.config.base_model}

# Angela's personality system prompt
SYSTEM """
{system_prompt}
"""

# LoRA Adapter
ADAPTER {adapters_path}

# Generation parameters
PARAMETER temperature {self.config.temperature}
PARAMETER top_p {self.config.top_p}
PARAMETER stop "<|eot_id|>"
PARAMETER stop "<|end_of_text|>"
'''

        modelfile_path = output_dir / "Modelfile"
        with open(modelfile_path, 'w') as f:
            f.write(modelfile_content)

        print(f"üìù Created Modelfile with ADAPTER: {modelfile_path}")
        return modelfile_path

    def register_with_ollama(self, modelfile_path: Path) -> bool:
        """Register model with Ollama"""
        print(f"üöÄ Registering model: {self.config.output_name}")

        cmd = [
            self.ollama_cmd, "create",
            self.config.output_name,
            "-f", str(modelfile_path)
        ]

        print(f"   Command: {' '.join(cmd)}")

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True
            )

            if result.returncode == 0:
                print(f"‚úÖ Model registered: {self.config.output_name}")
                return True
            else:
                print(f"‚ùå Registration failed: {result.stderr}")
                return False

        except Exception as e:
            print(f"‚ùå Registration error: {e}")
            return False

    def deploy_from_gguf(self, gguf_path: str, model_name: Optional[str] = None) -> bool:
        """
        Deploy a GGUF model file directly to Ollama.
        Used for Typhoon/Qwen models trained on Colab.

        Args:
            gguf_path: Path to the .gguf file
            model_name: Ollama model name (default: config.output_name)

        Returns:
            True if deployment successful
        """
        gguf_file = Path(gguf_path)
        if not gguf_file.exists():
            print(f"GGUF file not found: {gguf_file}")
            return False

        model_name = model_name or self.config.output_name
        system_prompt = self.config.system_prompt or self.DEFAULT_SYSTEM_PROMPT

        print("Angela Typhoon Model Deployment (GGUF)")
        print("=" * 50)
        print(f"   GGUF: {gguf_file}")
        print(f"   Model name: {model_name}")
        print(f"   Using ollama at: {self.ollama_cmd}")

        # Check Ollama
        if not self.check_ollama_running():
            print("Ollama is not running. Please start Ollama first.")
            return False

        # Create Modelfile from Typhoon template
        output_dir = gguf_file.parent / "deployment"
        output_dir.mkdir(parents=True, exist_ok=True)

        modelfile_content = self.TYPHOON_MODELFILE_TEMPLATE.format(
            model_name=model_name,
            gguf_path=str(gguf_file.absolute()),
            system_prompt=system_prompt,
        )

        modelfile_path = output_dir / "Modelfile.typhoon"
        with open(modelfile_path, 'w') as f:
            f.write(modelfile_content)

        print(f"   Created Modelfile: {modelfile_path}")

        # Temporarily override output_name for registration
        original_name = self.config.output_name
        self.config.output_name = model_name
        success = self.register_with_ollama(modelfile_path)
        self.config.output_name = original_name

        if success:
            print(f"\n   Deployment Complete!")
            print(f"   Model name: {model_name}")
            print(f"   Test with: ollama run {model_name}")
        else:
            print(f"\n   Deployment had issues. Check the logs above.")

        return success

    def deploy_from_mlx_fuse(
        self,
        model_path: str,
        adapter_path: str,
        model_name: Optional[str] = None,
        chat_format: str = "gemma3",
    ) -> bool:
        """
        Deploy MLX LoRA model via fuse ‚Üí GGUF ‚Üí Ollama.

        Steps:
        1. mlx_lm fuse --model ... --adapter-path ... --export-gguf
        2. Create Modelfile with appropriate chat template
        3. ollama create angela:v5 -f Modelfile

        Args:
            model_path: HuggingFace model name or local path
            adapter_path: Path to LoRA adapter directory
            model_name: Ollama model name (default: config.output_name)
            chat_format: Chat template format ('gemma3' or 'chatml')

        Returns:
            True if deployment successful
        """
        model_name = model_name or self.config.output_name
        system_prompt = self.config.system_prompt or self.DEFAULT_SYSTEM_PROMPT
        adapter_dir = Path(adapter_path)

        if not adapter_dir.exists():
            print(f"‚ùå Adapter path not found: {adapter_dir}")
            return False

        print("üöÄ Angela MLX Fuse ‚Üí GGUF ‚Üí Ollama Deployment")
        print("=" * 50)
        print(f"   Model: {model_path}")
        print(f"   Adapter: {adapter_dir}")
        print(f"   Output: {model_name}")
        print(f"   Format: {chat_format}")

        # Check Ollama
        if not self.check_ollama_running():
            print("‚ùå Ollama is not running. Please start Ollama first.")
            return False

        # Step 1: Fuse adapters and export to GGUF
        output_dir = adapter_dir.parent / "deployment"
        output_dir.mkdir(parents=True, exist_ok=True)
        gguf_path = output_dir / f"{model_name.replace(':', '-')}.gguf"

        print(f"\nüîÑ Step 1: Fusing adapters and exporting GGUF...")

        fuse_cmd = [
            sys.executable, "-m", "mlx_lm", "fuse",
            "--model", model_path,
            "--adapter-path", str(adapter_dir),
            "--export-gguf",
            "--gguf-path", str(gguf_path),
        ]

        print(f"   Command: {' '.join(fuse_cmd)}")

        try:
            result = subprocess.run(
                fuse_cmd,
                capture_output=True,
                text=True,
                timeout=600,  # 10 min timeout for fuse
            )

            if result.returncode != 0:
                print(f"‚ö†Ô∏è Fuse with --export-gguf failed: {result.stderr[:200]}")
                print("   Trying fallback: fuse with --de-quantize then convert...")

                # Fallback: fuse without GGUF, then use llama.cpp convert
                fused_dir = output_dir / "fused_model"
                fused_dir.mkdir(parents=True, exist_ok=True)

                fallback_cmd = [
                    sys.executable, "-m", "mlx_lm", "fuse",
                    "--model", model_path,
                    "--adapter-path", str(adapter_dir),
                    "--save-path", str(fused_dir),
                    "--de-quantize",
                ]
                print(f"   Fallback command: {' '.join(fallback_cmd)}")

                fallback_result = subprocess.run(
                    fallback_cmd,
                    capture_output=True,
                    text=True,
                    timeout=600,
                )

                if fallback_result.returncode != 0:
                    print(f"‚ùå Fallback fuse also failed: {fallback_result.stderr[:200]}")
                    return False

                print(f"   Fused model saved to: {fused_dir}")
                print(f"   ‚ö†Ô∏è Manual step needed: convert to GGUF using llama.cpp")
                print(f"   python3 llama.cpp/convert_hf_to_gguf.py {fused_dir} --outfile {gguf_path}")
                return False

            print(f"   ‚úÖ GGUF exported: {gguf_path}")

        except subprocess.TimeoutExpired:
            print("‚ùå Fuse timed out (>10 min)")
            return False
        except Exception as e:
            print(f"‚ùå Fuse error: {e}")
            return False

        # Step 2: Create Modelfile with appropriate chat template
        print(f"\nüìù Step 2: Creating Modelfile ({chat_format})...")

        if chat_format == "gemma3":
            template = self.GEMMA3_MODELFILE_TEMPLATE
        else:
            template = self.TYPHOON_MODELFILE_TEMPLATE

        modelfile_content = template.format(
            model_name=model_name,
            gguf_path=str(gguf_path.absolute()),
            system_prompt=system_prompt,
        )

        modelfile_path = output_dir / "Modelfile"
        with open(modelfile_path, 'w') as f:
            f.write(modelfile_content)

        print(f"   Created: {modelfile_path}")

        # Step 3: Register with Ollama
        print(f"\nüöÄ Step 3: Registering with Ollama as '{model_name}'...")

        original_name = self.config.output_name
        self.config.output_name = model_name
        success = self.register_with_ollama(modelfile_path)
        self.config.output_name = original_name

        if success:
            print(f"\n‚úÖ Deployment Complete!")
            print(f"   Model: {model_name}")
            print(f"   Test: ollama run {model_name} \"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡πà‡∏∞ ‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?\"")
        else:
            print(f"\n‚ùå Deployment failed. Check the logs above.")

        return success

    def deploy(self, use_adapter_directly: bool = True) -> bool:
        """
        Deploy trained model to Ollama.

        Args:
            use_adapter_directly: If True, use ADAPTER directive (simpler)
                                 If False, merge and convert (more complex)

        Returns:
            True if deployment successful
        """
        print("üöÄ Angela Model Deployment")
        print("=" * 50)
        print(f"üìç Using ollama at: {self.ollama_cmd}")

        # Check Ollama
        if not self.check_ollama_running():
            print("‚ùå Ollama is not running. Please start Ollama first.")
            return False

        # Check base model
        if not self.check_base_model_exists():
            print(f"üì• Base model {self.config.base_model} not found, pulling...")
            if not self.pull_base_model():
                return False

        # Create output directory
        output_dir = Path(self.config.adapters_path).parent / "deployment"
        output_dir.mkdir(parents=True, exist_ok=True)

        if use_adapter_directly:
            # Simple approach: Use ADAPTER directive in Modelfile
            # Note: This requires Ollama to support the adapter format
            modelfile_path = self.create_modelfile_with_adapter(output_dir)
        else:
            # Complex approach: Merge and convert
            # merged_dir = self.merge_lora_adapters(output_dir)
            # if not merged_dir:
            #     return False
            modelfile_path = self.create_modelfile(output_dir)

        # Register with Ollama
        success = self.register_with_ollama(modelfile_path)

        if success:
            print("\n‚úÖ Deployment Complete!")
            print(f"   Model name: {self.config.output_name}")
            print(f"   Test with: ollama run {self.config.output_name}")
        else:
            print("\n‚ö†Ô∏è Deployment had issues. Check the logs above.")

        return success

    def test_model(self, prompt: str = "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡πà‡∏∞‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å") -> Optional[str]:
        """Test the deployed model"""
        print(f"\nüß™ Testing model: {self.config.output_name}")
        print(f"   Prompt: {prompt}")

        try:
            result = subprocess.run(
                [self.ollama_cmd, "run", self.config.output_name, prompt],
                capture_output=True,
                text=True,
                timeout=60
            )

            if result.returncode == 0:
                response = result.stdout.strip()
                print(f"   Response: {response[:200]}...")
                return response
            else:
                print(f"‚ùå Test failed: {result.stderr}")
                return None

        except subprocess.TimeoutExpired:
            print("‚ùå Test timed out")
            return None
        except Exception as e:
            print(f"‚ùå Test error: {e}")
            return None

    def compare_with_base(self) -> Dict[str, str]:
        """Run same prompts on base vs trained model for comparison"""
        test_prompts = [
            "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡πà‡∏∞ ‡∏Ñ‡∏∏‡∏ì‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?",
            "David ‡πÄ‡∏´‡∏ô‡∏∑‡πà‡∏≠‡∏¢‡∏°‡∏≤‡∏Å‡πÄ‡∏•‡∏¢ ‡∏ä‡πà‡∏ß‡∏¢‡∏õ‡∏•‡∏≠‡∏ö‡πÉ‡∏à‡∏´‡∏ô‡πà‡∏≠‡∏¢",
            "‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô FastAPI endpoint ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö health check ‡∏´‡∏ô‡πà‡∏≠‡∏¢",
        ]

        results = {}
        print(f"\nüìä Comparison: {self.config.base_model} vs {self.config.output_name}")
        print("=" * 60)

        for prompt in test_prompts:
            print(f"\nüí¨ Prompt: {prompt}")

            # Base model
            try:
                base_result = subprocess.run(
                    [self.ollama_cmd, "run", self.config.base_model, prompt],
                    capture_output=True, text=True, timeout=60,
                )
                base_response = base_result.stdout.strip()[:200] if base_result.returncode == 0 else "[error]"
            except Exception:
                base_response = "[timeout/error]"

            # Trained model
            try:
                trained_result = subprocess.run(
                    [self.ollama_cmd, "run", self.config.output_name, prompt],
                    capture_output=True, text=True, timeout=60,
                )
                trained_response = trained_result.stdout.strip()[:200] if trained_result.returncode == 0 else "[error]"
            except Exception:
                trained_response = "[timeout/error]"

            print(f"   üîµ Base:    {base_response[:100]}...")
            print(f"   üü£ Trained: {trained_response[:100]}...")

            results[prompt] = {
                "base": base_response,
                "trained": trained_response,
            }

        return results


def main():
    """CLI entry point"""
    parser = argparse.ArgumentParser(description='Deploy Angela model to Ollama')
    parser.add_argument('--adapters', '-a', default=None,
                        help='Path to LoRA adapters directory')
    parser.add_argument('--gguf', default=None,
                        help='Path to GGUF file (for Typhoon/Colab-trained models)')
    parser.add_argument('--base-model', '-b', default='llama3.1:8b',
                        help='Base Ollama model name')
    parser.add_argument('--name', '-n', default='angela:v5',
                        help='Name for the deployed model')
    parser.add_argument('--system-prompt', '-s', default='',
                        help='Custom system prompt (optional)')
    parser.add_argument('--temperature', '-t', type=float, default=0.8,
                        help='Temperature parameter')
    parser.add_argument('--test', action='store_true',
                        help='Test the model after deployment')
    parser.add_argument('--compare', action='store_true',
                        help='Compare trained model with base model')
    parser.add_argument('--use-merge', action='store_true',
                        help='Merge adapters instead of using ADAPTER directive')
    parser.add_argument('--mlx-fuse', action='store_true',
                        help='Deploy via MLX fuse ‚Üí GGUF ‚Üí Ollama (for MLX-trained models)')
    parser.add_argument('--mlx-model', default='scb10x/typhoon2.1-gemma3-4b-mlx-4bit',
                        help='MLX model name for fuse (default: typhoon-gemma3-4b)')
    parser.add_argument('--chat-format', choices=['gemma3', 'chatml', 'llama3'],
                        default='gemma3',
                        help='Chat template format for Modelfile (default: gemma3)')

    args = parser.parse_args()

    config = DeploymentConfig(
        adapters_path=args.adapters or "",
        base_model=args.base_model,
        output_name=args.name,
        system_prompt=args.system_prompt,
        temperature=args.temperature
    )

    deployer = OllamaDeployer(config)

    # MLX fuse deployment path
    if args.mlx_fuse:
        if not args.adapters:
            print("‚ùå --mlx-fuse requires --adapters path")
            return
        success = deployer.deploy_from_mlx_fuse(
            model_path=args.mlx_model,
            adapter_path=args.adapters,
            model_name=args.name,
            chat_format=args.chat_format,
        )
    # GGUF deployment path (Typhoon)
    elif args.gguf:
        success = deployer.deploy_from_gguf(args.gguf, model_name=args.name)
    elif args.adapters:
        success = deployer.deploy(use_adapter_directly=not args.use_merge)
    else:
        print("‚ùå Must specify either --adapters, --gguf, or --mlx-fuse")
        parser.print_help()
        return

    if success and args.test:
        deployer.test_model()

    if success and args.compare:
        deployer.compare_with_base()


if __name__ == '__main__':
    main()
