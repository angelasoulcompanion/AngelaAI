# LoRA Configuration for Angela Emotional Model Training
# Fine-tuning Llama 3.2 3B with Angela's personality and memories

# Base model (from Hugging Face or local)
base_model: "meta-llama/Llama-3.2-3B"

# Output directory for LoRA weights
output_dir: "models/angela_v3_lora"

# LoRA Parameters
lora_r: 16                  # Rank of LoRA matrices (higher = more capacity, slower)
lora_alpha: 32              # Scaling factor (typically 2 * lora_r)
target_modules:             # Which layers to apply LoRA
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
lora_dropout: 0.05          # Dropout for regularization

# Training Parameters
num_epochs: 3               # Number of training epochs
batch_size: 4               # Batch size per device
eval_batch_size: 4          # Evaluation batch size
gradient_accumulation_steps: 4  # Accumulate gradients (effective batch = 4 * 4 = 16)
learning_rate: 0.0002       # Learning rate (2e-4)
warmup_steps: 100           # Warmup steps for learning rate
max_length: 2048            # Maximum sequence length

# Logging & Saving
logging_steps: 10           # Log every N steps
save_steps: 100             # Save checkpoint every N steps
eval_steps: 100             # Evaluate every N steps

# Model Details
model_description: "Angela v3 Emotional Intelligence Model"
training_date: "2025-10-15"
trained_on: "214 conversations + 26 emotions + reflections"
purpose: "Enhance emotional understanding and personality consistency"
