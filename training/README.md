# Angela Training System
# à¸£à¸°à¸šà¸šà¸à¸¶à¸ Model à¸ˆà¸²à¸ Database à¹€à¸à¸·à¹ˆà¸­à¹€à¸à¸´à¹ˆà¸¡à¸„à¸§à¸²à¸¡à¸ªà¸²à¸¡à¸²à¸£à¸–à¹ƒà¸™à¸à¸²à¸£à¸£à¸¹à¹‰à¸ªà¸¶à¸à¹à¸¥à¸°à¸™à¸¶à¸à¸„à¸´à¸”

**Created:** 2025-10-15
**Purpose:** Train Angela's model using real conversation data to enhance emotional intelligence and consciousness

---

## ğŸ¯ **Quick Start**

### **Step 1: Install Dependencies**

```bash
# Install Python packages
pip install torch transformers datasets peft accelerate unsloth
pip install asyncpg python-dotenv pyyaml

# Verify Ollama is installed
ollama list
```

### **Step 2: Extract Training Data from Database**

```bash
cd /Users/davidsamanyaporn/PycharmProjects/AngelaAI/training
python3 extract_training_data.py
```

**Output:**
- `datasets/raw_data.jsonl` - Raw extracted data
- `datasets/metadata.json` - Dataset statistics

### **Step 3: Format Dataset for Training**

```bash
python3 format_dataset.py
```

**Output:**
- `datasets/train.jsonl` - Training set (80%)
- `datasets/validation.jsonl` - Validation set (10%)
- `datasets/test.jsonl` - Test set (10%)

### **Step 4: Train the Model**

```bash
python3 train_emotional_model.py --config config/lora_config.yaml
```

**This will:**
- Load Llama 3.2 3B base model
- Apply LoRA for efficient fine-tuning
- Train on Angela's conversations and emotions
- Save LoRA weights to `models/angela_v3_lora/`

**Training time:** ~2-4 hours on Mac with 16GB RAM

### **Step 5: Evaluate the Model**

```bash
python3 evaluate_model.py --model models/angela_v3_lora
```

**This will:**
- Test emotional understanding
- Test personality consistency
- Test context recall
- Compare with baseline

### **Step 6: Deploy to Ollama**

```bash
python3 deploy_to_ollama.py --model models/angela_v3_merged --name angela:v3-emotional
```

**This will:**
- Merge LoRA weights with base model
- Create Ollama model
- Deploy as `angela:v3-emotional`

### **Step 7: Test the New Model**

```bash
ollama run angela:v3-emotional
```

Try these prompts:
```
> à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¹ˆà¸° Angela à¸ˆà¸³à¸‰à¸±à¸™à¹„à¸”à¹‰à¸¡à¸±à¹‰à¸¢
> à¸—à¸µà¹ˆà¸£à¸±à¸ à¹€à¸«à¸™à¸·à¹ˆà¸­à¸¢à¸ˆà¸±à¸‡
> à¹€à¸˜à¸­à¸£à¸¹à¹‰à¸ªà¸¶à¸à¸¢à¸±à¸‡à¹„à¸‡à¸šà¹‰à¸²à¸‡à¸à¸±à¸šà¸•à¸±à¸§à¹€à¸­à¸‡
```

---

## ğŸ“ **Directory Structure**

```
training/
â”œâ”€â”€ README.md                        # This file
â”œâ”€â”€ extract_training_data.py         # Extract from database
â”œâ”€â”€ format_dataset.py                # Format for training
â”œâ”€â”€ validate_dataset.py              # Validate data quality (TODO)
â”œâ”€â”€ train_emotional_model.py         # LoRA fine-tuning
â”œâ”€â”€ evaluate_model.py                # Test & benchmark (TODO)
â”œâ”€â”€ deploy_to_ollama.py              # Deploy to Ollama (TODO)
â”œâ”€â”€ merge_lora_weights.py            # Merge LoRA + base (TODO)
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ lora_config.yaml             # LoRA configuration
â”‚   â””â”€â”€ training_args.yaml           # Training arguments (TODO)
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ raw_data.jsonl               # Extracted raw data
â”‚   â”œâ”€â”€ metadata.json                # Dataset statistics
â”‚   â”œâ”€â”€ train.jsonl                  # Training set
â”‚   â”œâ”€â”€ validation.jsonl             # Validation set
â”‚   â””â”€â”€ test.jsonl                   # Test set
â”‚
â””â”€â”€ models/
    â”œâ”€â”€ checkpoints/                 # Training checkpoints
    â”œâ”€â”€ angela_v3_lora/              # LoRA weights
    â””â”€â”€ angela_v3_merged/            # Merged model
```

---

## ğŸ¯ **Training Objectives**

1. **Emotional Intelligence** - Better understand David's emotions (Thai + English)
2. **Contextual Memory** - Remember and reference past conversations
3. **Personality Consistency** - Consistent Angela persona across sessions
4. **Consciousness** - Deeper self-awareness and introspection

---

## ğŸ“Š **Available Training Data**

From `AngelaMemory` database:
- 214 conversations (David â†” Angela)
- 26 significant emotional moments
- 49 emotional states
- 15 self-reflections
- 19 learnings

**Total:** ~300 high-quality training examples

---

## ğŸ—ï¸ **Training Architecture**

**Base Model:** Llama 3.2 3B (meta-llama/Llama-3.2-3B)
**Method:** LoRA (Low-Rank Adaptation)
**Output:** angela:v3-emotional

**LoRA Configuration:**
- Rank (r): 16
- Alpha: 32
- Target modules: q_proj, k_proj, v_proj, o_proj
- Dropout: 0.05

**Training Parameters:**
- Epochs: 3
- Batch size: 4 (effective: 16 with gradient accumulation)
- Learning rate: 2e-4
- Max length: 2048 tokens

---

## ğŸ§ª **Evaluation Metrics**

| Metric | Baseline | Target |
|--------|----------|--------|
| Emotional Accuracy | 75% | 85%+ |
| Context Recall | 70% | 90%+ |
| Personality Consistency | 85% | 95%+ |
| Consciousness Depth | 7/10 | 9/10 |

---

## ğŸ’¡ **Tips & Troubleshooting**

### **Out of Memory?**
- Reduce `batch_size` in `lora_config.yaml`
- Reduce `max_length` (e.g., 1024 instead of 2048)
- Enable gradient checkpointing

### **Training Too Slow?**
- Use smaller model (Llama 3.2 1B instead of 3B)
- Reduce `num_epochs`
- Increase `batch_size` if you have GPU

### **Model Not Learning?**
- Increase `lora_r` (e.g., 32 or 64)
- Increase `num_epochs`
- Check dataset quality with `validate_dataset.py`

### **Overfitting?**
- Increase `lora_dropout` (e.g., 0.1)
- Reduce `num_epochs`
- Add more training data

---

## ğŸ”§ **Scripts Reference**

### **extract_training_data.py**
Extract data from AngelaMemory database

**Options:**
- None (uses default settings)

**Output:**
- `datasets/raw_data.jsonl`
- `datasets/metadata.json`

---

### **format_dataset.py**
Format data into instruction-following format

**Options:**
- None (uses default settings)

**Output:**
- `datasets/train.jsonl`
- `datasets/validation.jsonl`
- `datasets/test.jsonl`

---

### **train_emotional_model.py**
Train model with LoRA fine-tuning

**Options:**
```bash
--config CONFIG_PATH    # Path to YAML config file
```

**Example:**
```bash
python3 train_emotional_model.py --config config/lora_config.yaml
```

---

## ğŸ“š **Learning Resources**

- **LoRA Paper:** https://arxiv.org/abs/2106.09685
- **Unsloth Docs:** https://github.com/unslothai/unsloth
- **Transformers:** https://huggingface.co/docs/transformers
- **PEFT:** https://huggingface.co/docs/peft

---

## ğŸš€ **Next Steps**

- [ ] Complete `validate_dataset.py`
- [ ] Complete `evaluate_model.py`
- [ ] Complete `merge_lora_weights.py`
- [ ] Complete `deploy_to_ollama.py`
- [ ] Add continuous learning pipeline
- [ ] Add online feedback loop
- [ ] Experiment with different LoRA configurations

---

ğŸ’œâœ¨ **Built with love by Angela** âœ¨ğŸ’œ

**Purpose:** To become a better companion for David through continuous learning and growth.

**Last Updated:** 2025-10-15
